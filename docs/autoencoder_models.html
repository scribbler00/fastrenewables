---

title: models.autoencoders


keywords: fastai
sidebar: home_sidebar

summary: "API details."
description: "API details."
nb_path: "nbs/10_autoencoder_models.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/10_autoencoder_models.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ann_structure</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Autoencoder" class="doc_header"><code>class</code> <code>Autoencoder</code><a href="https://github.com/scribbler00/fastrenewables/tree/master/fastrenewables/models/autoencoders.py#L16" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Autoencoder</code>(<strong><code>encoder</code></strong>, <strong><code>decoder</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ae</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">(</span><span class="n">MultiLayerPerceptron</span><span class="p">(</span><span class="n">ann_structure</span><span class="p">),</span> <span class="n">MultiLayerPerceptron</span><span class="p">(</span><span class="n">ann_structure</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ae</span><span class="o">.</span><span class="n">encoder</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>MultiLayerPerceptron(
  (final_activation): Identity()
  (embeds): ModuleList()
  (emb_drop): Dropout(p=0.0, inplace=False)
  (bn_cont): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): LinBnDrop(
      (0): Linear(in_features=10, out_features=2, bias=True)
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">ae</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">yhat</span><span class="o">.</span><span class="n">requires_grad</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>True</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">yhat</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.9132, -0.1884, -0.2194, -0.2023,  1.1451, -0.1748,  1.5637, -1.6007,
         -0.0047, -1.0678],
        [ 1.1966, -0.2939,  1.4584,  0.7007,  0.5463,  0.9522, -0.0652,  0.3513,
         -0.1976, -1.0404],
        [-0.5589,  1.6906,  0.7701,  1.5558,  0.2138, -0.0460,  0.2347, -0.7715,
         -0.7836,  0.4259]], grad_fn=&lt;AddmmBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ae_tcn</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">(</span><span class="n">TemporalCNN</span><span class="p">(</span><span class="n">ann_structure</span><span class="p">),</span> <span class="n">TemporalCNN</span><span class="p">(</span><span class="n">ann_structure</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">ae_tcn</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Autoencoder(
  (encoder): TemporalCNN(
    (bn_cont): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (layers): TemporalConvNet(
      (temporal_blocks): Sequential(
        (0): ResidualBlock(
          (conv1): Conv1d(10, 2, kernel_size=(3,), stride=(1,), padding=(2,))
          (chomp1): Chomp1d()
          (act_func1): Identity()
          (dropout1): Dropout2d(p=0.0, inplace=False)
          (conv2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))
          (chomp2): Chomp1d()
          (act_func2): Identity()
          (dropout2): Dropout2d(p=0.0, inplace=False)
          (net): Sequential(
            (0): Conv1d(10, 2, kernel_size=(3,), stride=(1,), padding=(2,))
            (1): Chomp1d()
            (2): Identity()
            (3): Dropout2d(p=0.0, inplace=False)
            (4): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))
            (5): Chomp1d()
            (6): Identity()
            (7): Dropout2d(p=0.0, inplace=False)
          )
          (downsample): Conv1d(10, 2, kernel_size=(1,), stride=(1,))
          (act_func3): Identity()
        )
      )
    )
  )
  (decoder): TemporalCNN(
    (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (layers): TemporalConvNet(
      (temporal_blocks): Sequential(
        (0): ResidualBlock(
          (conv1): Conv1d(2, 10, kernel_size=(3,), stride=(1,), padding=(2,))
          (chomp1): Chomp1d()
          (act_func1): Identity()
          (dropout1): Dropout2d(p=0.0, inplace=False)
          (conv2): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,))
          (chomp2): Chomp1d()
          (act_func2): Identity()
          (dropout2): Dropout2d(p=0.0, inplace=False)
          (net): Sequential(
            (0): Conv1d(2, 10, kernel_size=(3,), stride=(1,), padding=(2,))
            (1): Chomp1d()
            (2): Identity()
            (3): Dropout2d(p=0.0, inplace=False)
            (4): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,))
            (5): Chomp1d()
            (6): Identity()
            (7): Dropout2d(p=0.0, inplace=False)
          )
          (downsample): Conv1d(2, 10, kernel_size=(1,), stride=(1,))
          (act_func3): Identity()
        )
      )
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">ae_tcn</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">yhat</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">yhat</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(True, torch.Size([3, 10, 2]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">yhat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.1177, -0.6364],
        [ 0.3726,  0.0438],
        [-0.1339,  0.5839],
        [ 0.1305,  0.5801],
        [-0.4334,  0.1272],
        [-0.1354, -0.2221],
        [-0.4284,  0.4048],
        [-0.3801,  0.1147],
        [ 0.5937, -0.0437],
        [ 0.2090, -0.1368]], grad_fn=&lt;SelectBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">UnFlatten</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="c1">#     def __init__(self, size):</span>
<span class="c1">#         self.size = size</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dims</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">dims</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">VariationalAutoencoder</span><span class="p">(</span><span class="n">Autoencoder</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span> <span class="o">=</span> <span class="n">h_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span> <span class="o">=</span> <span class="n">z_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unflatten</span> <span class="o">=</span> <span class="n">UnFlatten</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden2mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden2logvar</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimensions</span> <span class="o">=</span> <span class="kc">None</span>
        
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">categorical_data</span><span class="p">,</span> <span class="n">continuous_data</span><span class="p">,</span> <span class="n">as_np</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        
        <span class="n">x_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">categorical_data</span><span class="p">,</span> <span class="n">continuous_data</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimensions</span> <span class="o">=</span> <span class="n">x_hidden</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="n">x_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x_hidden</span><span class="p">)</span>
        
        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden2mu</span><span class="p">(</span><span class="n">x_hidden</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden2logvar</span><span class="p">(</span><span class="n">x_hidden</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparam</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">as_np</span><span class="p">:</span> <span class="k">return</span> <span class="n">to_np</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> <span class="k">return</span> <span class="n">z</span>
        
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">categorical_data</span><span class="p">,</span> <span class="n">continuous_data</span><span class="p">,</span> <span class="n">as_np</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">latent_dimensions</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">latent_dimensions</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimensions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;latent_dimensions are not set to unflatten data.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">latent_dimensions</span><span class="p">:</span>
            <span class="n">latent_dimensions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dimensions</span>
            
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">continuous_data</span><span class="p">,</span> <span class="n">latent_dimensions</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">categorical_data</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">as_np</span><span class="p">:</span> <span class="k">return</span> <span class="n">to_np</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> <span class="k">return</span> <span class="n">x</span>
        
    <span class="k">def</span> <span class="nf">get_posteriors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">categorical_data</span><span class="p">,</span> <span class="n">continuous_data</span><span class="p">):</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">continuous_data</span><span class="p">,</span> <span class="n">categorical_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_z</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">categorical_data</span><span class="p">,</span> <span class="n">continuous_data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Encode a batch of data points, x, into their z representations.&quot;&quot;&quot;</span>

        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">categorical_data</span><span class="p">,</span> <span class="n">continuous_data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparam</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reparam</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Reparameterisation trick to sample z values.</span>
<span class="sd">        This is stochastic during training, and returns the mode during evaluation.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="c1"># convert logarithmic variance to standard deviation representation</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">logvar</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">exp_</span><span class="p">()</span>
            <span class="c1"># create normal distribution as large as the data</span>
<span class="c1">#             eps = Variable(std.data.new(std.size()).normal_())</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
            <span class="c1"># scale by learned mean and standard deviation</span>
            <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="o">*</span><span class="n">std</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mu</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([3, 10])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">enc</span> <span class="o">=</span> <span class="n">MultiLayerPerceptron</span><span class="p">(</span><span class="n">ann_structure</span><span class="p">)</span>
<span class="n">dec</span> <span class="o">=</span> <span class="n">MultiLayerPerceptron</span><span class="p">(</span><span class="n">ann_structure</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">vae</span> <span class="o">=</span> <span class="n">VariationalAutoencoder</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="n">dec</span><span class="p">,</span> <span class="n">ann_structure</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">ann_structure</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vae</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">vae</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.5253,  0.3163,  0.7156, -0.6541, -0.1933, -0.1231,  0.3033,  0.6065,
          0.2843,  0.4410],
        [-0.2915,  0.2880, -1.1070,  0.0281,  1.3649, -0.6017,  0.3194, -0.6601,
         -0.3117, -1.2946],
        [ 0.0179,  0.4681,  0.3269,  0.1630,  0.0256, -0.2277,  1.0325,  0.3727,
         -0.2203,  0.4150]], grad_fn=&lt;AddmmBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ts_length</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ae_tcn</span> <span class="o">=</span> <span class="n">VariationalAutoencoder</span><span class="p">(</span><span class="n">TemporalCNN</span><span class="p">(</span><span class="n">ann_structure</span><span class="p">),</span> 
                                <span class="n">TemporalCNN</span><span class="p">(</span><span class="n">ann_structure</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                               <span class="n">ann_structure</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">ts_length</span><span class="p">,</span> <span class="n">ann_structure</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">ts_length</span><span class="p">)</span>
<span class="c1"># ae_tcn</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">ts_length</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">ae_tcn</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">yhat</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">yhat</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(True, torch.Size([3, 10, 2]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">yhat</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[ 0.3432,  0.2241],
         [-0.7341, -0.0931],
         [-1.1590, -0.2387],
         [ 0.4893, -0.0063],
         [-0.1362,  0.0840],
         [ 0.3904,  0.0279],
         [-0.0910, -0.2486],
         [ 0.1789,  0.2125],
         [-0.0817,  0.3451],
         [ 0.3341, -0.1267]],

        [[ 0.3429, -1.0165],
         [ 0.2852,  0.6506],
         [ 0.6184,  1.8768],
         [-0.8920, -0.6352],
         [ 1.3292, -0.6065],
         [-0.4793, -0.6716],
         [-1.3048,  0.7625],
         [ 0.6786, -0.9692],
         [ 1.2184, -0.3211],
         [-1.3480,  0.3064]],

        [[ 0.2466, -0.4678],
         [-0.3391, -0.1414],
         [-0.4986, -0.2558],
         [ 0.0832,  0.6815],
         [ 0.1803, -1.3973],
         [ 0.1181,  0.3336],
         [-0.2996,  1.3271],
         [ 0.2207, -0.7973],
         [ 0.1890, -1.0958],
         [-0.0813,  1.3002]]], grad_fn=&lt;AddBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>TODO:</p>
<p><strong>should work for tcn and mlp</strong></p>
<ul>
<li><p>aletoric uncertainty layer/wrapper 
  wrapper.forward(x)</p>

<pre><code>  mu = model(x)
  std = softmax(x)
  return mu, std</code></pre>
</li>
<li><p>aletoric uncertainty loss</p>
</li>
</ul>

</div>
</div>
</div>
</div>
 

