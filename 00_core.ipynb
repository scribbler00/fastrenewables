{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scribbler/anaconda/envs/dies/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import pandas as pd\n",
    "from nbdev.showdoc import *\n",
    "from fastai.data.external import *\n",
    "from fastcore.all import *\n",
    "from pathlib import PosixPath\n",
    "from fastcore.test import *\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def str_to_path(file: str):\n",
    "    \"Convers a string to a Posixpath.\"\n",
    "    if isinstance(file, str) and \"~\" in file:\n",
    "        file = os.path.expanduser(file)\n",
    "\n",
    "    file = Path(file)\n",
    "    \n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq_type(Path(\"\"), str_to_path(\"\"))\n",
    "test_eq_type(Path(\"\"), str_to_path(Path(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_hdf(file:PosixPath, key: str = \"/powerdata\", key_metadata=None):\n",
    "    \"Reads a hdf5 table based on the given key.\"\n",
    "    file = str_to_path(file)\n",
    "    if \"/\" not in key: key = \"/\" + key\n",
    "    with pd.HDFStore(file, \"r\") as store:\n",
    "        if key in store.keys():\n",
    "            df = store[key]\n",
    "            if key_metadata is not None:\n",
    "                df_meta = store[key_metadata]\n",
    "                for c in df_meta: df[c] = df_meta[c].values[0]\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n",
    "                  index=['a', 'b', 'c'])\n",
    "df.to_hdf('data.h5', key='df', mode='w')\n",
    "test_eq(df, read_hdf(\"data.h5\", key=\"df\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_csv(file:PosixPath, sep:str =\";\"):\n",
    "    \"Reads a csv file.\"\n",
    "    file = str_to_path(file)\n",
    "    df = pd.read_csv(str(file), sep=sep)\n",
    "    df.drop([\"Unnamed: 0\"], inplace=True, axis=1, errors=\"ignore\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},)\n",
    "df.to_csv('data.csv', sep=\";\")\n",
    "test_eq(df, read_csv(\"data.csv\", sep=\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(\n",
    "    files:PosixPath,\n",
    "    key:str =\"/powerdata\",\n",
    "    key_metadata=None,\n",
    "    sep:str=\";\"\n",
    ") -> pd.DataFrame:\n",
    "    \"Reads a number of CSV or HDF5 files depending on file ending.\"\n",
    "    \n",
    "    files = listify(files)\n",
    "    dfs=L()\n",
    "    for file in files:\n",
    "        if isinstance(file, str):\n",
    "            file = str_to_path(file)\n",
    "\n",
    "        if file.suffix == \".h5\":\n",
    "            df = read_hdf(file, key, key_metadata=key_metadata)\n",
    "        elif file.suffix == \".csv\":\n",
    "            df = read_csv(file, sep=\";\")\n",
    "        else:\n",
    "            raise f\"File ending of file {file} not supported.\"\n",
    "\n",
    "        dfs += df\n",
    "        \n",
    "    return pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n",
    "#                   index=['a', 'b', 'c'])\n",
    "# df.to_hdf('data.h5', key='df', mode='w')\n",
    "# test_eq(df, read_files(\"data.h5\", key=\"df\")[0])\n",
    "\n",
    "# df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},)\n",
    "# df.to_csv('data.csv', sep=\";\")\n",
    "# test_eq(df, read_files(\"data.csv\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"/home/scribbler/data/DAF_ICON_Synthetic_Wind_Power_processed/00011.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_from_dtypes(df: pd.DataFrame, y_names: list, pre_process:list, procs: list, \n",
    "                   add_y_to_x=False, add_x_to_y=False, ignore_cols=\"\", \n",
    "                   add_seasonal_feautres=True,splits=None):\n",
    "    \n",
    "    y_names = listify(y_names)\n",
    "    ignore_cols = listify(ignore_cols)\n",
    "    \n",
    "    to = TabularPandas(df, y_names=y_names, procs=pre_process, do_setup=True, reduce_memory=False)\n",
    "    df = to.items\n",
    "    \n",
    "    x_columns, cat_columns = cont_cat_split(df, dep_var=y_names, max_card=1000)\n",
    "    \n",
    "    if add_y_to_x:\n",
    "        x_columns += y_names\n",
    "    if add_x_to_y:\n",
    "        y_names += x_columns\n",
    "        \n",
    "    if splits is not None: splits = splits(range_of(df))\n",
    "            \n",
    "    to = TabularPandas(\n",
    "        df,\n",
    "        procs=procs,\n",
    "        cat_names=[c for c in cat_columns if c  not in ignore_cols],\n",
    "        cont_names=[c for c in x_columns if c  not in ignore_cols],\n",
    "        y_names=[c for c in y_names if c  not in ignore_cols],\n",
    "        splits=splits,\n",
    "        do_setup=True,\n",
    "        inplace=True,\n",
    "        y_block=RegressionBlock(),\n",
    "    )\n",
    "\n",
    "    return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddSeasonalFeatures(TabularProc):\n",
    "    order=1\n",
    "    def encodes(self, to):\n",
    "        to.items[\"Month\"] = to.items.index.month\n",
    "        to.items[\"Day\"] = to.items.index.day\n",
    "        to.items[\"Hour\"] = to.items.index.hour\n",
    "\n",
    "class DropYear(TabularProc):\n",
    "    \"Drops a complete year.\"\n",
    "    order = 1\n",
    "    def __init__(self, year=2020):\n",
    "        year = str(year)\n",
    "        self.year = pd.to_datetime(f\"{year}-01-01\", utc=True)\n",
    "        \n",
    "    def encodes(self, to): \n",
    "        mask = to.items.index < self.year\n",
    "        to.items.drop(to.items[mask].index, inplace=True)\n",
    "        \n",
    "class DropCols(TabularProc):\n",
    "    \"Drops rows by column.\"\n",
    "    order = 0\n",
    "    def __init__(self, cols):\n",
    "        self.cols = listify(cols)\n",
    "        \n",
    "    def encodes(self, to): \n",
    "        to.items.drop(self.cols, axis=1, inplace=True, errors=\"ignore\")\n",
    "        \n",
    "class FilterByCol(TabularProc):\n",
    "    \"Drops rows by column.\"\n",
    "    order = 0\n",
    "    def __init__(self, col_name, keep=True):\n",
    "        self.col_name = col_name\n",
    "        self.keep = keep\n",
    "        \n",
    "    def encodes(self, to): \n",
    "        mask = to.items[self.col_name].astype(bool).values\n",
    "        if not self.keep: mask = ~mask\n",
    "        to.items.drop(to.items[mask].index, inplace=True)\n",
    "\n",
    "class FilterMonths(TabularProc):\n",
    "    \"Filter dataframe for specific months.\"\n",
    "    order = 2\n",
    "    def __init__(self, months=range(1,13)):\n",
    "        self.months = listify(months)\n",
    "        \n",
    "    def encodes(self, to): \n",
    "        mask = ~to.items.index.month.isin(self.months)\n",
    "        to.items.drop(to.items[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = !ls /home/scribbler/data/DAF_ICON_Synthetic_Wind_Power_processed/*.h5\n",
    "# files[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(files)\n",
    "# n_files = int(len(files)-len(files)/4)\n",
    "# n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = read_files(files[0:n_files], key_metadata=\"metadata\")\n",
    "# dfs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_drop = L(\"long\", \"lat\", \"loc_id\", \"target_file_name\", \"input_file_name\", \"num_train_samples\", \"num_test_samples\")\n",
    "# to = tp_from_dtypes(dfs, y_names=\"PowerGeneration\", \n",
    "#                     pre_process=[DropCols(cols_to_drop), FilterByCol(\"TestFlag\"), AddSeasonalFeatures],\n",
    "# #                     TODO: Normalize per task, add task embedding and implement normalization trough task id\n",
    "#                     procs=[Normalize, Categorify], \n",
    "#                     add_x_to_y=False, ignore_cols=\"TestFlag\", splits=RandomSplitter(valid_pct=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to.cont_names[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to.cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls = to.dataloaders(bs=1024)\n",
    "# learn = tabular_learner(dls, metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors = L()\n",
    "# for f in files[n_files:]:\n",
    "#     df_test = read_files(f, key_metadata=\"metadata\")\n",
    "#     to_test = tp_from_dtypes(df_test, y_names=\"PowerGeneration\", \n",
    "#                     pre_process=[DropCols(cols_to_drop), FilterByCol(\"TestFlag\", keep=False), AddSeasonalFeatures],\n",
    "# #                     TODO: Normalize per task, add task embedding and implement normalization trough task id\n",
    "#                     procs=[], \n",
    "#                     add_x_to_y=False, ignore_cols=\"TestFlag\")\n",
    "#     to_test_2 = to.new(to_test.items)\n",
    "#     dl_test = learn.dls.test_dl(to_test.items, bs=64)\n",
    "#     targ, preds = learn.get_preds(dl=dl_test)\n",
    "#     e = (((targ-preds)**2).mean()**0.5)\n",
    "#     errors += e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.boxplot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
