{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scribbler/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#export\n",
    "import pandas as pd\n",
    "from nbdev.showdoc import *\n",
    "from fastai.data.external import *\n",
    "from fastcore.all import *\n",
    "from pathlib import PosixPath\n",
    "from fastcore.test import *\n",
    "from fastai.tabular.all import *\n",
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def str_to_path(file: str):\n",
    "    \"Convers a string to a Posixpath.\"\n",
    "    if isinstance(file, str) and \"~\" in file:\n",
    "        file = os.path.expanduser(file)\n",
    "\n",
    "    file = Path(file)\n",
    "    \n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq_type(Path(\"\"), str_to_path(\"\"))\n",
    "test_eq_type(Path(\"\"), str_to_path(Path(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_hdf(file:PosixPath, key: str = \"/powerdata\", key_metadata=None):\n",
    "    \"Reads a hdf5 table based on the given key.\"\n",
    "    file = str_to_path(file)\n",
    "    if \"/\" not in key: key = \"/\" + key\n",
    "    with pd.HDFStore(file, \"r\") as store:\n",
    "        if key in store.keys():\n",
    "            df = store[key]\n",
    "            if key_metadata is not None:\n",
    "                df_meta = store[key_metadata]\n",
    "                for c in df_meta: df[c] = df_meta[c].values[0]\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n",
    "                  index=['a', 'b', 'c'])\n",
    "df.to_hdf('data.h5', key='df', mode='w')\n",
    "test_eq(df, read_hdf(\"data.h5\", key=\"df\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_csv(file:PosixPath, sep:str =\";\"):\n",
    "    \"Reads a csv file.\"\n",
    "    file = str_to_path(file)\n",
    "    df = pd.read_csv(str(file), sep=sep)\n",
    "    df.drop([\"Unnamed: 0\"], inplace=True, axis=1, errors=\"ignore\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},)\n",
    "df.to_csv('data.csv', sep=\";\")\n",
    "test_eq(df, read_csv(\"data.csv\", sep=\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_files(\n",
    "    files:PosixPath,\n",
    "    key:str =\"/powerdata\",\n",
    "    key_metadata=None,\n",
    "    sep:str=\";\"\n",
    ") -> pd.DataFrame:\n",
    "    \"Reads a number of CSV or HDF5 files depending on file ending.\"\n",
    "    \n",
    "    files = listify(files)\n",
    "    dfs=L()\n",
    "    for file in files:\n",
    "        if isinstance(file, str):\n",
    "            file = str_to_path(file)\n",
    "\n",
    "        if file.suffix == \".h5\":\n",
    "            df = read_hdf(file, key, key_metadata=key_metadata)\n",
    "        elif file.suffix == \".csv\":\n",
    "            df = read_csv(file, sep=\";\")\n",
    "        else:\n",
    "            raise f\"File ending of file {file} not supported.\"\n",
    "\n",
    "        dfs += df\n",
    "        \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n",
    "                  index=['a', 'b', 'c'])\n",
    "df.to_hdf('data.h5', key='df', mode='w')\n",
    "test_eq(df, read_files(\"data.h5\", key=\"df\")[0])\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},)\n",
    "df.to_csv('data.csv', sep=\";\")\n",
    "test_eq(df, read_files(\"data.csv\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"/home/scribbler/data/DAF_ICON_Synthetic_Wind_Power_processed/00011.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AddSeasonalFeatures(TabularProc):\n",
    "    order=10\n",
    "    def encodes(self, to):\n",
    "        to.items[\"Month\"] = to.items.index.month\n",
    "        to.items[\"Day\"] = to.items.index.day\n",
    "        to.items[\"Hour\"] = to.items.index.hour\n",
    "\n",
    "class DropYear(TabularProc):\n",
    "    \"Drops a complete year.\"\n",
    "    order = 10\n",
    "    def __init__(self, year=2020):\n",
    "        year = str(year)\n",
    "        self.year = pd.to_datetime(f\"{year}-01-01\", utc=True)\n",
    "        \n",
    "    def encodes(self, to): \n",
    "        mask = to.items.index < self.year\n",
    "        to.items.drop(to.items[mask].index, inplace=True)\n",
    "        \n",
    "class NormalizePerTask(TabularProc):\n",
    "    \"Normalize per TaskId\"\n",
    "    order = 10\n",
    "    def __init__(self, task_id_col=\"TaskID\"):\n",
    "        self.task_id_col = task_id_col\n",
    "    def setups(self, to:Tabular):\n",
    "        self.means = getattr(to, 'train', to)[to.cont_names + \"TaskID\"].groupby(\"TaskID\").mean()\n",
    "        self.stds = getattr(to, 'train', to)[to.cont_names + \"TaskID\"].groupby(\"TaskID\").std(ddof=0)+1e-7\n",
    "#         store_attr(but='to', means=dict(getattr(to, 'train', to).conts.mean()),\n",
    "#                    stds=dict(getattr(to, 'train', to).conts.std(ddof=0)+1e-7))\n",
    "#         return self(to)\n",
    "        return self(to)\n",
    "\n",
    "    def encode(self, to):\n",
    "        for task_id in to.items[self.task_id_col].unique():\n",
    "            to.conts[to.loc[:,self.task_id_col] == task_id] = ((to.conts[to.loc[:,self.task_id_col] == task_id] - self.means.loc[task_id]) / self.stds.loc[task_id]).describe()\n",
    "\n",
    "        \n",
    "class DropCols(TabularProc):\n",
    "    \"Drops rows by column name.\"\n",
    "    order = 10\n",
    "    def __init__(self, cols):\n",
    "        self.cols = listify(cols)\n",
    "        \n",
    "    def encodes(self, to): \n",
    "        to.items.drop(self.cols, axis=1, inplace=True, errors=\"ignore\")\n",
    "        \n",
    "class FilterByCol(TabularProc):\n",
    "    \"Drops rows by column.\"\n",
    "    order = 10\n",
    "    def __init__(self, col_name, keep=True, drop_col_after_filter=True):\n",
    "        self.col_name = col_name\n",
    "        self.keep = keep\n",
    "        self.drop_col_after_filter=drop_col_after_filter\n",
    "        \n",
    "    def encodes(self, to): \n",
    "        mask = to.items[self.col_name].astype(bool).values\n",
    "        if not self.keep: mask = ~mask\n",
    "        to.items.drop(to.items[mask].index, inplace=True)\n",
    "        if self.drop_col_after_filter: to.items.drop(self.col_name, axis=1, inplace=True, errors=\"ignore\")\n",
    "\n",
    "class FilterMonths(TabularProc):\n",
    "    \"Filter dataframe for specific months.\"\n",
    "    order = 10\n",
    "    def __init__(self, months=range(1,13)):\n",
    "        self.months = listify(months)\n",
    "        \n",
    "    def encodes(self, to): \n",
    "        mask = ~to.items.index.month.isin(self.months)\n",
    "        to.items.drop(to.items[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TabularRenewables(CollBase, GetAttr, FilteredBase):\n",
    "    def __init__(self, dfs, procs=None, cat_names=None, cont_names=None, \n",
    "                 y_names=None, add_y_to_x=False, add_x_to_y=False, pre_process=None, \n",
    "                 include_task_id=False, splits=None):\n",
    "        self.task_id_col = \"TaskID\"\n",
    "        self.y_names = listify(y_names)\n",
    "        self.pre_process = pre_process\n",
    "        \n",
    "        \n",
    "        self.dfs = L()\n",
    "        for task_id,df in enumerate(dfs):\n",
    "            df = TabularPandas(df, y_names=self.y_names, procs=pre_process, \n",
    "                                      do_setup=True, reduce_memory=False).items\n",
    "            df[self.task_id_col] = task_id\n",
    "            \n",
    "            self.dfs += df\n",
    "            \n",
    "        \n",
    "        self.cont_names, self.cat_names = cont_cat_split(self.dfs[0], dep_var=y_names, max_card=1000)\n",
    "        if not include_task_id: self.cat_names = [c for c in self.cat_names if c!= self.task_id_col]\n",
    "        \n",
    "        if add_y_to_x:\n",
    "            self.cont_names += self.y_names\n",
    "        if add_x_to_y:\n",
    "            self.y_names += self.cont_names\n",
    "\n",
    "        merged_df = pd.concat(self.dfs, axis=0)\n",
    "        if splits is not None: splits = splits(range_of(merged_df))\n",
    "            \n",
    "        self.to = TabularPandas(\n",
    "            merged_df,\n",
    "            procs=procs,\n",
    "            cat_names=self.cat_names,\n",
    "            cont_names=self.cont_names,\n",
    "            y_names=self.y_names,\n",
    "            splits=splits,\n",
    "            do_setup=True,\n",
    "            inplace=True,\n",
    "            y_block=RegressionBlock(),\n",
    "        )\n",
    "        super().__init__(self.to)\n",
    "        \n",
    "    def new(self, df):\n",
    "        return type(self.to)(df, do_setup=False, reduce_memory=False, y_block=TransformBlock(),\n",
    "                          **attrdict(self, 'procs','cat_names','cont_names','y_names', 'device'))\n",
    "\n",
    "    def subset(self, i): return self.to.new(self.items[slice(0,self.to.split) if i==0 else slice(self.to.split,len(self.to))])\n",
    "    def copy(self): self.items = self.to.copy(); return self\n",
    "    def decode(self): return self.to.procs.decode(self.to)\n",
    "    def decode_row(self, row): return self.to.new(pd.DataFrame(row).T).decode().items.iloc[0]\n",
    "    def show(self, max_n=10, **kwargs): self.to.show(max_n, **kwargs)\n",
    "    def setup(self): self.to.procs.setup(self)\n",
    "    def process(self): self.to.procs(self)\n",
    "    def loc(self): return self.items.loc\n",
    "    def iloc(self): return _TabIloc(self)\n",
    "    def targ(self): return self.to.items[self.y_names]\n",
    "    def x_names (self): return self.to.cat_names + self.to.cont_names\n",
    "    def n_subsets(self): return 2\n",
    "    def y(self): return self.to[self.to.y_names[0]]\n",
    "    def new_empty(self): return self.new(pd.DataFrame({}, columns=self.to.items.columns))\n",
    "    def to_device(self, d=None):\n",
    "        self.device = d\n",
    "        return self\n",
    "    \n",
    "    def procs(self):\n",
    "        return self.to.procs\n",
    "\n",
    "    def all_col_names (self):\n",
    "        ys = [n for n in self.to.y_names if n in self.to.items.columns]\n",
    "        return self.to.x_names + self.to.y_names if len(ys) == len(self.to.y_names) else self.to.x_names\n",
    "    \n",
    "properties(TabularRenewables,'loc','iloc','targ','all_col_names','n_subsets','x_names','y', \"procs\")\n",
    "\n",
    "fastai.tabular.core._add_prop(TabularRenewables, 'cat')\n",
    "fastai.tabular.core._add_prop(TabularRenewables, 'cont')\n",
    "fastai.tabular.core._add_prop(TabularRenewables, 'y')\n",
    "fastai.tabular.core._add_prop(TabularRenewables, 'x')\n",
    "fastai.tabular.core._add_prop(TabularRenewables, 'all_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\r\n",
      "Converted index.ipynb.\r\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
