{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models.autoencoders\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from fastrenewables.tabular.model import *\n",
    "from fastrenewables.timeseries.model import *\n",
    "from fastai.tabular.all import *\n",
    "from torch.autograd import Variable\n",
    "from sklearn.datasets import make_regression\n",
    "from fastai.learner import *\n",
    "from fastrenewables.utils_pytorch import *\n",
    "from fastrenewables.losses import VAEReconstructionLoss\n",
    "from blitz.utils import variational_estimator\n",
    "from fastrenewables.utils_blitz import set_train_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@variational_estimator\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        z = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "        \n",
    "    \n",
    "    def decode(self, categorical_data, continuous_data, as_np=False):\n",
    "        x = self.decoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(x)\n",
    "        else: return x\n",
    "        \n",
    "    def forward(self, categorical_data, continuous_data):\n",
    "        x = self.encode(categorical_data, continuous_data)\n",
    "        x = self.decode(categorical_data, x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, mode: bool = True):\n",
    "        super().train(mode)\n",
    "        set_train_mode(self, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some data that we wann to compress through an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2000\n",
    "X, y = make_regression(n_samples=N, n_features=20, n_informative=15)\n",
    "df = pd.DataFrame(X)\n",
    "x_names = [str(c) for c in df.columns]\n",
    "df.columns = x_names\n",
    "df[\"y\"] = (y - y.min())/(y.max()-y.min())\n",
    "dls = TabularDataLoaders.from_df(df, cont_names=x_names, y_names=x_names, deivce=\"cpu\", procs=Normalize, bs=N//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2000.000000\n",
       "mean        0.523222\n",
       "std         0.138718\n",
       "min         0.000000\n",
       "25%         0.428973\n",
       "50%         0.526034\n",
       "75%         0.616048\n",
       "max         1.000000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 20]), torch.Size([200, 20]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.one_batch()[1].shape, dls.one_batch()[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = get_c(dls)\n",
    "ann_structure = [num_features, num_features*5, 5]\n",
    "ae = Autoencoder(MultiLayerPerceptron(ann_structure), MultiLayerPerceptron(ann_structure[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, ae, metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.400510</td>\n",
       "      <td>1.059659</td>\n",
       "      <td>1.029397</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.139224</td>\n",
       "      <td>0.915602</td>\n",
       "      <td>0.956871</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.015282</td>\n",
       "      <td>0.814481</td>\n",
       "      <td>0.902486</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.939656</td>\n",
       "      <td>0.782552</td>\n",
       "      <td>0.884620</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.889630</td>\n",
       "      <td>0.762809</td>\n",
       "      <td>0.873389</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.854850</td>\n",
       "      <td>0.752478</td>\n",
       "      <td>0.867455</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.829248</td>\n",
       "      <td>0.744346</td>\n",
       "      <td>0.862755</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.809016</td>\n",
       "      <td>0.740375</td>\n",
       "      <td>0.860450</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.793158</td>\n",
       "      <td>0.737706</td>\n",
       "      <td>0.858898</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.779633</td>\n",
       "      <td>0.736026</td>\n",
       "      <td>0.857920</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast based on latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a model that is a wrapper for an autoencoder to forecast a regression or classification based on the latent space from an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@variational_estimator\n",
    "class AutoencoderForecast(nn.Module):\n",
    "    def __init__(self, autoencoder, forecast_model):\n",
    "        super().__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.forecast_model = forecast_model\n",
    "        \n",
    "    def forward(self, categorical_data, continuous_data):\n",
    "        \n",
    "        latent_space = self.autoencoder.encode(categorical_data, continuous_data)\n",
    "        yhat = self.forecast_model(categorical_data, latent_space)\n",
    "        \n",
    "        return yhat\n",
    "    \n",
    "    def train(self, mode: bool = True):\n",
    "        super().train(mode)\n",
    "        set_train_mode(self, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the data loader that has the target feature as output in the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (encoder): (\n",
      "  (Identity())\n",
      "  (ModuleList())\n",
      "  (Dropout(p=0.0, inplace=False))\n",
      "  (BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Requires grad: False\n",
      "   (layers): (\n",
      "    Sequential (0): (\n",
      "      (Linear(in_features=20, out_features=100, bias=False)) Requires grad: False\n",
      "      (ReLU(inplace=True))\n",
      "      (BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Requires grad: False\n",
      "    )\n",
      "    Sequential (1): (\n",
      "      (Linear(in_features=100, out_features=5, bias=True)) Requires grad: False\n",
      "    )\n",
      "  )\n",
      ")\n",
      " (decoder): (\n",
      "  (Identity())\n",
      "  (ModuleList())\n",
      "  (Dropout(p=0.0, inplace=False))\n",
      "  (BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Requires grad: False\n",
      "   (layers): (\n",
      "    Sequential (0): (\n",
      "      (Linear(in_features=5, out_features=100, bias=False)) Requires grad: False\n",
      "      (ReLU(inplace=True))\n",
      "      (BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Requires grad: False\n",
      "    )\n",
      "    Sequential (1): (\n",
      "      (Linear(in_features=100, out_features=20, bias=True)) Requires grad: False\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print_requires_grad(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TabularDataLoaders.from_df(df, cont_names=x_names, y_names=\"y\", deivce=\"cpu\",procs=Normalize, bs=N//10)\n",
    "mlp_regression = MultiLayerPerceptron([ann_structure[-1], get_c(dls)])\n",
    "model = AutoencoderForecast(learn.model, mlp_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AutoencoderForecast (Input shape: 200 x torch.Size([200, 20]))\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     []                  \n",
       "Identity                                                       \n",
       "BatchNorm1d                               40         False     \n",
       "____________________________________________________________________________\n",
       "                     200 x 100           \n",
       "Linear                                    2000       False     \n",
       "ReLU                                                           \n",
       "BatchNorm1d                               200        False     \n",
       "____________________________________________________________________________\n",
       "                     200 x 5             \n",
       "Linear                                    505        False     \n",
       "ReLU                                                           \n",
       "Identity                                                       \n",
       "BatchNorm1d                               10         True      \n",
       "____________________________________________________________________________\n",
       "                     200 x 1             \n",
       "Linear                                    6          True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 2,761\n",
       "Total trainable params: 16\n",
       "Total non-trainable params: 2,745\n",
       "\n",
       "Optimizer used: <function Adam at 0x7f2c55772f70>\n",
       "Loss function: FlattenedLoss of MSELoss()\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.314161</td>\n",
       "      <td>0.260491</td>\n",
       "      <td>0.510383</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.247836</td>\n",
       "      <td>0.158668</td>\n",
       "      <td>0.398332</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.199370</td>\n",
       "      <td>0.097397</td>\n",
       "      <td>0.312085</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.160442</td>\n",
       "      <td>0.061318</td>\n",
       "      <td>0.247625</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.130171</td>\n",
       "      <td>0.042172</td>\n",
       "      <td>0.205357</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5, lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UnFlatten(nn.Module):\n",
    "        \n",
    "    def forward(self, input, dims):\n",
    "        return input.view(*dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@variational_estimator\n",
    "class VariationalAutoencoder(Autoencoder):\n",
    "    def __init__(self, encoder, decoder, h_dim, z_dim):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.flatten = Flatten()\n",
    "        self.unflatten = UnFlatten()\n",
    "        \n",
    "        self.hidden2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hidden2logvar = nn.Linear(h_dim, z_dim)\n",
    "        self.latent_dimensions = None\n",
    "        self._mu, self._logvar = None, None\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        \n",
    "        x_hidden = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        self.latent_dimensions = x_hidden.shape\n",
    "        \n",
    "        x_hidden = self.flatten(x_hidden)\n",
    "        \n",
    "        mu, logvar = self.hidden2mu(x_hidden), self.hidden2logvar(x_hidden)\n",
    "        \n",
    "        # required for vae loss\n",
    "        self._mu, self._logvar = mu, logvar\n",
    "        \n",
    "        z = self.reparam(mu, logvar)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "\n",
    "    def decode(self, categorical_data, continuous_data, as_np=False, latent_dimensions=None):\n",
    "        \n",
    "        if not latent_dimensions and not self.latent_dimensions:\n",
    "            raise ValueError(\"latent_dimensions are not set to unflatten data.\")\n",
    "        if not latent_dimensions:\n",
    "            latent_dimensions = self.latent_dimensions\n",
    "            \n",
    "        x = self.unflatten(continuous_data, latent_dimensions)\n",
    "        \n",
    "        x = self.decoder(categorical_data, x)\n",
    "        \n",
    "        if as_np: return to_np(x)\n",
    "        else: return x\n",
    "        \n",
    "    def get_posteriors(self, categorical_data, continuous_data):\n",
    "\n",
    "        return self.encode(continuous_data, categorical_data)\n",
    "\n",
    "    def get_z(self, categorical_data, continuous_data):\n",
    "        \"\"\"Encode a batch of data points, x, into their z representations.\"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(categorical_data, continuous_data)\n",
    "        \n",
    "        return self.reparam(mu, logvar)\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        \"\"\"Reparameterisation trick to sample z values.\n",
    "        This is stochastic during training, and returns the mode during evaluation.\"\"\"\n",
    "\n",
    "        if self.training:\n",
    "            # convert logarithmic variance to standard deviation representation\n",
    "            std = torch.exp(logvar / 2)\n",
    "            \n",
    "            # create normal distribution as large as the data\n",
    "            eps = torch.randn_like(std)\n",
    "            # scale by learned mean and standard deviation\n",
    "            return mu + eps*std\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super().train(mode)\n",
    "        set_train_mode(self, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TabularDataLoaders.from_df(df, cont_names=x_names, y_names=x_names, deivce=\"cpu\", procs=Normalize, bs=N//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = get_c(dls)\n",
    "ann_structure = [num_features, num_features*5, 5]\n",
    "ae = VariationalAutoencoder(MultiLayerPerceptron(ann_structure), \n",
    "                            MultiLayerPerceptron(ann_structure[::-1]), \n",
    "                            ann_structure[-1], ann_structure[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, ae, loss_func=VAEReconstructionLoss(ae), metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.625394</td>\n",
       "      <td>1.027008</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.353855</td>\n",
       "      <td>1.009958</td>\n",
       "      <td>0.981047</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.232008</td>\n",
       "      <td>0.987417</td>\n",
       "      <td>0.979559</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.164160</td>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.970965</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.120605</td>\n",
       "      <td>0.967925</td>\n",
       "      <td>0.961196</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.089931</td>\n",
       "      <td>0.962036</td>\n",
       "      <td>0.962075</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.068224</td>\n",
       "      <td>0.961635</td>\n",
       "      <td>0.958414</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.051640</td>\n",
       "      <td>0.960990</td>\n",
       "      <td>0.960682</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.038519</td>\n",
       "      <td>0.955296</td>\n",
       "      <td>0.958315</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.028276</td>\n",
       "      <td>0.957994</td>\n",
       "      <td>0.960198</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast based on latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TabularDataLoaders.from_df(df, cont_names=x_names, y_names=\"y\", deivce=\"cpu\",procs=Normalize, bs=N//10)\n",
    "mlp_regression = MultiLayerPerceptron([ann_structure[-1], get_c(dls)])\n",
    "model = AutoencoderForecast(learn.model, mlp_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AutoencoderForecast (Input shape: 200 x torch.Size([200, 20]))\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     []                  \n",
       "Identity                                                       \n",
       "BatchNorm1d                               40         False     \n",
       "____________________________________________________________________________\n",
       "                     200 x 100           \n",
       "Linear                                    2000       False     \n",
       "ReLU                                                           \n",
       "BatchNorm1d                               200        False     \n",
       "____________________________________________________________________________\n",
       "                     200 x 5             \n",
       "Linear                                    505        False     \n",
       "ReLU                                                           \n",
       "Flatten                                                        \n",
       "Linear                                    30         False     \n",
       "Linear                                    30         False     \n",
       "Identity                                                       \n",
       "BatchNorm1d                               10         True      \n",
       "____________________________________________________________________________\n",
       "                     200 x 1             \n",
       "Linear                                    6          True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 2,821\n",
       "Total trainable params: 16\n",
       "Total non-trainable params: 2,805\n",
       "\n",
       "Optimizer used: <function Adam at 0x7f2c55772f70>\n",
       "Loss function: FlattenedLoss of MSELoss()\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, model, metrics=rmse)\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.655513</td>\n",
       "      <td>0.409079</td>\n",
       "      <td>0.639593</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.527697</td>\n",
       "      <td>0.295510</td>\n",
       "      <td>0.543608</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432311</td>\n",
       "      <td>0.213990</td>\n",
       "      <td>0.462591</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.358125</td>\n",
       "      <td>0.138118</td>\n",
       "      <td>0.371642</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.297741</td>\n",
       "      <td>0.079909</td>\n",
       "      <td>0.282682</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5, lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks that the autoencoders also work with temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = 24\n",
    "n_features = 10\n",
    "n_samples = 3\n",
    "latent_dim  = 2 \n",
    "ann_structure = [10, latent_dim]\n",
    "\n",
    "x = torch.randn(( n_samples, n_features,ts_length), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_tcn = Autoencoder(TemporalCNN(ann_structure), TemporalCNN(ann_structure[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = ae_tcn(None, x)\n",
    "\n",
    "test_eq(True, yhat.requires_grad)\n",
    "test_eq([n_samples, n_features, ts_length], list(yhat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_tcn = VariationalAutoencoder(TemporalCNN(ann_structure), \n",
    "                                TemporalCNN(ann_structure[::-1]),\n",
    "                               ann_structure[-1]*ts_length, ann_structure[-1]*ts_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = vae_tcn(None, x)\n",
    "\n",
    "test_eq(True, yhat.requires_grad)\n",
    "test_eq([n_samples, n_features, ts_length], list(yhat.shape))\n",
    "test_eq([n_samples, latent_dim*ts_length], list(vae_tcn._mu.shape))\n",
    "test_eq([n_samples, latent_dim*ts_length], list(vae_tcn._logvar.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
