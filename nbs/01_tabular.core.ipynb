{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tabular.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "import pandas as pd\n",
    "from fastai.data.external import *\n",
    "from fastcore.all import *\n",
    "from pathlib import PosixPath\n",
    "from fastcore.test import *\n",
    "from fastai.tabular.all import *\n",
    "import fastai\n",
    "from fastai.tabular.core import _maybe_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers to read files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def str_to_path(file: str):\n",
    "    \"Convers a string to a Posixpath.\"\n",
    "    if isinstance(file, str) and \"~\" in file:\n",
    "        file = os.path.expanduser(file)\n",
    "\n",
    "    file = Path(file)\n",
    "\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fastai.tabular.core.df_shrink_dtypes(df, skip=[], obj2cat=True, int2uint=False)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shrink_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq_type(Path(\"\"), str_to_path(\"\"))\n",
    "test_eq_type(Path(\"\"), str_to_path(Path(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_hdf(file:PosixPath, key: str = \"/powerdata\", key_metadata=None):\n",
    "    \"Reads a hdf5 table based on the given key.\"\n",
    "    file = str_to_path(file)\n",
    "    if \"/\" not in key: key = \"/\" + key\n",
    "    with pd.HDFStore(file, \"r\") as store:\n",
    "        if key in store.keys():\n",
    "            df = store[key]\n",
    "            if key_metadata is not None:\n",
    "                df_meta = store[key_metadata]\n",
    "                for c in df_meta: df[c] = df_meta[c].values[0]\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n",
    "                  index=['a', 'b', 'c'])\n",
    "df.to_hdf('data.h5', key='df', mode='w')\n",
    "test_eq(df, read_hdf(\"data.h5\", key=\"df\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_csv(file:PosixPath, sep:str =\";\"):\n",
    "    \"Reads a csv file.\"\n",
    "    file = str_to_path(file)\n",
    "    df = pd.read_csv(str(file), sep=sep)\n",
    "    df.drop([\"Unnamed: 0\"], inplace=True, axis=1, errors=\"ignore\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},)\n",
    "df.to_csv('data.csv', sep=\";\")\n",
    "test_eq(df, read_csv(\"data.csv\", sep=\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_files(\n",
    "    files:PosixPath,\n",
    "    key:str =\"/powerdata\",\n",
    "    key_metadata=None,\n",
    "    sep:str=\";\",\n",
    "    add_task_id=True\n",
    ") -> pd.DataFrame:\n",
    "    \"Reads a number of CSV or HDF5 files depending on file ending.\"\n",
    "\n",
    "    files = listify(files)\n",
    "    dfs=L()\n",
    "    for task_id,file in enumerate(files):\n",
    "        if isinstance(file, str):\n",
    "            file = str_to_path(file)\n",
    "\n",
    "        if file.suffix == \".h5\":\n",
    "            df = read_hdf(file, key, key_metadata=key_metadata)\n",
    "        elif file.suffix == \".csv\":\n",
    "            df = read_csv(file, sep=\";\")\n",
    "        else:\n",
    "            raise f\"File ending of file {file} not supported.\"\n",
    "        if add_task_id:df[\"TaskID\"]=task_id\n",
    "        dfs += df\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n",
    "                  index=['a', 'b', 'c'])\n",
    "df.to_hdf('data.h5', key='df', mode='w')\n",
    "test_eq(df, read_files(\"data.h5\", key=\"df\", add_task_id=False)[0])\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},)\n",
    "df.to_csv('data.csv', sep=\";\")\n",
    "test_eq(df, read_files(\"data.csv\", add_task_id=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# this is merely a class to differentiate between fastai processing and renewbale pre-processing functionality\n",
    "class RenewablesTabularProc(TabularProc):\n",
    "    include_in_new=False\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestProc(RenewablesTabularProc): pass\n",
    "test_eq(isinstance(TestProc(), RenewablesTabularProc), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CreateTimeStampIndex(RenewablesTabularProc):\n",
    "    order=0\n",
    "    include_in_new=True\n",
    "    def __init__(self, col_name, offset_correction=None):\n",
    "        self.col_name = col_name\n",
    "        self.offset_correction = offset_correction\n",
    "\n",
    "    def encodes(self, to):\n",
    "        df = to.items\n",
    "        \n",
    "        def create_timestamp_index(df, drop_index=True):\n",
    "            df.reset_index(drop=drop_index, inplace=True)\n",
    "            df.rename({self.col_name: \"TimeUTC\"}, axis=1, inplace=True)\n",
    "            #  in case the timestamp is index give it a proper timestamp,e.g., in GermanSolarFarm dataset\n",
    "            if \"0000-\" in str(df.TimeUTC[0]):\n",
    "                df.TimeUTC = df.TimeUTC.apply(\n",
    "                    lambda x: x.replace(\"0000-\", \"2015-\").replace(\"0001-\", \"2016-\")\n",
    "                )\n",
    "            df.TimeUTC = pd.to_datetime(df.TimeUTC, infer_datetime_format=True, utc=True)\n",
    "            df.set_index(\"TimeUTC\", inplace=True)\n",
    "            df.index = df.index.rename(\"TimeUTC\")\n",
    "\n",
    "            #  for GermanSolarFarm, the index is not corret. Should have a three hour resolution but is one...\n",
    "            if self.offset_correction is not None:\n",
    "                i, new_index = 0, []\n",
    "                for cur_index in df.index:\n",
    "                    new_index.append(cur_index + pd.DateOffset(hours=i))\n",
    "                    i += self.offset_correction\n",
    "                df.index = new_index\n",
    "\n",
    "        if self.col_name in df.columns:\n",
    "            create_timestamp_index(df, drop_index=True)\n",
    "        # properly already processed\n",
    "        elif self.col_name == to.items.index.name:  \n",
    "            create_timestamp_index(df, drop_index=False)\n",
    "        else:\n",
    "            warnings.warn(f\"Timetamps column {self.col_name} not in columns {df.columns} or df.index.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(index=None):\n",
    "    df = pd.DataFrame(index=range(0,5), columns = ['A', 'B', 'C'] ).fillna(0)\n",
    "    if index is not None: df[\"TimeStamps\"] = index\n",
    "    return TabularPandas(df)\n",
    "# tests basic functionality to set a proper timestamp based index\n",
    "index = ['2015-01-01-01', '2015-01-01-02', '2015-01-02-03', '2015-02-01-23', '2015-02-01-13'] \n",
    "to = get_test_data(index)\n",
    "test_eq(CreateTimeStampIndex(col_name=\"TimeStamps\")(to).items.index, pd.to_datetime(index, utc=True))\n",
    "\n",
    "# corrects missing year\n",
    "index_missing_year = ['0000-01-01-01', '0000-01-01-02', '0000-01-02-03', '0000-02-01-23', '0000-02-01-13'] \n",
    "to = get_test_data(index_missing_year)\n",
    "test_eq(CreateTimeStampIndex(col_name=\"TimeStamps\")(to).items.index, pd.to_datetime(index, utc=True))\n",
    "\n",
    "# check if warning is triggered, due to wrong column name\n",
    "to = get_test_data(index)\n",
    "test_call = lambda: CreateTimeStampIndex(col_name=\"FalseColumnName\")(to)\n",
    "test_warns(test_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_samples_per_day(df):\n",
    "    \"\"\"\n",
    "    Extract the amount of entries per day from the DataFrame\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        the DataFrame used for the conversion.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    integer\n",
    "        amount of entries per day.\n",
    "    \"\"\"\n",
    "    samples_per_day = -1\n",
    "    \n",
    "    if len(df) == 0: return samples_per_day\n",
    "    mins = 0\n",
    "    for i in range(1, 10):\n",
    "        mins = (df.index[-i] - df.index[-(i + 1)]).seconds // 60\n",
    "        # 15 min resolution\n",
    "        if mins == 15:\n",
    "            samples_per_day = 24 * 4\n",
    "            break\n",
    "        # hourly resolution\n",
    "        elif mins == 60:\n",
    "            samples_per_day = 24\n",
    "            break\n",
    "        # three hour resolution\n",
    "        elif mins == 180:\n",
    "            samples_per_day = 8\n",
    "            break\n",
    "    if samples_per_day == -1:\n",
    "        raise ValueError(f\"{mins} is an unknown sampling time.\")\n",
    "\n",
    "    return samples_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_samples_per_day(index):\n",
    "    return pd.DataFrame(index=pd.to_datetime(index),\n",
    "                      columns = ['A', 'B', 'C'] ).fillna(0)\n",
    "df = test_data_samples_per_day(index = pd.to_datetime(['2018-01-01-00:00', '2019-01-01-04:00', '2020-01-01-07:00',] ))\n",
    "test_eq(8, get_samples_per_day(df))\n",
    "df = test_data_samples_per_day(index = pd.to_datetime(['2018-01-01-00:00', '2019-01-01-02:00', '2020-01-01-03:00',] ))\n",
    "test_eq(24, get_samples_per_day(df))\n",
    "df = test_data_samples_per_day(index = pd.to_datetime(['2018-01-01-00:00', '2019-01-01-01:15', '2020-01-01-01:30',] ))\n",
    "test_eq(96, get_samples_per_day(df))\n",
    "test_eq(-1, get_samples_per_day(test_data_samples_per_day([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _interpolate_df(df, sample_time=\"15Min\", limit=5, drop_na=False):\n",
    "        df = df[~df.index.duplicated()]\n",
    "        upsampled = df.resample(sample_time)\n",
    "        df  = upsampled.interpolate(method=\"linear\", limit=limit)\n",
    "        \n",
    "        if drop_na: df = df.dropna(axis=0)\n",
    "\n",
    "        if \"Hour\" in df.columns:\n",
    "            df[\"Hour\"] = df.index.hour\n",
    "        if \"Month\" in df.columns:\n",
    "            df[\"Month\"] = df.index.month\n",
    "        if \"Day\" in df.columns:\n",
    "            df[\"Day\"] = df.index.day\n",
    "        if \"Week\" in df.columns:\n",
    "            df[\"Week\"] = df.index.week\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_interpolate(index=pd.to_datetime(['2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-03:00'])):\n",
    "    np.random.seed(2)\n",
    "    df = pd.DataFrame(index=pd.to_datetime(index),\n",
    "                      data=np.random.randint(0,10,size=(len(index),3)),\n",
    "                  columns = ['A', 'B', 'C'] )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_data_interpolate()\n",
    "inp_df = _interpolate_df(df, limit=5, drop_na=True)\n",
    "# instead of three values per hours we now have four plus the last timesamp\n",
    "# duplicated values are droppped\n",
    "test_eq(9, inp_df.shape[0])\n",
    "test_eq(2, inp_df.iloc[-2,0])\n",
    "# one for every timestamp plus one for the first two timestamps, assure that duplicates are dropped\n",
    "df = test_data_interpolate(index = pd.to_datetime(['2018-01-01-01:00', '2018-01-01-01:00', '2018-01-01-03:00', '2018-01-01-04:00']),)\n",
    "test_eq(5, _interpolate_df(df, limit=1, drop_na=True).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _apply_group_by(to:pd.DataFrame, group_by_col, func, **kwargs):\n",
    "    if group_by_col in to.columns:\n",
    "        dfs = L()\n",
    "        for k,df_g in to.groupby(group_by_col):\n",
    "            dfs += func(df_g, **kwargs)\n",
    "        df = pd.concat(dfs, axis=0)\n",
    "    else:\n",
    "        df = func(to, **kwargs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Interpolate(RenewablesTabularProc):\n",
    "    order=50\n",
    "    include_in_new=True\n",
    "    def __init__(self, sample_time = \"15Min\", limit=5, drop_na=True, group_by_col=\"TaskID\"):\n",
    "        self.sample_time = sample_time\n",
    "        self.limit = limit\n",
    "        self.drop_na = drop_na\n",
    "        self.group_by_col = group_by_col\n",
    "    \n",
    "    def setups(self, to: Tabular):\n",
    "        self.n_samples_per_day = get_samples_per_day(to.items)\n",
    "        if self.n_samples_per_day == -1:\n",
    "            warnings.warn(\"Could not determine samples per day. Skip processing.\")\n",
    "    \n",
    "    def encodes(self, to):\n",
    "        # if values of a columns are the same in each row (categorical features)\n",
    "        # we make that those stay the same during interpolation\n",
    "        if self.group_by_col in to.items.columns:\n",
    "            d = defaultdict(object)\n",
    "            non_unique_columns = L()\n",
    "            for group_id, df in to.items.groupby(self.group_by_col):\n",
    "                for c in df.columns:\n",
    "                    if len(df[c].unique())==1 and c!=self.group_by_col:\n",
    "                        d[(group_id,c)] = df[c][0]\n",
    "                    else:\n",
    "                        non_unique_columns += c\n",
    "\n",
    "            if self.n_samples_per_day == -1: return\n",
    "            non_unique_columns = np.unique(non_unique_columns)\n",
    "        else:\n",
    "            non_unique_columns = to.items.columns\n",
    "        # interpolate non unique columns         \n",
    "        df = _apply_group_by(to.items.loc[:,np.unique(non_unique_columns)], self.group_by_col, _interpolate_df)\n",
    "        to.items = df\n",
    "        if self.group_by_col in to.items.columns:\n",
    "            for group_id,col_name in d.keys():\n",
    "                mask = to[self.group_by_col]==group_id\n",
    "                to.items.loc[mask,col_name]=d[(group_id, col_name)] \n",
    "        \n",
    "\n",
    "        if len(to.cont_names)>0:\n",
    "            mask = to[to.cont_names].isna().values[:,0]\n",
    "            to.items = to.items[~mask]\n",
    "        # pandas converts the datatype to float if np.NaN is present, lets revert that                \n",
    "        to.items = to.items.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-07345a38428e>:14: UserWarning: Could not determine samples per day. Skip processing.\n",
      "  warnings.warn(\"Could not determine samples per day. Skip processing.\")\n"
     ]
    }
   ],
   "source": [
    "df = test_data_interpolate(index = pd.to_datetime(['2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-03:00']),)\n",
    "df[\"TaskID\"] = [1,1,2,2,2]\n",
    "to = TabularPandas(df, procs=Interpolate, do_setup=True)\n",
    "test_eq(5, to.items.loc[to.items.TaskID==1].shape[0])\n",
    "test_eq(9, to.items.loc[to.items.TaskID==2].shape[0])\n",
    "\n",
    "df = test_data_interpolate(index = pd.to_datetime(['2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-03:00']),)\n",
    "df[\"TaskID\"] = [1,1,2,2,2]\n",
    "df[\"B\"] = [1,1,2,2,2]\n",
    "to = TabularPandas(df, cont_names=\"A\", procs=Interpolate, do_setup=True)\n",
    "test_eq([1,1,1,1,1,2,2,2,2,2,2,2,2,2], list(to.items.B))\n",
    "test_eq(9, to.items.loc[to.items.TaskID==2].shape[0])\n",
    "\n",
    "df = test_data_interpolate(index = pd.to_datetime(['2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-03:00']),)\n",
    "to = TabularPandas(df,  cont_names=\"A\", procs=Interpolate, do_setup=True)\n",
    "test_eq(9, to.items.shape[0])\n",
    "\n",
    "df = test_data_interpolate(index = [])\n",
    "to = TabularPandas(df, procs=Interpolate, do_setup=True)\n",
    "test_eq(0, to.items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _create_consistent_number_of_sampler_per_day(\n",
    "    df: pd.DataFrame, n_samples_per_day: int = 24\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove days with less than the specified amount of samples from the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        the DataFrame used for the conversion.\n",
    "    n_samples_per_day : integer\n",
    "        the amount of samples each day in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        the given DataFrame, now with a consistent amount of samples each day.\n",
    "    \"\"\"\n",
    "    sample_col = df.columns[0]\n",
    "    # Create a list of booleans, where each day with 'less than n_samples_per_day' samples is denoted with 'True'\n",
    "    mask = df.resample(\"D\").apply(len)[sample_col]\n",
    "    mask = (mask < n_samples_per_day) & (mask > 0)\n",
    "\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i]:\n",
    "            new_day = mask.index[i] + pd.DateOffset(days=1)\n",
    "            new_day.hours = 0\n",
    "\n",
    "            cur_mask = (df.index < mask.index[i]) | (df.index >= new_day)\n",
    "            df = df[cur_mask]\n",
    "\n",
    "    mask = df.resample(\"D\").apply(len)[sample_col]\n",
    "    mask = (mask < n_samples_per_day) & (mask > 0)\n",
    "\n",
    "    if mask.sum() != 0:\n",
    "        raise ValueError(\"Wrong sample frequency.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 15:00:00</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 18:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 21:00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02 00:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02 03:00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     A  B  C\n",
       "2018-01-01 15:00:00  4  3  7\n",
       "2018-01-01 18:00:00  6  1  3\n",
       "2018-01-01 21:00:00  5  8  4\n",
       "2018-01-02 00:00:00  6  3  9\n",
       "2018-01-02 03:00:00  2  0  4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.date_range(start='1/1/2018', periods=10, freq='3H')\n",
    "df = test_data_interpolate(index)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _create_consistent_number_of_sampler_per_day(df, n_samples_per_day= get_samples_per_day(df))\n",
    "# last two rows are removed, as they are not a \"complete day\"\n",
    "test_eq(8, df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterInconsistentSamplesPerDay(RenewablesTabularProc):\n",
    "    order=100  \n",
    "    include_in_new=True\n",
    "    def __init__(self, group_by_col=\"TaskID\"):\n",
    "        self.group_by_col = group_by_col\n",
    "        \n",
    "    def setups(self, to: Tabular):\n",
    "        self.n_samples_per_day = get_samples_per_day(to.items)\n",
    "    \n",
    "    def encodes(self, to):\n",
    "        to.items = _apply_group_by(to.items, self.group_by_col, _create_consistent_number_of_sampler_per_day, \n",
    "                        n_samples_per_day=self.n_samples_per_day)\n",
    "        \n",
    "        assert (to.items.shape[0]%self.n_samples_per_day) == 0, \"Incorrect number of samples after filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(True, FilterInconsistentSamplesPerDay.order > Interpolate.order)\n",
    "index = pd.date_range(start='1/1/2018', periods=30, freq='3H')\n",
    "df = test_data_interpolate(list(index))\n",
    "df[\"TaskID\"] = [1 if i<11 else 2 for i in range(len(df))]\n",
    "to  = TabularPandas(df, procs=FilterInconsistentSamplesPerDay, do_setup=True)\n",
    "# equal to two days with eight samples per day\n",
    "test_eq(16, to.items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 03:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 06:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 09:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 12:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 15:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 18:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 21:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03 03:00:00</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AddSeasonalFeatures(RenewablesTabularProc):\n",
    "    order=0\n",
    "    include_in_new=True\n",
    "    def __init__(self, as_cont=True):\n",
    "        self.as_cont = as_cont\n",
    "    \n",
    "    def encodes(self, to):\n",
    "        as_sin = lambda value, max_value: np.sin(2*np.pi*value/max_value)\n",
    "        as_cos = lambda value, max_value: np.cos(2*np.pi*value/max_value)\n",
    "        \n",
    "        if self.as_cont:\n",
    "            to.items[\"MonthSin\"] = as_sin(to.items.index.month, 12)\n",
    "            to.items[\"MonthCos\"] = as_cos(to.items.index.month, 12)\n",
    "            to.items[\"DaySin\"] = as_sin(to.items.index.day, 31)\n",
    "            to.items[\"DayCos\"] = as_cos(to.items.index.day, 31)\n",
    "            to.items[\"HourSin\"] = as_sin(to.items.index.hour, 24)\n",
    "            to.items[\"HourCos\"] = as_cos(to.items.index.hour, 24)\n",
    "            \n",
    "        else:\n",
    "            to.items[\"Month\"] = to.items.index.month\n",
    "            to.items[\"Day\"] = to.items.index.day\n",
    "            to.items[\"Hour\"] = to.items.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = get_test_data(index=['2018-01-01-01', '2018-01-01-02', '2018-01-02-03', '2018-02-01-23', '2018-02-01-13'] )\n",
    "CreateTimeStampIndex(\"TimeStamps\")(to)\n",
    "AddSeasonalFeatures(as_cont=False)(to)\n",
    "test_eq(np.array([1,1,1,2,2]), to.items.Month.values)\n",
    "test_eq(np.array([1,1,2,1,1]), to.items.Day.values)\n",
    "test_eq(np.array([1,2,3,23,13]), to.items.Hour.values)\n",
    "# TODO test sin/cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterByCol(RenewablesTabularProc):\n",
    "    \"Drops rows by column.\"\n",
    "    order = 9\n",
    "    def __init__(self, col_name, drop=True, drop_col_after_filter=True):\n",
    "        self.col_name = col_name\n",
    "        self.drop = drop\n",
    "        self.drop_col_after_filter=drop_col_after_filter\n",
    "\n",
    "    def encodes(self, to):\n",
    "        mask = to.items[self.col_name].astype(bool).values\n",
    "        if not self.drop: mask = ~mask\n",
    "        to.items.drop(to.items[mask].index, inplace=True)\n",
    "        if self.drop_col_after_filter: to.items.drop(self.col_name, axis=1, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = get_test_data()\n",
    "to.loc[:,\"C\"] = [0,0,1,1,0]\n",
    "FilterByCol(col_name=\"C\", drop_col_after_filter=True, drop=True)(to)\n",
    "test_eq(list(to.items.index),[0,1,4])\n",
    "test_eq(to.items.columns,[\"A\",\"B\"])\n",
    "\n",
    "to = get_test_data()\n",
    "to.loc[:,\"C\"] = [0,0,1,1,0]\n",
    "FilterByCol(col_name=\"C\", drop_col_after_filter=False, drop=False)(to)\n",
    "test_eq(list(to.items.index),[2,3])\n",
    "test_eq(to.items.columns,[\"A\",\"B\", \"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterYear(RenewablesTabularProc):\n",
    "    \"Filter a list of years. By default the years are dropped.\"\n",
    "    order = 9\n",
    "    def __init__(self, year, drop=True):\n",
    "        \"year(s) to filter, whether to drop or keep the years.\"\n",
    "        year = listify(year)\n",
    "        self.year = L(int(y) for y in year)\n",
    "        self.drop = drop\n",
    "\n",
    "    def encodes(self, to):\n",
    "        mask = None\n",
    "        for y in self.year:\n",
    "            cur_mask = to.items.index.year == y\n",
    "            if mask is None: mask = cur_mask\n",
    "            else: mask = mask | cur_mask\n",
    "\n",
    "        if not self.drop: mask = ~mask\n",
    "        to.items.drop(to.items[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_filter_year():\n",
    "    index = ['2018-01-01-01', '2019-01-01-02', '2020-01-02-03',] \n",
    "    return TabularPandas(pd.DataFrame(index=pd.to_datetime(index),\n",
    "                      columns = ['A', 'B', 'C'] ).fillna(0))\n",
    "    \n",
    "to = test_data_filter_year()\n",
    "FilterYear(year=2018)(to)\n",
    "test_eq(np.array([2019,2020]), to.items.index.year)\n",
    "to = test_data_filter_year()\n",
    "FilterYear(year=2020, drop=False)(to)\n",
    "test_eq(np.array([2020]), to.items.index.year)\n",
    "to = test_data_filter_year()\n",
    "FilterYear(year=[2018,2020], drop=True)(to)\n",
    "test_eq(np.array([2019]), to.items.index.year)\n",
    "to = test_data_filter_year()\n",
    "FilterYear(year=[2018,2020], drop=False)(to)\n",
    "test_eq(np.array([2018,2020]), to.items.index.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterMonths(RenewablesTabularProc):\n",
    "    \"Filter dataframe for specific months.\"\n",
    "    order = 9\n",
    "    def __init__(self, months=range(1,13), drop=False):\n",
    "        self.months = listify(months)\n",
    "        self.drop = drop\n",
    "\n",
    "    def encodes(self, to):\n",
    "        mask = to.items.index.month.isin(self.months)\n",
    "        if not self.drop: mask = ~mask\n",
    "        to.items.drop(to.items[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_filter_month():\n",
    "    to = get_test_data(index=['2018-01-01-01', '2018-02-01-02', '2018-03-02-03', '2018-04-01-23', '2018-05-01-13'])\n",
    "    CreateTimeStampIndex(\"TimeStamps\")(to)\n",
    "    return to\n",
    "\n",
    "def test_filter_month(months,drop,expected_result):\n",
    "    to = get_test_data_filter_month()\n",
    "    FilterMonths(months,drop)(to)\n",
    "    test_eq(to.items.index.month, expected_result)\n",
    "    \n",
    "test_filter_month([1,2], False, [1,2])\n",
    "test_filter_month(range(1,3), False, [1,2])\n",
    "test_filter_month([1], False, [1])\n",
    "test_filter_month([1,2], True, [3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterDays(RenewablesTabularProc):\n",
    "    \"Filter dataframe for specific months.\"\n",
    "    order = 10\n",
    "    def __init__(self, num_days):\n",
    "        self.num_days = num_days\n",
    "        \n",
    "    def setups(self, to: Tabular):\n",
    "        self.n_samples_per_day = get_samples_per_day(to.items)\n",
    "        \n",
    "    def encodes(self, to):\n",
    "        to.items = to.items[-(self.n_samples_per_day * self.num_days):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(True, FilterYear.order<FilterDays.order)\n",
    "test_eq(True, FilterMonths.order<FilterDays.order)\n",
    "test_eq(True, FilterByCol.order<FilterDays.order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DropCols(RenewablesTabularProc):\n",
    "    \"Drops rows by column name.\"\n",
    "    order = 10\n",
    "    def __init__(self, cols):\n",
    "        self.cols = listify(cols)\n",
    "\n",
    "    def encodes(self, to):\n",
    "        to.items.drop(self.cols, axis=1, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = get_test_data()\n",
    "DropCols(None)(to)\n",
    "test_eq(to.items.columns, [\"A\", \"B\", \"C\"])\n",
    "to = get_test_data()\n",
    "DropCols([])(to)\n",
    "test_eq(to.items.columns, [\"A\", \"B\", \"C\"])\n",
    "to = get_test_data()\n",
    "DropCols([\"C\"])(to)\n",
    "test_eq(to.items.columns, [\"A\", \"B\"])\n",
    "to = get_test_data()\n",
    "DropCols([\"A\", \"B\"])(to)\n",
    "test_eq(to.items.columns, [\"C\"])\n",
    "to = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Normalize(RenewablesTabularProc):\n",
    "    \"Normalize per TaskId\"\n",
    "    order = 1\n",
    "    include_in_new=True\n",
    "    def __init__(self, cols_to_ignore=[]):\n",
    "        self.cols_to_ignore = cols_to_ignore\n",
    "\n",
    "    def setups(self, to: Tabular):\n",
    "        self.rel_cols = [c for c in to.cont_names if c not in self.cols_to_ignore]\n",
    "        self.means = getattr(to, \"train\", to)[self.rel_cols].mean()\n",
    "        self.stds = getattr(to, \"train\", to)[self.rel_cols].std(ddof=0) + 1e-7\n",
    "\n",
    "    def encodes(self, to):\n",
    "        to.loc[:, self.rel_cols] = (to.loc[:, self.rel_cols] - self.means) / self.stds\n",
    "\n",
    "    def decodes(self, to):\n",
    "        to.loc[:, self.rel_cols] = to.loc[:, self.rel_cols] * self.stds + self.means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we assume, that we have NWP as input features, we can always extract the past. \n",
    "Therefore, we should normalize before filtering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(True, Normalize.order<FilterMonths.order)\n",
    "test_eq(True, Normalize.order<FilterByCol.order)\n",
    "test_eq(True, Normalize.order<FilterYear.order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BinFeatures(TabularProc):\n",
    "    \"Creates bin from categorical features.\"\n",
    "    order = 1\n",
    "    include_in_new=True\n",
    "    def __init__(self, column_names, bin_sizes=5):\n",
    "        # TODO: Add possiblitiy to add custom bins\n",
    "        self.column_names = listify(column_names)\n",
    "        self.bin_sizes = listify(bin_sizes)\n",
    "        if len(self.bin_sizes) == 1: self.bin_sizes = L(self.bin_sizes[0] for _ in self.column_names)\n",
    "\n",
    "    def setups(self, to:Tabular):\n",
    "        train_to = getattr(to, 'train', to)\n",
    "        self.bin_edges = {c:pd.qcut(train_to.items[c], q=bs, retbins=True)[1] for c,bs in zip(self.column_names,self.bin_sizes)}\n",
    "\n",
    "\n",
    "    def encodes(self, to):\n",
    "        for c in self.bin_edges.keys():\n",
    "            to.items.loc[:,c] = pd.cut(to.items[c], bins=self.bin_edges[c],\n",
    "                                       labels=range(1, len(self.bin_edges[c])),\n",
    "                                       include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test corner cases of minimum and maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n",
    "df.iloc[0,:] = 0\n",
    "df.iloc[-1,:] = 100\n",
    "to = TabularPandas(df, cont_names=[\"A\", \"B\", \"C\"], y_names=\"D\", procs=BinFeatures(column_names=[\"A\", \"B\", \"C\"]))\n",
    "test_eq(to.items.iloc[-1,:][[\"A\", \"B\", \"C\"]].values, [5,5,5])\n",
    "test_eq(to.items.iloc[0,:][[\"A\", \"B\", \"C\"]].values, [1,1,1])\n",
    "# Test for nas.\n",
    "test_eq(to.items.isna().sum().sum(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if it is also works along with categorify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n",
    "df.iloc[0,:] = 0\n",
    "df.iloc[-1,:] = 100\n",
    "to = TabularPandas(df, cont_names=[\"A\", \"B\", \"C\"], y_names=\"D\", procs=[BinFeatures(column_names=[\"A\", \"B\", \"C\"]), Categorify()])\n",
    "test_eq(to.items.iloc[-1,:][[\"A\", \"B\", \"C\"]].values, [5,5,5])\n",
    "test_eq(to.items.iloc[0,:][[\"A\", \"B\", \"C\"]].values, [1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension of TabularPandas with pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def _add_prop(cls, o):\n",
    "    setattr(cls, camel2snake(o.__class__.__name__), o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# class TabularRenewables(TabularPandas):\n",
    "#     def __init__(self, dfs, procs=None, cat_names=None, cont_names=None, do_setup=True, reduce_memory=False,\n",
    "#                  y_names=None, add_y_to_x=False, add_x_to_y=False, pre_process=None, device=None, splits=None, y_block=RegressionBlock()):\n",
    "\n",
    "#         self.pre_process = listify(pre_process)\n",
    "#         cont_names = listify(cont_names)\n",
    "#         cat_names = listify(cat_names)\n",
    "#         y_names = listify(y_names)\n",
    "#         procs = listify(procs)\n",
    "        \n",
    "#         # TODO: add_y_to_x, add_x_to_y? can also achieved through cont_names and y_names\n",
    "\n",
    "#         for pp in procs:\n",
    "#             if isinstance(pp, RenewablesTabularProc):\n",
    "#                 warnings.warn(f\"Element {pp} of procs is RenewablesTabularProc, might not work with TabularPandas.\")\n",
    "                \n",
    "        \n",
    "\n",
    "#         if len(self.pre_process) > 0:\n",
    "#             self.prepared_to = TabularPandas(dfs, y_names=y_names, \n",
    "#                                              procs=self.pre_process, \n",
    "#                                              cont_names=cont_names,\n",
    "# #                                              cat_names=cat_names,\n",
    "#                                              y_block=y_block,\n",
    "#                                              do_setup=True, \n",
    "#                                              reduce_memory=False)\n",
    "#             self.pre_process = self.prepared_to.procs\n",
    "#             prepared_df = self.prepared_to.items\n",
    "#             for pp in self.pre_process: \n",
    "#                 if getattr(pp, \"include_in_new\", False): _add_prop(self, pp)\n",
    "                    \n",
    "#             print(prepared_df.PowerGeneration)\n",
    "#             new_cat_names = [c for c in cat_names if c in prepared_df.columns ]\n",
    "#             new_cont_names = [c for c in cont_names if c in prepared_df.columns]\n",
    "#             new_y_names = [c for c in y_names if c in prepared_df.columns]\n",
    "            \n",
    "#             def _warn_removed_features(newcs,oldcs):\n",
    "#                 if len(newcs) != len(oldcs): warnings.warn(f\"Removed features from {oldcs} to be {newcs}.\")\n",
    "#             _warn_removed_features(new_cat_names, cat_names)\n",
    "#             _warn_removed_features(new_cont_names, cont_names)\n",
    "#             _warn_removed_features(new_y_names, y_names)\n",
    "#         else:\n",
    "#             prepared_df = dfs\n",
    "            \n",
    "        \n",
    "        \n",
    "#         if splits is not None: splits = splits(range_of(prepared_df))\n",
    "#         super().__init__(prepared_df,\n",
    "#             procs=procs,\n",
    "#             cat_names=cat_names,\n",
    "#             cont_names=cont_names,\n",
    "#             y_names=y_names,\n",
    "#             splits=splits,\n",
    "#             do_setup=do_setup,\n",
    "#             inplace=True,\n",
    "#             y_block=y_block,\n",
    "#             reduce_memory=reduce_memory)\n",
    "\n",
    "#     def new(self, df, pre_process=None, splits=None):\n",
    "#         return type(self)(df, do_setup=False, reduce_memory=False, y_block=TransformBlock(),\n",
    "#                           pre_process=pre_process, splits=splits,\n",
    "#                           **attrdict(self, 'procs','cat_names','cont_names','y_names', 'device'))\n",
    "\n",
    "#     def show(self, max_n=10, **kwargs):\n",
    "#         to_tmp = self.new(self.all_cols[:max_n])\n",
    "#         to_tmp.items[\"TaskID\"] = self.items.TaskID[:max_n]\n",
    "# #         display_df(to_tmp.items)\n",
    "#         display_df(to_tmp.decode().items)\n",
    "\n",
    "class TabularRenewables(TabularPandas):\n",
    "    def __init__(self, dfs, procs=None, cat_names=None, cont_names=None, do_setup=True, reduce_memory=False,\n",
    "                 y_names=None, add_y_to_x=False, add_x_to_y=False, pre_process=None, device=None, splits=None, y_block=RegressionBlock()):\n",
    "\n",
    "        self.pre_process = pre_process\n",
    "        self._original_pre_process = self.pre_process\n",
    "        cont_names = listify(cont_names)\n",
    "        cat_names = listify(cat_names)\n",
    "        y_names = listify(y_names)\n",
    "        self.pre_process = listify(pre_process)\n",
    "\n",
    "        for pp in procs:\n",
    "            if isinstance(pp, RenewablesTabularProc):\n",
    "                warnings.warn(f\"Element {pp} of procs is RenewablesTabularProc, might not work with TabularPandas.\")\n",
    "         \n",
    "\n",
    "        if len(self.pre_process) > 0:\n",
    "            self.prepared_to = TabularPandas(dfs, y_names=y_names, \n",
    "                                             procs=self.pre_process, cont_names=cont_names,\n",
    "                                          do_setup=True, reduce_memory=False)\n",
    "            self.pre_process = self.prepared_to.procs\n",
    "            prepared_df = self.prepared_to.items\n",
    "            for pp in self.pre_process: \n",
    "                if getattr(pp, \"include_in_new\", False): _add_prop(self, pp)\n",
    "        else:\n",
    "            prepared_df = dfs\n",
    "\n",
    "        if splits is not None: splits = splits(range_of(prepared_df))\n",
    "        super().__init__(prepared_df,\n",
    "            procs=procs,\n",
    "            cat_names=cat_names,\n",
    "            cont_names=cont_names,\n",
    "            y_names=y_names,\n",
    "            splits=splits,\n",
    "            do_setup=do_setup,\n",
    "            inplace=True,\n",
    "            y_block=y_block,\n",
    "            reduce_memory=reduce_memory)\n",
    "\n",
    "    def new(self, df, pre_process=None, splits=None, new_task=False):\n",
    "        pre_process = listify(pre_process)\n",
    "        if new_task:\n",
    "            for pp in self._original_pre_process:\n",
    "                if getattr(pp, \"include_in_new\", False):\n",
    "                    pre_process += [pp]\n",
    "                    \n",
    "        return type(self)(df, do_setup=False, reduce_memory=False, y_block=TransformBlock(),\n",
    "                          pre_process=pre_process, splits=splits,\n",
    "                          **attrdict(self, 'procs','cat_names','cont_names','y_names', 'device'))\n",
    "\n",
    "    def show(self, max_n=10, **kwargs):\n",
    "        to_tmp = self.new(self.all_cols[:max_n])\n",
    "        to_tmp.items[\"TaskID\"] = self.items.TaskID[:max_n]\n",
    "        display_df(to_tmp.decode().items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CategoryMap(CollBase):\n",
    "#     \"Collection of categories with the reverse mapping in `o2i`\"\n",
    "#     def __init__(self, col, sort=True, add_na=False, strict=False):\n",
    "#         print(col)\n",
    "#         if is_categorical_dtype(col):\n",
    "#             print(\"bla\")\n",
    "#             items = L(col.cat.categories, use_list=True)\n",
    "#             #Remove non-used categories while keeping order\n",
    "#             if strict: items = L(o for o in items if o in col.unique())\n",
    "#         else:\n",
    "#             if not hasattr(col,'unique'): col = L(col, use_list=True)\n",
    "#             # `o==o` is the generalized definition of non-NaN used by Pandas\n",
    "#             items = L(o for o in col.unique() if o==o)\n",
    "#             if sort: items = items.sorted()\n",
    "#         self.items = '#na#' + items if add_na else items\n",
    "#         self.o2i = defaultdict(int, self.items.val2idx()) if add_na else dict(self.items.val2idx())\n",
    "\n",
    "#     def map_objs(self,objs):\n",
    "#         \"Map `objs` to IDs\"\n",
    "#         return L(self.o2i[o] for o in objs)\n",
    "\n",
    "#     def map_ids(self,ids):\n",
    "#         \"Map `ids` to objects in vocab\"\n",
    "#         return L(self.items[o] for o in ids)\n",
    "\n",
    "#     def __eq__(self,b): return all_equal(b,self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Categorify(TabularProc):\n",
    "#     \"Transform the categorical variables to something similar to `pd.Categorical`\"\n",
    "#     order = 1\n",
    "\n",
    "#     def setups(self, to):\n",
    "#         store_attr(\n",
    "#             classes={\n",
    "#                 n: CategoryMap(to.iloc[:, n].items, add_na=(n in to.cat_names))\n",
    "#                 for n in to.cat_names\n",
    "#             },\n",
    "#             but=\"to\",\n",
    "#         )\n",
    "\n",
    "#     def encodes(self, to):\n",
    "#         to.transform(to.cat_names, partial(_apply_cats, self.classes, 1))\n",
    "\n",
    "#     def decodes(self, to):\n",
    "#         to.transform(to.cat_names, partial(_decode_cats, self.classes))\n",
    "\n",
    "#     def __getitem__(self, k):\n",
    "#         return self.classes[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assure that we can de-normlize each task we assure that the task id is always stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated feature\n",
    "# df = pd.DataFrame(index=range(0,5), columns = ['A', 'B', 'C'] ).fillna(0)\n",
    "# to = TabularRenewables(df, cat_names=[\"A\"], cont_names=[\"B\"], y_names=[\"C\"], pre_process=DropCols(\"A\"))\n",
    "# test_eq(len(to.cat_names), 0)\n",
    "# df = pd.DataFrame(index=range(0,5), columns = ['A', 'B', 'C'] ).fillna(0)\n",
    "# to = TabularRenewables(df, cat_names=[\"A\"], cont_names=[\"B\"], y_names=[\"C\"], pre_process=DropCols(\"B\"))\n",
    "# test_eq(len(to.cont_names), 0)\n",
    "# df = pd.DataFrame(index=range(0,5), columns = ['A', 'B', 'C'] ).fillna(0)\n",
    "# to = TabularRenewables(df, cat_names=[\"A\"], cont_names=[\"B\"], y_names=[\"C\"], pre_process=DropCols(\"C\"))\n",
    "# test_eq(len(to.y_names), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ReadTabBatchRenewables(ItemTransform):\n",
    "    \"Transform `TabularPandas` values into a `Tensor` with the ability to decode\"\n",
    "    def __init__(self, to): self.to = to.new_empty()\n",
    "\n",
    "    def encodes(self, to):\n",
    "        self.task_ids = to.items[[\"TaskID\"]]\n",
    "        if not to.with_cont: res = (tensor(to.cats).long(),)\n",
    "        # TODO: some pre-processing causes to.conts.values of type object, while types\n",
    "        # of the dataframe are float, therefore assure conversion through astype\n",
    "        # --> this is caused by Interpolate\n",
    "        else: res = (tensor(to.cats).long(),tensor(to.conts.astype(float)).float())\n",
    "        ys = [n for n in to.y_names if n in to.items.columns]\n",
    "        # same problem as above with type of to.targ\n",
    "        if len(ys) == len(to.y_names): res = res + (tensor(to.targ.astype(float)),)\n",
    "        if to.device is not None: res = to_device(res, to.device)\n",
    "        return res\n",
    "\n",
    "    def decodes(self, o):\n",
    "        o = [_maybe_expand(o_) for o_ in to_np(o) if o_.size != 0]\n",
    "        vals = np.concatenate(o, axis=1)\n",
    "        try: df = pd.DataFrame(vals, columns=self.to.all_col_names)\n",
    "        except: df = pd.DataFrame(vals, columns=self.to.x_names)\n",
    "\n",
    "        to = self.to.new(df)\n",
    "        to.items[\"TaskID\"]=self.task_ids.values\n",
    "\n",
    "        return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates()\n",
    "class TabDataLoaderRenewables(TfmdDL):\n",
    "    \"A transformed `DataLoader` for Tabular data\"\n",
    "    def __init__(self, dataset, bs=16, shuffle=False, after_batch=None, num_workers=0, **kwargs):\n",
    "        if after_batch is None: after_batch = L(TransformBlock().batch_tfms)+ReadTabBatchRenewables(dataset)\n",
    "        super().__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n",
    "\n",
    "    def create_batch(self, b): return self.dataset.iloc[b]\n",
    "    def do_item(self, s):      return 0 if s is None else s\n",
    "\n",
    "TabularRenewables._dl_type = TabDataLoaderRenewables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NormalizePerTask(TabularProc):\n",
    "    \"Normalize per TaskId\"\n",
    "    order = 1\n",
    "    include_in_new=True\n",
    "    def __init__(self, task_id_col=\"TaskID\"):\n",
    "        self.task_id_col = task_id_col\n",
    "    def setups(self, to:Tabular):\n",
    "        self.means = getattr(to, 'train', to)[to.cont_names + \"TaskID\"].groupby(\"TaskID\").mean()\n",
    "        self.stds = getattr(to, 'train', to)[to.cont_names + \"TaskID\"].groupby(\"TaskID\").std(ddof=0)+1e-7\n",
    "\n",
    "\n",
    "    def encodes(self, to):\n",
    "        for task_id in to.items[self.task_id_col].unique():\n",
    "            # in case this is a new task, we update the means and stds\n",
    "            if task_id not in self.means.index:\n",
    "                mu = getattr(to, 'train', to)[to.cont_names + \"TaskID\"].groupby(\"TaskID\").mean()\n",
    "\n",
    "                self.means= self.means.append(mu)\n",
    "                self.stds = self.stds.append(getattr(to, 'train', to)[to.cont_names + \"TaskID\"].groupby(\"TaskID\").std(ddof=0)+1e-7)\n",
    "\n",
    "\n",
    "            mask = to.loc[:,self.task_id_col] == task_id\n",
    "\n",
    "            to.loc[mask, to.cont_names] = ((to.conts[mask] - self.means.loc[task_id]) / self.stds.loc[task_id])\n",
    "\n",
    "    def decodes(self, to, split_idx=None):\n",
    "        for task_id in to.items[self.task_id_col].unique():\n",
    "            # in case this is a new task, we update the means and stds\n",
    "            if task_id not in self.means.index:\n",
    "                warnings.warn(\"Missing task id, could not decode.\")\n",
    "\n",
    "            mask = to.loc[:,self.task_id_col] == task_id\n",
    "\n",
    "            to.loc[mask, to.cont_names] = to.conts[mask] * self.stds.loc[task_id] + self.means.loc[task_id]\n",
    "        return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_task_normalization(index=None, procs=NormalizePerTask):\n",
    "    df = pd.DataFrame(index=range(1,11), columns = ['A', 'B', 'C'] , \n",
    "                      data=np.array([list(range(1,11)), list(range(11,21)), list(range(21,31))]).T)\n",
    "    if index is not None: df[\"TimeStamps\"] = index\n",
    "    df[\"TaskID\"] = L(1 if i <= 5  else 2 for i in range(1,11))\n",
    "    index = ['2015-01-01-01', '2015-01-01-02', '2015-01-02-03', '2015-02-01-23', '2015-02-01-13',\n",
    "        '2016-01-01-01', '2016-01-01-02', '2016-06-02-03', '2016-02-01-23', '2016-02-01-13'] \n",
    "    df[\"TimeStamps\"] = index\n",
    "    to = TabularRenewables(df, pre_process=CreateTimeStampIndex(col_name=\"TimeStamps\"), \n",
    "                           procs=[NormalizePerTask], cont_names=[\"A\", \"B\"] , y_names=\"C\", \n",
    "                           cat_names=[\"TaskID\"]\n",
    "                          )\n",
    "    df[\"TimeStamps\"] = pd.to_datetime(index, utc=True)\n",
    "    df.set_index(\"TimeStamps\",inplace=True)\n",
    "    return df,to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "original_df, to = get_test_data_task_normalization()\n",
    "test_eq(original_df.astype(float), to.decode().items.astype(float))\n",
    "test_eq(np.array([[3,13],[8,18]]), to.normalize_per_task.means.values)\n",
    "test_close(np.array([[1.41421366, 1.41421366],[1.41421366, 1.41421366]]), to.normalize_per_task.stds.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dataloader and show a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df, to = get_test_data_task_normalization()\n",
    "dl = to.dataloaders(bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>-1.414213</td>\n",
       "      <td>-1.414213</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 03:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 23:00:00+00:00</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 13:00:00+00:00</th>\n",
       "      <td>1.414213</td>\n",
       "      <td>1.414213</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00+00:00</th>\n",
       "      <td>-1.414213</td>\n",
       "      <td>-1.414213</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00+00:00</th>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-02 03:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 23:00:00+00:00</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 13:00:00+00:00</th>\n",
       "      <td>1.414213</td>\n",
       "      <td>1.414213</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  A         B   C  TaskID\n",
       "TimeUTC                                                  \n",
       "2015-01-01 01:00:00+00:00 -1.414213 -1.414213  21       1\n",
       "2015-01-01 02:00:00+00:00 -0.707107 -0.707107  22       1\n",
       "2015-01-02 03:00:00+00:00  0.000000  0.000000  23       1\n",
       "2015-02-01 23:00:00+00:00  0.707107  0.707107  24       1\n",
       "2015-02-01 13:00:00+00:00  1.414213  1.414213  25       1\n",
       "2016-01-01 01:00:00+00:00 -1.414213 -1.414213  26       2\n",
       "2016-01-01 02:00:00+00:00 -0.707107 -0.707107  27       2\n",
       "2016-06-02 03:00:00+00:00  0.000000  0.000000  28       2\n",
       "2016-02-01 23:00:00+00:00  0.707107  0.707107  29       2\n",
       "2016-02-01 13:00:00+00:00  1.414213  1.414213  30       2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TaskID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 03:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 23:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 13:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-02 03:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 23:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 13:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeStamps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 03:00:00+00:00</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 23:00:00+00:00</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 13:00:00+00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00+00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00+00:00</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-02 03:00:00+00:00</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 23:00:00+00:00</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 13:00:00+00:00</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            A   B   C  TaskID\n",
       "TimeStamps                                   \n",
       "2015-01-01 01:00:00+00:00   1  11  21       1\n",
       "2015-01-01 02:00:00+00:00   2  12  22       1\n",
       "2015-01-02 03:00:00+00:00   3  13  23       1\n",
       "2015-02-01 23:00:00+00:00   4  14  24       1\n",
       "2015-02-01 13:00:00+00:00   5  15  25       1\n",
       "2016-01-01 01:00:00+00:00   6  16  26       2\n",
       "2016-01-01 02:00:00+00:00   7  17  27       2\n",
       "2016-06-02 03:00:00+00:00   8  18  28       2\n",
       "2016-02-01 23:00:00+00:00   9  19  29       2\n",
       "2016-02-01 13:00:00+00:00  10  20  30       2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [2],\n",
       "         [1],\n",
       "         [2]], device='cuda:0'),\n",
       " tensor([[-1.4142, -1.4142],\n",
       "         [-0.7071, -0.7071],\n",
       "         [ 1.4142,  1.4142],\n",
       "         [ 0.0000,  0.0000]], device='cuda:0'),\n",
       " tensor([[26.],\n",
       "         [27.],\n",
       "         [25.],\n",
       "         [28.]], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TaskID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dl.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives an example on how to add a new task, that is normalized based on the first year. E.g. when the features are numerical weather predictions. As those are themselves forecasts, we can always extract the past and use the data for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df, to = get_test_data_task_normalization()\n",
    "original_df[\"TaskID\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeStamps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>-1.414213</td>\n",
       "      <td>-1.414213</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 03:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 23:00:00+00:00</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 13:00:00+00:00</th>\n",
       "      <td>1.414213</td>\n",
       "      <td>1.414213</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  A         B   C  TaskID\n",
       "TimeStamps                                               \n",
       "2015-01-01 01:00:00+00:00 -1.414213 -1.414213  21       3\n",
       "2015-01-01 02:00:00+00:00 -0.707107 -0.707107  22       3\n",
       "2015-01-02 03:00:00+00:00  0.000000  0.000000  23       3\n",
       "2015-02-01 23:00:00+00:00  0.707107  0.707107  24       3\n",
       "2015-02-01 13:00:00+00:00  1.414213  1.414213  25       3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment=None\n",
    "# setups task normalization\n",
    "to_new = to.new(original_df, pre_process=FilterYear(2016, drop=True))\n",
    "to_new.process()\n",
    "to_new.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.440892e-17</td>\n",
       "      <td>-4.440892e-17</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.118034e+00</td>\n",
       "      <td>1.118034e+00</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.414213e+00</td>\n",
       "      <td>-1.414213e+00</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.071067e-01</td>\n",
       "      <td>-7.071067e-01</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.071067e-01</td>\n",
       "      <td>7.071067e-01</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.414213e+00</td>\n",
       "      <td>1.414213e+00</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A             B          C  TaskID\n",
       "count  5.000000e+00  5.000000e+00   5.000000     5.0\n",
       "mean  -4.440892e-17 -4.440892e-17  23.000000     3.0\n",
       "std    1.118034e+00  1.118034e+00   1.581139     0.0\n",
       "min   -1.414213e+00 -1.414213e+00  21.000000     3.0\n",
       "25%   -7.071067e-01 -7.071067e-01  22.000000     3.0\n",
       "50%    0.000000e+00  0.000000e+00  23.000000     3.0\n",
       "75%    7.071067e-01  7.071067e-01  24.000000     3.0\n",
       "max    1.414213e+00  1.414213e+00  25.000000     3.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_new.items.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the second year based on the mean and std of the first year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_new = to.new(original_df, pre_process=FilterYear(2016, drop=False))\n",
    "to_new.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data has larger values in the second year, the normalization is quite off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeStamps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00+00:00</th>\n",
       "      <td>2.121320</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00+00:00</th>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-02 03:00:00+00:00</th>\n",
       "      <td>3.535534</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 23:00:00+00:00</th>\n",
       "      <td>4.242640</td>\n",
       "      <td>4.242640</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 13:00:00+00:00</th>\n",
       "      <td>4.949747</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  A         B   C  TaskID\n",
       "TimeStamps                                               \n",
       "2016-01-01 01:00:00+00:00  2.121320  2.121320  26       3\n",
       "2016-01-01 02:00:00+00:00  2.828427  2.828427  27       3\n",
       "2016-06-02 03:00:00+00:00  3.535534  3.535534  28       3\n",
       "2016-02-01 23:00:00+00:00  4.242640  4.242640  29       3\n",
       "2016-02-01 13:00:00+00:00  4.949747  4.949747  30       3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_new.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be seen in the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.535534</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.118034</td>\n",
       "      <td>1.118034</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.121320</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.535534</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.242640</td>\n",
       "      <td>4.242640</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.949747</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              A         B          C  TaskID\n",
       "count  5.000000  5.000000   5.000000     5.0\n",
       "mean   3.535534  3.535534  28.000000     3.0\n",
       "std    1.118034  1.118034   1.581139     0.0\n",
       "min    2.121320  2.121320  26.000000     3.0\n",
       "25%    2.828427  2.828427  27.000000     3.0\n",
       "50%    3.535534  3.535534  28.000000     3.0\n",
       "75%    4.242640  4.242640  29.000000     3.0\n",
       "max    4.949747  4.949747  30.000000     3.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_new.items.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a dataloader and verify if we can display the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TaskID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_new.dataloaders(bs=4).show_batch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
