{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tabular.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tabular.core\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "import pandas as pd\n",
    "from fastai.data.external import *\n",
    "from fastcore.all import *\n",
    "from pathlib import PosixPath\n",
    "from fastcore.test import *\n",
    "from fastai.tabular.all import *\n",
    "import fastai\n",
    "from fastai.tabular.core import _maybe_expand\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers to read files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def str_to_path(file: str):\n",
    "    \"Convers a string to a Posixpath.\"\n",
    "    if isinstance(file, str) and \"~\" in file:\n",
    "        file = os.path.expanduser(file)\n",
    "\n",
    "    file = Path(file)\n",
    "\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq_type(Path(\"\"), str_to_path(\"\"))\n",
    "test_eq_type(Path(\"\"), str_to_path(Path(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_hdf(file:PosixPath, key: str = \"/powerdata\", key_metadata=None):\n",
    "    \"Reads a hdf5 table based on the given key.\"\n",
    "    file = str_to_path(file)\n",
    "    if \"/\" not in key: key = \"/\" + key\n",
    "    with pd.HDFStore(file, \"r\") as store:\n",
    "        if key in store.keys():\n",
    "            df = store[key]\n",
    "            if key_metadata is not None:\n",
    "                df_meta = store[key_metadata]\n",
    "                for c in df_meta: df[c] = df_meta[c].values[0]\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n",
    "                  index=['a', 'b', 'c'])\n",
    "df.to_hdf('data.h5', key='df', mode='w')\n",
    "test_eq(df, read_hdf(\"data.h5\", key=\"df\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_csv(file:PosixPath, sep:str =\";\"):\n",
    "    \"Reads a csv file.\"\n",
    "    file = str_to_path(file)\n",
    "    df = pd.read_csv(str(file), sep=sep)\n",
    "    df.drop([\"Unnamed: 0\"], inplace=True, axis=1, errors=\"ignore\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},)\n",
    "df.to_csv('data.csv', sep=\";\")\n",
    "test_eq(df, read_csv(\"data.csv\", sep=\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_files(\n",
    "    files:PosixPath,\n",
    "    key:str =\"/powerdata\",\n",
    "    key_metadata=None,\n",
    "    sep:str=\";\",\n",
    "    add_task_id=True\n",
    ") -> pd.DataFrame:\n",
    "    \"Reads a number of CSV or HDF5 files depending on file ending.\"\n",
    "\n",
    "    files = listify(files)\n",
    "    dfs=L()\n",
    "    for task_id,file in enumerate(files):\n",
    "        if isinstance(file, str):\n",
    "            file = str_to_path(file)\n",
    "\n",
    "        if file.suffix == \".h5\":\n",
    "            df = read_hdf(file, key, key_metadata=key_metadata)\n",
    "        elif file.suffix == \".csv\":\n",
    "            df = read_csv(file, sep=\";\")\n",
    "        else:\n",
    "            raise f\"File ending of file {file} not supported.\"\n",
    "        if add_task_id:df[\"TaskID\"]=task_id\n",
    "        dfs += df\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n",
    "                  index=['a', 'b', 'c'])\n",
    "df.to_hdf('data.h5', key='df', mode='w')\n",
    "test_eq(df, read_files(\"data.h5\", key=\"df\", add_task_id=False)[0])\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},)\n",
    "df.to_csv('data.csv', sep=\";\")\n",
    "test_eq(df, read_files(\"data.csv\", add_task_id=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# this is merely a class to differentiate between fastai processing and renewbale pre-processing functionality\n",
    "class RenewablesTabularProc(TabularProc):\n",
    "    include_in_new=False\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestProc(RenewablesTabularProc): pass\n",
    "test_eq(isinstance(TestProc(), RenewablesTabularProc), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CreateTimeStampIndex(RenewablesTabularProc):\n",
    "    order=0\n",
    "    include_in_new=True\n",
    "    def __init__(self, col_name, offset_correction=None):\n",
    "        self.col_name = col_name\n",
    "        self.offset_correction = offset_correction\n",
    "\n",
    "    def encodes(self, to):\n",
    "        df = to.items\n",
    "        \n",
    "        def create_timestamp_index(df, drop_index=True):\n",
    "            df.reset_index(drop=drop_index, inplace=True)\n",
    "            df.rename({self.col_name: \"TimeUTC\"}, axis=1, inplace=True)\n",
    "            #  in case the timestamp is index give it a proper timestamp,e.g., in GermanSolarFarm dataset\n",
    "            if \"0000-\" in str(df.TimeUTC[0]):\n",
    "                df.TimeUTC = df.TimeUTC.apply(\n",
    "                    lambda x: x.replace(\"0000-\", \"2015-\").replace(\"0001-\", \"2016-\")\n",
    "                )\n",
    "            df.TimeUTC = pd.to_datetime(df.TimeUTC, infer_datetime_format=True, utc=True)\n",
    "            df.set_index(\"TimeUTC\", inplace=True)\n",
    "            df.index = df.index.rename(\"TimeUTC\")\n",
    "\n",
    "            #  for GermanSolarFarm, the index is not corret. Should have a three hour resolution but is one...\n",
    "            if self.offset_correction is not None:\n",
    "                i, new_index = 0, []\n",
    "                for cur_index in df.index:\n",
    "                    new_index.append(cur_index + pd.DateOffset(hours=i))\n",
    "                    i += self.offset_correction\n",
    "                df.index = new_index\n",
    "\n",
    "        if self.col_name in df.columns:\n",
    "            create_timestamp_index(df, drop_index=True)\n",
    "        # properly already processed\n",
    "        elif self.col_name == to.items.index.name:  \n",
    "            create_timestamp_index(df, drop_index=False)\n",
    "        else:\n",
    "            warnings.warn(f\"Timetamps column {self.col_name} not in columns {df.columns} or df.index.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(index=None):\n",
    "    df = pd.DataFrame(index=range(0,5), columns = ['A', 'B', 'C'] ).fillna(0)\n",
    "    if index is not None: df[\"TimeStamps\"] = index\n",
    "    return TabularPandas(df)\n",
    "# tests basic functionality to set a proper timestamp based index\n",
    "index = ['2015-01-01-01', '2015-01-01-02', '2015-01-02-03', '2015-02-01-23', '2015-02-01-13'] \n",
    "to = get_test_data(index)\n",
    "test_eq(CreateTimeStampIndex(col_name=\"TimeStamps\")(to).items.index, pd.to_datetime(index, utc=True))\n",
    "\n",
    "# corrects missing year\n",
    "index_missing_year = ['0000-01-01-01', '0000-01-01-02', '0000-01-02-03', '0000-02-01-23', '0000-02-01-13'] \n",
    "to = get_test_data(index_missing_year)\n",
    "test_eq(CreateTimeStampIndex(col_name=\"TimeStamps\")(to).items.index, pd.to_datetime(index, utc=True))\n",
    "\n",
    "# check if warning is triggered, due to wrong column name\n",
    "to = get_test_data(index)\n",
    "test_call = lambda: CreateTimeStampIndex(col_name=\"FalseColumnName\")(to)\n",
    "test_warns(test_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_samples_per_day(df, n_samples_to_check=100, expected_samples=[8,24,96]):\n",
    "    \"\"\"\n",
    "    Extract the amount of entries per day from the DataFrame in the first n_samples_to_check.\n",
    "    Aborts, ones the first meaningful *number of samples per day* is found.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        the DataFrame used for the conversion.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    integer\n",
    "        amount of entries per day.\n",
    "    \"\"\"\n",
    "    samples_per_day = -1\n",
    "    \n",
    "    if len(df) == 0: return samples_per_day\n",
    "    mins = 0\n",
    "    for i in range(1, min(n_samples_to_check,len(df))):\n",
    "        mins = (df.index[i] - df.index[i -1]).seconds // 60\n",
    "        if (24*60)%mins==0:\n",
    "            samples_per_day = (24*60)/mins\n",
    "            if samples_per_day in expected_samples: break\n",
    "\n",
    "    if samples_per_day == -1:\n",
    "        raise ValueError(f\"{mins} is an unknown sampling time.\")\n",
    "        \n",
    "    return int(samples_per_day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_samples_per_day(index):\n",
    "    return pd.DataFrame(index=pd.to_datetime(index),\n",
    "                      columns = ['A', 'B', 'C'] ).fillna(0)\n",
    "df = test_data_samples_per_day(index = pd.to_datetime(['2018-01-01-00:00', '2019-01-01-04:00', '2020-01-01-07:00',] ))\n",
    "test_eq(8, get_samples_per_day(df))\n",
    "df = test_data_samples_per_day(index = pd.to_datetime(['2018-01-01-00:00', '2019-01-01-02:00', '2020-01-01-03:00',] ))\n",
    "test_eq(24, get_samples_per_day(df))\n",
    "df = test_data_samples_per_day(index = pd.to_datetime(['2018-01-01-00:00', '2019-01-01-01:15', '2020-01-01-01:30',] ))\n",
    "test_eq(96, get_samples_per_day(df))\n",
    "test_eq(-1, get_samples_per_day(test_data_samples_per_day([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _interpolate_df(df, sample_time=\"15Min\", limit=5, drop_na=False):\n",
    "        df = df[~df.index.duplicated()]\n",
    "        upsampled = df.resample(sample_time)\n",
    "        df  = upsampled.interpolate(method=\"linear\", limit=limit)\n",
    "        \n",
    "        if drop_na: df = df.dropna(axis=0)\n",
    "\n",
    "        if \"Hour\" in df.columns:\n",
    "            df[\"Hour\"] = df.index.hour\n",
    "        if \"Month\" in df.columns:\n",
    "            df[\"Month\"] = df.index.month\n",
    "        if \"Day\" in df.columns:\n",
    "            df[\"Day\"] = df.index.day\n",
    "        if \"Week\" in df.columns:\n",
    "            df[\"Week\"] = df.index.week\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_interpolate(index=pd.to_datetime(['2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-03:00'])):\n",
    "    np.random.seed(2)\n",
    "    df = pd.DataFrame(index=pd.to_datetime(index),\n",
    "                      data=np.random.randint(0,10,size=(len(index),3)),\n",
    "                  columns = ['A', 'B', 'C'] )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_data_interpolate()\n",
    "inp_df = _interpolate_df(df, limit=5, drop_na=True)\n",
    "# instead of three values per hours we now have four plus the last timesamp\n",
    "# duplicated values are droppped\n",
    "test_eq(9, inp_df.shape[0])\n",
    "test_eq(2, inp_df.iloc[-2,0])\n",
    "# one for every timestamp plus one for the first two timestamps, assure that duplicates are dropped\n",
    "df = test_data_interpolate(index = pd.to_datetime(['2018-01-01-01:00', '2018-01-01-01:00', '2018-01-01-03:00', '2018-01-01-04:00']),)\n",
    "test_eq(5, _interpolate_df(df, limit=1, drop_na=True).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _apply_group_by(to:pd.DataFrame, group_by_col, func, **kwargs):\n",
    "    if group_by_col in to.columns:\n",
    "        dfs = L()\n",
    "        for k,df_g in to.groupby(group_by_col):\n",
    "            dfs += func(df_g, **kwargs)\n",
    "        df = pd.concat(dfs, axis=0)\n",
    "    else:\n",
    "        df = func(to, **kwargs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Interpolate(RenewablesTabularProc):\n",
    "    order=50\n",
    "    include_in_new=True\n",
    "    def __init__(self, sample_time = \"15Min\", limit=5, drop_na=True, group_by_col=\"TaskID\"):\n",
    "        self.sample_time = sample_time\n",
    "        self.limit = limit\n",
    "        self.drop_na = drop_na\n",
    "        self.group_by_col = group_by_col\n",
    "    \n",
    "    def setups(self, to: Tabular):\n",
    "        self.n_samples_per_day = get_samples_per_day(to.items)\n",
    "        if self.n_samples_per_day == -1:\n",
    "            warnings.warn(\"Could not determine samples per day. Skip processing.\")\n",
    "    \n",
    "    def encodes(self, to):\n",
    "        \n",
    "        if self.n_samples_per_day == -1: return\n",
    "        \n",
    "        # if values of a columns are the same in each row (categorical features)\n",
    "        # we make that those stay the same during interpolation\n",
    "        if self.group_by_col in to.items.columns:\n",
    "            d = defaultdict(object)\n",
    "            non_unique_columns = L()\n",
    "            for group_id, df in to.items.groupby(self.group_by_col):\n",
    "                for c in df.columns:\n",
    "                    if len(df[c].unique())==1 and c!=self.group_by_col:\n",
    "                        d[(group_id,c)] = df[c][0]\n",
    "                    else:\n",
    "                        non_unique_columns += c\n",
    "            \n",
    "            non_unique_columns = np.unique(non_unique_columns)\n",
    "        else:\n",
    "            non_unique_columns = to.items.columns\n",
    "        # interpolate non unique columns         \n",
    "        df = _apply_group_by(to.items.loc[:,np.unique(non_unique_columns)], self.group_by_col, _interpolate_df)\n",
    "        to.items = df\n",
    "        if self.group_by_col in to.items.columns:\n",
    "            for group_id,col_name in d.keys():\n",
    "                mask = to[self.group_by_col]==group_id\n",
    "                to.items.loc[mask,col_name]=d[(group_id, col_name)] \n",
    "        \n",
    "        # TODO: to infer dtype through pd.inferdtype\n",
    "        # use this for conversion\n",
    "        # in case there is an object inside, throw an error\n",
    "        if len(to.cont_names)>0:\n",
    "            mask = to[to.cont_names].isna().values[:,0]\n",
    "            to.items = to.items[~mask]\n",
    "        # pandas converts the datatype to float if np.NaN is present, lets revert that                \n",
    "        to.items = to.items.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-e36a25c82886>:14: UserWarning: Could not determine samples per day. Skip processing.\n",
      "  warnings.warn(\"Could not determine samples per day. Skip processing.\")\n"
     ]
    }
   ],
   "source": [
    "df = test_data_interpolate(index = pd.to_datetime(['2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-03:00']),)\n",
    "df[\"TaskID\"] = [1,1,2,2,2]\n",
    "to = TabularPandas(df, procs=Interpolate, do_setup=True)\n",
    "test_eq(5, to.items.loc[to.items.TaskID==1].shape[0])\n",
    "test_eq(9, to.items.loc[to.items.TaskID==2].shape[0])\n",
    "\n",
    "df = test_data_interpolate(index = pd.to_datetime(['2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-03:00']),)\n",
    "df[\"TaskID\"] = [1,1,2,2,2]\n",
    "df[\"B\"] = [1,1,2,2,2]\n",
    "to = TabularPandas(df, cont_names=\"A\", procs=Interpolate, do_setup=True)\n",
    "test_eq([1,1,1,1,1,2,2,2,2,2,2,2,2,2], list(to.items.B))\n",
    "test_eq(9, to.items.loc[to.items.TaskID==2].shape[0])\n",
    "\n",
    "df = test_data_interpolate(index = pd.to_datetime(['2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-01:00', '2018-01-01-02:00', '2018-01-01-03:00']),)\n",
    "to = TabularPandas(df,  cont_names=\"A\", procs=Interpolate, do_setup=True)\n",
    "test_eq(9, to.items.shape[0])\n",
    "\n",
    "df = test_data_interpolate(index = [])\n",
    "to = TabularPandas(df, procs=Interpolate, do_setup=True)\n",
    "test_eq(0, to.items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _create_consistent_number_of_sampler_per_day(\n",
    "    df: pd.DataFrame, n_samples_per_day: int = 24\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove days with less than the specified amount of samples from the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        the DataFrame used for the conversion.\n",
    "    n_samples_per_day : integer\n",
    "        the amount of samples each day in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        the given DataFrame, now with a consistent amount of samples each day.\n",
    "    \"\"\"\n",
    "    # Create a list of booleans, where each day with 'less than n_samples_per_day' samples is denoted with 'True'\n",
    "    mask = df.groupby(pd.Grouper(freq=\"D\")).apply(len)\n",
    "    mask = (mask < n_samples_per_day)\n",
    "    \n",
    "    bad_days = list(chain.from_iterable([list(pd.date_range(start=b, periods=n_samples_per_day, freq=f'{(24 * 60) // n_samples_per_day}Min'))\n",
    "                for b in mask[mask].index]))\n",
    "\n",
    "    return df[~df.index.isin(bad_days)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 15:00:00</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 18:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 21:00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02 00:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02 03:00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     A  B  C\n",
       "2018-01-01 15:00:00  4  3  7\n",
       "2018-01-01 18:00:00  6  1  3\n",
       "2018-01-01 21:00:00  5  8  4\n",
       "2018-01-02 00:00:00  6  3  9\n",
       "2018-01-02 03:00:00  2  0  4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.date_range(start='1/1/2018', periods=10, freq='3H')\n",
    "df = test_data_interpolate(index)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1514764800000000000, 1514775600000000000, 1514786400000000000,\n",
       "            1514797200000000000, 1514808000000000000, 1514818800000000000,\n",
       "            1514829600000000000, 1514840400000000000, 1514851200000000000,\n",
       "            1514862000000000000],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _create_consistent_number_of_sampler_per_day(df, n_samples_per_day= get_samples_per_day(df))\n",
    "# last two rows are removed, as they are not a \"complete day\"\n",
    "test_eq(8, df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterInconsistentSamplesPerDay(RenewablesTabularProc):\n",
    "    order=100  \n",
    "    include_in_new=True\n",
    "    def __init__(self, group_by_col=\"TaskID\"):\n",
    "        self.group_by_col = group_by_col\n",
    "        \n",
    "    def setups(self, to: Tabular):\n",
    "        self.n_samples_per_day = get_samples_per_day(to.items)\n",
    "    \n",
    "    def encodes(self, to):\n",
    "        to.items = _apply_group_by(to.items, self.group_by_col, _create_consistent_number_of_sampler_per_day, \n",
    "                        n_samples_per_day=self.n_samples_per_day)\n",
    "#         to.items = _create_consistent_number_of_sampler_per_day(to.items, n_samples_per_day=self.n_samples_per_day)\n",
    "        \n",
    "        assert (to.items.shape[0]%self.n_samples_per_day) == 0, \"Incorrect number of samples after filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(True, FilterInconsistentSamplesPerDay.order > Interpolate.order)\n",
    "index = pd.date_range(start='1/1/2018', periods=30, freq='3H')\n",
    "df = test_data_interpolate(list(index))\n",
    "df[\"TaskID\"] = [1 if i<11 else 2 for i in range(len(df))]\n",
    "to  = TabularPandas(df, procs=FilterInconsistentSamplesPerDay, do_setup=True)\n",
    "# equal to two days with eight samples per day\n",
    "test_eq(16, to.items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 03:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 06:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 09:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 12:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 15:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 18:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 21:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03 03:00:00</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AddSeasonalFeatures(RenewablesTabularProc):\n",
    "    order=0\n",
    "    include_in_new=True\n",
    "    def __init__(self, as_cont=True):\n",
    "        self.as_cont = as_cont\n",
    "    \n",
    "    def encodes(self, to):\n",
    "        as_sin = lambda value, max_value: np.sin(2*np.pi*value/max_value)\n",
    "        as_cos = lambda value, max_value: np.cos(2*np.pi*value/max_value)\n",
    "        \n",
    "        if self.as_cont:\n",
    "            to.items[\"MonthSin\"] = as_sin(to.items.index.month, 12)\n",
    "            to.items[\"MonthCos\"] = as_cos(to.items.index.month, 12)\n",
    "            to.items[\"DaySin\"] = as_sin(to.items.index.day, 31)\n",
    "            to.items[\"DayCos\"] = as_cos(to.items.index.day, 31)\n",
    "            to.items[\"HourSin\"] = as_sin(to.items.index.hour, 24)\n",
    "            to.items[\"HourCos\"] = as_cos(to.items.index.hour, 24)\n",
    "            \n",
    "        else:\n",
    "            to.items[\"Month\"] = to.items.index.month\n",
    "            to.items[\"Day\"] = to.items.index.day\n",
    "            to.items[\"Hour\"] = to.items.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = get_test_data(index=['2018-01-01-01', '2018-01-01-02', '2018-01-02-03', '2018-02-01-23', '2018-02-01-13'] )\n",
    "CreateTimeStampIndex(\"TimeStamps\")(to)\n",
    "AddSeasonalFeatures(as_cont=False)(to)\n",
    "test_eq(np.array([1,1,1,2,2]), to.items.Month.values)\n",
    "test_eq(np.array([1,1,2,1,1]), to.items.Day.values)\n",
    "test_eq(np.array([1,2,3,23,13]), to.items.Hour.values)\n",
    "# TODO test sin/cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterByCol(RenewablesTabularProc):\n",
    "    \"Drops rows by column.\"\n",
    "    order = 9\n",
    "    def __init__(self, col_name, drop=True, drop_col_after_filter=True):\n",
    "        self.col_name = col_name\n",
    "        self.drop = drop\n",
    "        self.drop_col_after_filter=drop_col_after_filter\n",
    "\n",
    "    def encodes(self, to):\n",
    "        mask = to.items[self.col_name].astype(bool).values\n",
    "        if self.drop: mask = ~mask\n",
    "        to.items = to.items[mask]\n",
    "        if self.drop_col_after_filter: to.items.drop(self.col_name, axis=1, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = get_test_data()\n",
    "to.loc[:,\"C\"] = [0,0,1,1,0]\n",
    "FilterByCol(col_name=\"C\", drop_col_after_filter=True, drop=True)(to)\n",
    "test_eq(list(to.items.index),[0,1,4])\n",
    "test_eq(to.items.columns,[\"A\",\"B\"])\n",
    "\n",
    "to = get_test_data()\n",
    "to.loc[:,\"C\"] = [0,0,1,1,0]\n",
    "FilterByCol(col_name=\"C\", drop_col_after_filter=False, drop=False)(to)\n",
    "test_eq(list(to.items.index),[2,3])\n",
    "test_eq(to.items.columns,[\"A\",\"B\", \"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterYear(RenewablesTabularProc):\n",
    "    \"Filter a list of years. By default the years are dropped.\"\n",
    "    order = 9\n",
    "    def __init__(self, year, drop=True):\n",
    "        \"year(s) to filter, whether to drop or keep the years.\"\n",
    "        year = listify(year)\n",
    "        self.year = L(int(y) for y in year)\n",
    "        self.drop = drop\n",
    "\n",
    "    def encodes(self, to):\n",
    "        mask = None\n",
    "        for y in self.year:\n",
    "            cur_mask = to.items.index.year == y\n",
    "            if mask is None: mask = cur_mask\n",
    "            else: mask = mask | cur_mask\n",
    "\n",
    "        if not self.drop: mask = ~mask\n",
    "        to.items.drop(to.items[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_filter_year():\n",
    "    index = ['2018-01-01-01', '2019-01-01-02', '2020-01-02-03',] \n",
    "    return TabularPandas(pd.DataFrame(index=pd.to_datetime(index),\n",
    "                      columns = ['A', 'B', 'C'] ).fillna(0))\n",
    "    \n",
    "to = test_data_filter_year()\n",
    "FilterYear(year=2018)(to)\n",
    "test_eq(np.array([2019,2020]), to.items.index.year)\n",
    "to = test_data_filter_year()\n",
    "FilterYear(year=2020, drop=False)(to)\n",
    "test_eq(np.array([2020]), to.items.index.year)\n",
    "to = test_data_filter_year()\n",
    "FilterYear(year=[2018,2020], drop=True)(to)\n",
    "test_eq(np.array([2019]), to.items.index.year)\n",
    "to = test_data_filter_year()\n",
    "FilterYear(year=[2018,2020], drop=False)(to)\n",
    "test_eq(np.array([2018,2020]), to.items.index.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterHalf(RenewablesTabularProc):\n",
    "    \"First half of the data is used for training and the other half of validation/testing.\"\n",
    "    order = 9\n",
    "    def __init__(self, drop=False, bydate=True):\n",
    "        \"\"\"\n",
    "        Whether to drop or keep the first half.\n",
    "        When bydate is true the average date, between the first and last date is used to filter the data.\n",
    "        If bydate is false the amount of data is splitted by half, so that train and validation/testing have an equal amount of available data.\n",
    "        \"\"\"\n",
    "        self.drop = drop\n",
    "        self.bydate = bydate\n",
    "        \n",
    "    def setups(self, to: Tabular):\n",
    "        df = to.items.sort_index()\n",
    "        if self.bydate:\n",
    "        \n",
    "            self.first_date = df.index[0]\n",
    "            self.last_date = df.index[-1]\n",
    "            self.split_date = self.first_date + (self.last_date-self.first_date)/2\n",
    "        else:\n",
    "            idx = len(df)//2\n",
    "            self.split_date = df.index[idx]\n",
    "\n",
    "    def encodes(self, to):\n",
    "        mask = to.items.index < self.split_date\n",
    "\n",
    "        if not self.drop: mask = ~mask\n",
    "        to.items.drop(to.items[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_filter_by_half(procs):\n",
    "    index = ['2018-01-01-01', '2019-01-01-02', '2020-01-02-03',] \n",
    "    index=pd.to_datetime(index)\n",
    "    return index, TabularPandas(pd.DataFrame(index=index,\n",
    "                      columns = ['A', 'B', 'C'] ).fillna(0), procs=procs,)\n",
    "\n",
    "index, res = test_data_filter_by_half(FilterHalf())\n",
    "test_eq(index[0:2], res.items.index)\n",
    "index, res = test_data_filter_by_half(FilterHalf(drop=True))\n",
    "test_eq([index[2]], res.items.index)\n",
    "\n",
    "index, res = test_data_filter_by_half(FilterHalf(bydate=False))\n",
    "test_eq([index[0]], res.items.index)\n",
    "index, res = test_data_filter_by_half(FilterHalf(bydate=False, drop=True))\n",
    "test_eq(index[1:], res.items.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterMonths(RenewablesTabularProc):\n",
    "    \"Filter dataframe for specific months.\"\n",
    "    order = 9\n",
    "    def __init__(self, months=range(1,13), drop=False):\n",
    "        self.months = listify(months)\n",
    "        self.drop = drop\n",
    "\n",
    "    def encodes(self, to):\n",
    "        mask = to.items.index.month.isin(self.months)\n",
    "        if not self.drop: mask = ~mask\n",
    "        to.items.drop(to.items[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_filter_month():\n",
    "    to = get_test_data(index=['2018-01-01-01', '2018-02-01-02', '2018-03-02-03', '2018-04-01-23', '2018-05-01-13'])\n",
    "    CreateTimeStampIndex(\"TimeStamps\")(to)\n",
    "    return to\n",
    "\n",
    "def test_filter_month(months,drop,expected_result):\n",
    "    to = get_test_data_filter_month()\n",
    "    FilterMonths(months,drop)(to)\n",
    "    test_eq(to.items.index.month, expected_result)\n",
    "    \n",
    "test_filter_month([1,2], False, [1,2])\n",
    "test_filter_month(range(1,3), False, [1,2])\n",
    "test_filter_month([1], False, [1])\n",
    "test_filter_month([1,2], True, [3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilterDays(RenewablesTabularProc):\n",
    "    \"Filter dataframe for specific months.\"\n",
    "    order = 10\n",
    "    def __init__(self, num_days):\n",
    "        self.num_days = num_days\n",
    "        \n",
    "    def setups(self, to: Tabular):\n",
    "        self.n_samples_per_day = get_samples_per_day(to.items)\n",
    "        \n",
    "    def encodes(self, to):\n",
    "        to.items = to.items[-(self.n_samples_per_day * self.num_days):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(True, FilterYear.order<FilterDays.order)\n",
    "test_eq(True, FilterMonths.order<FilterDays.order)\n",
    "test_eq(True, FilterByCol.order<FilterDays.order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DropCols(RenewablesTabularProc):\n",
    "    \"Drops rows by column name.\"\n",
    "    include_in_new=True\n",
    "    order = 10\n",
    "    def __init__(self, cols):\n",
    "        self.cols = listify(cols)\n",
    "\n",
    "    def encodes(self, to):\n",
    "        to.items.drop(self.cols, axis=1, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = get_test_data()\n",
    "DropCols(None)(to)\n",
    "test_eq(to.items.columns, [\"A\", \"B\", \"C\"])\n",
    "to = get_test_data()\n",
    "DropCols([])(to)\n",
    "test_eq(to.items.columns, [\"A\", \"B\", \"C\"])\n",
    "to = get_test_data()\n",
    "DropCols([\"C\"])(to)\n",
    "test_eq(to.items.columns, [\"A\", \"B\"])\n",
    "to = get_test_data()\n",
    "DropCols([\"A\", \"B\"])(to)\n",
    "test_eq(to.items.columns, [\"C\"])\n",
    "to = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Normalize(RenewablesTabularProc):\n",
    "    \"Normalize per TaskId\"\n",
    "    order = 1\n",
    "    include_in_new=True\n",
    "    def __init__(self, cols_to_ignore=[]):\n",
    "        self.cols_to_ignore = cols_to_ignore\n",
    "\n",
    "    def setups(self, to: Tabular):\n",
    "        self.rel_cols = [c for c in to.cont_names if c not in self.cols_to_ignore]\n",
    "        self.means = getattr(to, \"train\", to)[self.rel_cols].mean()\n",
    "        self.stds = getattr(to, \"train\", to)[self.rel_cols].std(ddof=0) + 1e-7\n",
    "\n",
    "    def encodes(self, to):\n",
    "        to.loc[:, self.rel_cols] = (to.loc[:, self.rel_cols] - self.means) / self.stds\n",
    "\n",
    "    def decodes(self, to):\n",
    "        to.loc[:, self.rel_cols] = to.loc[:, self.rel_cols] * self.stds + self.means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we assume, that we have NWP as input features, we can always extract the past. \n",
    "Therefore, we should normalize before filtering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(True, Normalize.order<FilterMonths.order)\n",
    "test_eq(True, Normalize.order<FilterByCol.order)\n",
    "test_eq(True, Normalize.order<FilterYear.order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BinFeatures(TabularProc):\n",
    "    \"Creates bin from categorical features.\"\n",
    "    order = 1\n",
    "    include_in_new=True\n",
    "    def __init__(self, column_names, bin_sizes=5):\n",
    "        # TODO: Add possiblitiy to add custom bins\n",
    "        self.column_names = listify(column_names)\n",
    "        self.bin_sizes = listify(bin_sizes)\n",
    "        if len(self.bin_sizes) == 1: self.bin_sizes = L(self.bin_sizes[0] for _ in self.column_names)\n",
    "\n",
    "    def setups(self, to:Tabular):\n",
    "        train_to = getattr(to, 'train', to)\n",
    "        self.bin_edges = {c:pd.qcut(train_to.items[c], q=bs, retbins=True)[1] for c,bs in zip(self.column_names,self.bin_sizes)}\n",
    "\n",
    "\n",
    "    def encodes(self, to):\n",
    "        for c in self.bin_edges.keys():\n",
    "            to.items.loc[:,c] = pd.cut(to.items[c], bins=self.bin_edges[c],\n",
    "                                       labels=range(1, len(self.bin_edges[c])),\n",
    "                                       include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test corner cases of minimum and maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n",
    "df.iloc[0,:] = 0\n",
    "df.iloc[-1,:] = 100\n",
    "to = TabularPandas(df, cont_names=[\"A\", \"B\", \"C\"], y_names=\"D\", procs=BinFeatures(column_names=[\"A\", \"B\", \"C\"]))\n",
    "test_eq(to.items.iloc[-1,:][[\"A\", \"B\", \"C\"]].values, [5,5,5])\n",
    "test_eq(to.items.iloc[0,:][[\"A\", \"B\", \"C\"]].values, [1,1,1])\n",
    "# Test for nas.\n",
    "test_eq(to.items.isna().sum().sum(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if it is also works along with categorify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n",
    "df.iloc[0,:] = 0\n",
    "df.iloc[-1,:] = 100\n",
    "to = TabularPandas(df, cont_names=[\"A\", \"B\", \"C\"], y_names=\"D\", procs=[BinFeatures(column_names=[\"A\", \"B\", \"C\"]), Categorify()])\n",
    "test_eq(to.items.iloc[-1,:][[\"A\", \"B\", \"C\"]].values, [5,5,5])\n",
    "test_eq(to.items.iloc[0,:][[\"A\", \"B\", \"C\"]].values, [1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension of TabularPandas with pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def _add_prop(cls, o):\n",
    "    setattr(cls, camel2snake(o.__class__.__name__), o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RenewableSplits:\n",
    "    pass\n",
    "\n",
    "\n",
    "class ByWeeksSplitter(RenewableSplits):\n",
    "    def __init__(self, every_n_weeks: int = 4):\n",
    "        self.every_n_weeks = every_n_weeks\n",
    "    \n",
    "    @staticmethod\n",
    "    def _inner(cur_dataset, every_n_weeks):\n",
    "        # plus one for one week of validation data\n",
    "        mask = ((cur_dataset.index.isocalendar().week % (every_n_weeks+1)) == 0).values\n",
    "        indices = np.arange(len(cur_dataset))\n",
    "        return list(indices[list(~mask)]), list(indices[list(mask)])\n",
    "    \n",
    "\n",
    "    def __call__(self, o):\n",
    "        return self._inner(o, self.every_n_weeks)\n",
    "    \n",
    "\n",
    "class TrainTestSplitByDays(RenewableSplits):\n",
    "    def __init__(self, test_size: float = 0.25):\n",
    "        self.test_size = test_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def _inner(cur_dataset, test_size):\n",
    "        unique_days = np.unique(cur_dataset.index.date)\n",
    "        train_days, test_days = train_test_split(\n",
    "            unique_days, random_state=42, test_size=0.25\n",
    "        )\n",
    "        train_mask = np.array([True if d in train_days else False for d in cur_dataset.index.date])\n",
    "        \n",
    "        indices = np.arange(len(cur_dataset))\n",
    "        return list(indices[list(train_mask)]), list(indices[list(~train_mask)])\n",
    "    \n",
    "\n",
    "    def __call__(self, o):\n",
    "        return self._inner(o, self.test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabularRenewables(TabularPandas):\n",
    "    def __init__(self, dfs, procs=None, cat_names=None, cont_names=None, do_setup=True, reduce_memory=False,\n",
    "                 y_names=None, add_y_to_x=False, add_x_to_y=False, \n",
    "                 pre_process=None, device=None, splits=None, y_block=RegressionBlock(),\n",
    "                 group_id=\"TaskID\",\n",
    "                inplace=False):\n",
    "\n",
    "        self.pre_process = pre_process\n",
    "        self._original_pre_process = self.pre_process\n",
    "        cont_names = listify(cont_names)\n",
    "        cat_names = listify(cat_names)\n",
    "        y_names = listify(y_names)\n",
    "        self.pre_process = listify(pre_process)\n",
    "        self.group_id = group_id\n",
    "\n",
    "        for pp in listify(procs):\n",
    "            if isinstance(pp, RenewablesTabularProc):\n",
    "                warnings.warn(f\"Element {pp} of procs is RenewablesTabularProc, might not work with TabularPandas.\")\n",
    "         \n",
    "\n",
    "        if len(self.pre_process) > 0:\n",
    "            self.prepared_to = TabularPandas(dfs, y_names=y_names, \n",
    "                                             procs=self.pre_process, cont_names=cont_names,\n",
    "                                          do_setup=True, reduce_memory=False, inplace=inplace, y_block=y_block)\n",
    "            self.pre_process = self.prepared_to.procs\n",
    "            prepared_df = self.prepared_to.items\n",
    "            for pp in self.pre_process: \n",
    "                if getattr(pp, \"include_in_new\", False): _add_prop(self, pp)\n",
    "        else:\n",
    "            prepared_df = dfs\n",
    "\n",
    "        if splits is not None: \n",
    "            if isinstance(splits, RenewableSplits): self.splits = splits(prepared_df)\n",
    "            else: self.splits = splits(range_of(prepared_df))\n",
    "        else:\n",
    "            self.splits = None\n",
    "            \n",
    "        super().__init__(prepared_df,\n",
    "            procs=procs,\n",
    "            cat_names=cat_names,\n",
    "            cont_names=cont_names,\n",
    "            y_names=y_names,\n",
    "            splits=self.splits,\n",
    "            do_setup=do_setup,\n",
    "            inplace=inplace,\n",
    "            y_block=y_block,\n",
    "            reduce_memory=reduce_memory)\n",
    "\n",
    "    def new(self, df, pre_process=None, splits=None, include_preprocess=False):\n",
    "        pre_process = listify(pre_process)\n",
    "        if include_preprocess:\n",
    "            for pp in self._original_pre_process:\n",
    "                if getattr(pp, \"include_in_new\", False):\n",
    "                    pre_process += [pp]\n",
    "                    \n",
    "        return type(self)(df, do_setup=False, reduce_memory=False, y_block=TransformBlock(),\n",
    "                          pre_process=pre_process, splits=splits,\n",
    "                          **attrdict(self, 'procs','cat_names','cont_names','y_names', 'device', 'group_id'))\n",
    "\n",
    "    def show(self, max_n=10, **kwargs):\n",
    "        to_tmp = self.new(self.all_cols[:max_n])\n",
    "        to_tmp.items[\"TaskID\"] = self.items.TaskID[:max_n]\n",
    "        display_df(to_tmp.decode().items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ReadTabBatchRenewables(ItemTransform):\n",
    "    \"Transform `TabularPandas` values into a `Tensor` with the ability to decode\"\n",
    "    def __init__(self, to): self.to = to.new_empty()\n",
    "\n",
    "    def encodes(self, to):\n",
    "        self.task_ids = to.items[[\"TaskID\"]]\n",
    "        if not to.with_cont: res = (tensor(to.cats).long(),)\n",
    "        # TODO: some pre-processing causes to.conts.values of type object, while types\n",
    "        # of the dataframe are float, therefore assure conversion through astype\n",
    "        # --> this is caused by Interpolate\n",
    "        else: res = (tensor(to.cats).long(),tensor(to.conts.astype(float)).float())\n",
    "        ys = [n for n in to.y_names if n in to.items.columns]\n",
    "        # same problem as above with type of to.targ\n",
    "        # preliminary bug fix \n",
    "        # is continous target?\n",
    "        if getattr(to, 'regression_setup', False):\n",
    "            ys_type = float\n",
    "        else:\n",
    "            ys_type = int\n",
    "        if len(ys) == len(to.y_names): res = res + (tensor(to.targ.astype(ys_type)),)\n",
    "        if to.device is not None: res = to_device(res, to.device)\n",
    "        return res\n",
    "\n",
    "    def decodes(self, o):\n",
    "        o = [_maybe_expand(o_) for o_ in to_np(o) if o_.size != 0]\n",
    "        vals = np.concatenate(o, axis=1)\n",
    "        try: df = pd.DataFrame(vals, columns=self.to.all_col_names)\n",
    "        except: df = pd.DataFrame(vals, columns=self.to.x_names)\n",
    "\n",
    "        to = self.to.new(df)\n",
    "        to.items[\"TaskID\"]=self.task_ids.values\n",
    "\n",
    "        return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates()\n",
    "class TabDataLoaderRenewables(TfmdDL):\n",
    "    \"A transformed `DataLoader` for Tabular data\"\n",
    "    def __init__(self, dataset, bs=16, shuffle=False, after_batch=None, num_workers=0, **kwargs):\n",
    "        if after_batch is None: after_batch = L(TransformBlock().batch_tfms)+ReadTabBatchRenewables(dataset)\n",
    "        super().__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n",
    "\n",
    "    def create_batch(self, b): return self.dataset.iloc[b]\n",
    "    def do_item(self, s):      return 0 if s is None else s\n",
    "\n",
    "TabularRenewables._dl_type = TabDataLoaderRenewables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NormalizePerTask(TabularProc):\n",
    "    \"Normalize per TaskId\"\n",
    "    order = 1\n",
    "    include_in_new=True\n",
    "    def __init__(self, task_id_col=\"TaskID\", ignore_cont_cols=[]):\n",
    "        self.task_id_col = task_id_col\n",
    "        self.ignore_cont_cols = ignore_cont_cols\n",
    "    def setups(self, to:Tabular):\n",
    "        self.relevant_cols = L(c for c in to.cont_names if c not in self.ignore_cont_cols)\n",
    "        \n",
    "        self.means = getattr(to, 'train', to)[self.relevant_cols + \"TaskID\"].groupby(\"TaskID\").mean()\n",
    "        self.stds = getattr(to, 'train', to)[self.relevant_cols + \"TaskID\"].groupby(\"TaskID\").std(ddof=0)+1e-7\n",
    "\n",
    "\n",
    "    def encodes(self, to):\n",
    "        to.loc[:, self.relevant_cols] = to.loc[:, self.relevant_cols].astype(np.float64)\n",
    "        for task_id in to.items[self.task_id_col].unique():\n",
    "            # in case this is a new task, we update the means and stds\n",
    "            if task_id not in self.means.index:\n",
    "                mu = getattr(to, 'train', to)[self.relevant_cols + \"TaskID\"].groupby(\"TaskID\").mean()\n",
    "\n",
    "                self.means= self.means.append(mu)\n",
    "                self.stds = self.stds.append(getattr(to, 'train', to)[self.relevant_cols + \"TaskID\"].groupby(\"TaskID\").std(ddof=0)+1e-7)\n",
    "\n",
    "\n",
    "            mask = to.loc[:,self.task_id_col] == task_id\n",
    "\n",
    "            to.loc[mask, self.relevant_cols] = ((to.loc[mask, self.relevant_cols] - self.means.loc[task_id]) / self.stds.loc[task_id])\n",
    "\n",
    "    def decodes(self, to, split_idx=None):\n",
    "        for task_id in to.items[self.task_id_col].unique():\n",
    "            # in case this is a new task, we update the means and stds\n",
    "            if task_id not in self.means.index:\n",
    "                warnings.warn(\"Missing task id, could not decode.\")\n",
    "\n",
    "            mask = to.loc[:,self.task_id_col] == task_id\n",
    "\n",
    "            to.loc[mask, self.relevant_cols] = to.conts[mask] * self.stds.loc[task_id] + self.means.loc[task_id]\n",
    "        return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_task_normalization(index=None, procs=NormalizePerTask, ignore_cols = L()):\n",
    "    df = pd.DataFrame(index=range(1,11), columns = ['A', 'B', 'C'] , \n",
    "                      data=np.array([list(range(1,11)), list(range(11,21)), list(range(21,31))]).T)\n",
    "    if index is not None: df[\"TimeStamps\"] = index\n",
    "    df[\"TaskID\"] = L(1 if i <= 5  else 2 for i in range(1,11))\n",
    "    index = ['2015-01-01-01', '2015-01-01-02', '2015-01-02-03', '2015-02-01-23', '2015-02-01-13',\n",
    "        '2016-01-01-01', '2016-01-01-02', '2016-06-02-03', '2016-02-01-23', '2016-02-01-13'] \n",
    "    df[\"TimeStamps\"] = index\n",
    "    to = TabularRenewables(df, pre_process=CreateTimeStampIndex(col_name=\"TimeStamps\"), \n",
    "                           procs=[NormalizePerTask(ignore_cont_cols=ignore_cols)], cont_names=[\"A\", \"B\"] , y_names=\"C\", \n",
    "                           cat_names=[\"TaskID\"]\n",
    "                          )\n",
    "    df[\"TimeStamps\"] = pd.to_datetime(index, utc=True)\n",
    "    df.set_index(\"TimeStamps\",inplace=True)\n",
    "    return df,to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "original_df, to = get_test_data_task_normalization()\n",
    "test_close(original_df.astype(float).values, to.decode().items.astype(float).values)\n",
    "test_eq(np.array([[3,13],[8,18]]), to.normalize_per_task.means.values)\n",
    "test_close(np.array([[1.41421366, 1.41421366],[1.41421366, 1.41421366]]), to.normalize_per_task.stds.values)\n",
    "test_eq((2,2), to.normalize_per_task.means.values.shape)\n",
    "test_eq((2,2), to.normalize_per_task.stds.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "original_df, to = get_test_data_task_normalization(ignore_cols=L(\"A\"))\n",
    "test_close(original_df.astype(float).values, to.decode().items.astype(float).values)\n",
    "test_eq((2,1), to.normalize_per_task.means.values.shape)\n",
    "test_eq((2,1), to.normalize_per_task.stds.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VerifyAndNormalizeTarget(TabularProc):\n",
    "    \"Normalize per TaskId\"\n",
    "    order = 1\n",
    "    include_in_new=True\n",
    "    def __init__(self, reset_min_value=0.0, reset_max_value=1.05, \\\n",
    "                 max_value_for_normalization=1.5, task_id_col=\"TaskID\",):\n",
    "        self.task_id_col = task_id_col\n",
    "        self.reset_min_value, self.reset_max_value = reset_min_value, reset_max_value\n",
    "        self.max_value_for_normalization = max_value_for_normalization\n",
    "    def setups(self, to:Tabular):\n",
    "        self.relevant_cols = to.y_names\n",
    "        \n",
    "        self.maxs = getattr(to, 'train', to)[self.relevant_cols + \"TaskID\"].groupby(\"TaskID\").max()\n",
    "        self.mins = getattr(to, 'train', to)[self.relevant_cols + \"TaskID\"].groupby(\"TaskID\").min()+1e-9\n",
    "\n",
    "\n",
    "    def encodes(self, to):\n",
    "#         return \n",
    "        to.loc[:, self.relevant_cols] = to.loc[:, self.relevant_cols].astype(np.float64)\n",
    "        for task_id in to.items[self.task_id_col].unique():\n",
    "            # in case this is a new task, we update the maxs and mins\n",
    "            if task_id not in self.maxs.index:\n",
    "                task_max = getattr(to, 'train', to)[self.relevant_cols + \"TaskID\"].groupby(\"TaskID\").max()\n",
    "                task_min = getattr(to, 'train', to)[self.relevant_cols + \"TaskID\"].groupby(\"TaskID\").min()+1e-9\n",
    "    \n",
    "                self.maxs.append(task_max)\n",
    "                self.mins.append(task_min)\n",
    "\n",
    "\n",
    "            mask = to.loc[:,self.task_id_col] == task_id\n",
    "            \n",
    "            \n",
    "            if (to.loc[mask, self.relevant_cols].max() > self.max_value_for_normalization).any():\n",
    "                to.loc[mask, self.relevant_cols] = (to.loc[mask, self.relevant_cols] - self.mins.loc[task_id]) \\\n",
    "                                                            / (self.maxs.loc[task_id] - self.mins.loc[task_id])\n",
    "                                                                                                               \n",
    "            for cur_relevant_column in self.relevant_cols:\n",
    "                to.loc[mask, cur_relevant_column] = to.loc[mask, cur_relevant_column].where(~(to.loc[mask, cur_relevant_column] > self.reset_max_value), \n",
    "                                                       self.reset_max_value)\n",
    "                to.loc[mask, cur_relevant_column] = to.loc[mask, cur_relevant_column].where(~(to.loc[mask, cur_relevant_column] < self.reset_min_value), \n",
    "                                                       self.reset_min_value)\n",
    "        return to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_task_target_normalization(index=None, reset_min_value=0.0, reset_max_value=1.05, \\\n",
    "                 max_value_for_normalization=1.5):\n",
    "    df = pd.DataFrame(index=range(1,11), columns = ['A', 'B', 'C'] , \n",
    "                      data=np.array([list(range(1,11)), list(range(11,21)), list(range(21,31))]).T)\n",
    "    if index is not None: df[\"TimeStamps\"] = index\n",
    "    df[\"TaskID\"] = L(1 if i <= 5  else 2 for i in range(1,11))\n",
    "    index = ['2015-01-01-01', '2015-01-01-02', '2015-01-02-03', '2015-02-01-23', '2015-02-01-13',\n",
    "        '2016-01-01-01', '2016-01-01-02', '2016-06-02-03', '2016-02-01-23', '2016-02-01-13'] \n",
    "    df[\"TimeStamps\"] = index\n",
    "    to = TabularRenewables(df, pre_process=CreateTimeStampIndex(col_name=\"TimeStamps\"), \n",
    "                           procs=[VerifyAndNormalizeTarget(reset_min_value,reset_max_value,\n",
    "                                                           max_value_for_normalization)], \n",
    "                           cont_names=[\"A\"] , y_names=[\"B\", \"C\"], \n",
    "                           cat_names=[\"TaskID\"]\n",
    "                          )\n",
    "    df[\"TimeStamps\"] = pd.to_datetime(index, utc=True)\n",
    "    df.set_index(\"TimeStamps\",inplace=True)\n",
    "    return df,to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,to = get_test_data_task_target_normalization(reset_min_value=0, reset_max_value=1.05, \n",
    "                                                max_value_for_normalization=1.5)\n",
    "# all values are larger than max_value_for_normalization, so all tasks should be normalized\n",
    "test_close(0, to.ys.min())\n",
    "test_close(1, to.ys.max())\n",
    "\n",
    "df,to = get_test_data_task_target_normalization(reset_min_value=22, reset_max_value=29, \n",
    "                                                max_value_for_normalization=35)\n",
    "\n",
    "# everything is lower than 22, so all values should be 22 for feature B\n",
    "test_close(22, to.ys[\"B\"].mean())\n",
    "test_close(22, to.ys[\"C\"].min())\n",
    "test_close(29, to.ys[\"C\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabDataset(fastuple):\n",
    "    \"A dataset from a `TabularRenewable` object\"\n",
    "    # Stolen from https://muellerzr.github.io/fastblog/2020/04/22/TabularNumpy.html\n",
    "    def __init__(self, to):\n",
    "        self.cats = tensor(to_np(to.cats).astype(np.long))\n",
    "        self.conts = tensor(to_np(to.conts).astype(np.float32))\n",
    "        \n",
    "        if getattr(to, 'regression_setup', False):\n",
    "            ys_type = np.float32\n",
    "        else:\n",
    "            ys_type = np.long\n",
    "        self.ys = tensor(to_np(to.ys).astype(ys_type))\n",
    "        \n",
    "        self.cont_names = to.cont_names\n",
    "        self.cat_names = to.cat_names\n",
    "        self.y_names = to.y_names\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx[0]\n",
    "        return self.cats[idx:idx+self.bs], self.conts[idx:idx+self.bs], self.ys[idx:idx+self.bs]\n",
    "\n",
    "    def __len__(self): return len(self.cats)\n",
    "    \n",
    "    def show(self, max_n=10, **kwargs): \n",
    "        df_cont = pd.DataFrame(data=self.conts[:max_n], columns=self.cont_names)\n",
    "        df_cat = pd.DataFrame(data=self.cats[:max_n], columns=self.cat_names)\n",
    "        df_y = pd.DataFrame(data=self.ys[:max_n], columns=self.y_names)\n",
    "        display_df(pd.concat([df_cont, df_cat, df_y], axis=1))\n",
    "        \n",
    "    def show_batch(self, max_n=10, **kwargs): \n",
    "        self.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, bs=32, num_workers=0, device='cuda', \n",
    "                 to_device=True, shuffle=False, drop_last=True,**kwargs):\n",
    "        \"A `DataLoader` based on a `TabDataset\"\n",
    "        device = device if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, \n",
    "                         device=device, drop_last=drop_last, **kwargs)\n",
    "        \n",
    "        self.dataset.bs=bs\n",
    "        if to_device:self.to_device()\n",
    "    \n",
    "    def create_item(self, s): return s\n",
    "    \n",
    "    def to_device(self, device=None):\n",
    "        if device is None: device = self.device\n",
    "        self.dataset.cats.to(device)\n",
    "        self.dataset.conts.to(device)\n",
    "        self.dataset.ys.to(device)\n",
    "    \n",
    "    def create_batch(self, b):\n",
    "        \"Create a batch of data\"\n",
    "        cat, cont, y = self.dataset[b]\n",
    "        return cat.to(self.device), cont.to(self.device), y.to(self.device)\n",
    "\n",
    "    def get_idxs(self):\n",
    "        \"Get index's to select\"\n",
    "        idxs = Inf.count if self.indexed else Inf.nones\n",
    "        if self.n is not None: idxs = list(range(len(self.dataset)))\n",
    "        return idxs\n",
    "\n",
    "    def shuffle_fn(self):\n",
    "        \"Shuffle the interior dataset\"\n",
    "        rng = np.random.permutation(len(self.dataset))\n",
    "        self.dataset.cats = self.dataset.cats[rng]\n",
    "        self.dataset.conts = self.dataset.conts[rng]\n",
    "        self.dataset.ys = self.dataset.ys[rng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabDataLoaders(DataLoaders):\n",
    "    def __init__(self, to, bs=64, val_bs=None, shuffle_train=True, device='cpu', **kwargs):\n",
    "        train_ds = TabDataset(to.train)\n",
    "        valid_ds = TabDataset(to.valid)\n",
    "        val_bs = bs if val_bs is None else val_bs\n",
    "        train = TabDataLoader(train_ds, bs=bs, shuffle=shuffle_train, device=device, **kwargs)\n",
    "        valid = TabDataLoader(valid_ds, bs=val_bs, shuffle=False, device=device, **kwargs)\n",
    "        super().__init__(train, valid, device=device, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets verify that we can the split the data by weeks. \n",
    "Splitting by weeks often allows to achieve an representative validation error while reducing training effort\n",
    "compared to cross validation. In this scenario four weeks are considered as training data and the next week is \n",
    "validation data, then the next four weeks are consisdered as training data, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 06:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 12:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 18:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02 00:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     A  B  C\n",
       "2018-01-01 00:00:00  1  1  1\n",
       "2018-01-01 06:00:00  1  1  1\n",
       "2018-01-01 12:00:00  1  1  1\n",
       "2018-01-01 18:00:00  1  1  1\n",
       "2018-01-02 00:00:00  1  1  1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.date_range(start='1/1/2018', end='31/12/2018', freq='6H')\n",
    "df = pd.DataFrame(columns=[\"A\", \"B\", \"C\"], index=index).fillna(1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = TabularRenewables(df, y_names=\"C\", procs=Normalize, \n",
    "                  pre_process=AddSeasonalFeatures, \n",
    "                  splits=ByWeeksSplitter(every_n_weeks=4)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. the first and fifth week is accounted as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-02-04 00:00:00     5\n",
       "2018-02-04 06:00:00     5\n",
       "2018-02-04 12:00:00     5\n",
       "2018-02-04 18:00:00     5\n",
       "2018-03-05 00:00:00    10\n",
       "2018-03-05 06:00:00    10\n",
       "Name: week, dtype: UInt32"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.valid.items.index.isocalendar().week[24:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dataloader and show a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df, to = get_test_data_task_normalization()\n",
    "dl = to.dataloaders(bs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**normalized data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>-1.414213</td>\n",
       "      <td>-1.414213</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 03:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 23:00:00+00:00</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 13:00:00+00:00</th>\n",
       "      <td>1.414213</td>\n",
       "      <td>1.414213</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00+00:00</th>\n",
       "      <td>-1.414213</td>\n",
       "      <td>-1.414213</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00+00:00</th>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-02 03:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 23:00:00+00:00</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 13:00:00+00:00</th>\n",
       "      <td>1.414213</td>\n",
       "      <td>1.414213</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  A         B   C  TaskID\n",
       "TimeUTC                                                  \n",
       "2015-01-01 01:00:00+00:00 -1.414213 -1.414213  21       1\n",
       "2015-01-01 02:00:00+00:00 -0.707107 -0.707107  22       1\n",
       "2015-01-02 03:00:00+00:00  0.000000  0.000000  23       1\n",
       "2015-02-01 23:00:00+00:00  0.707107  0.707107  24       1\n",
       "2015-02-01 13:00:00+00:00  1.414213  1.414213  25       1\n",
       "2016-01-01 01:00:00+00:00 -1.414213 -1.414213  26       2\n",
       "2016-01-01 02:00:00+00:00 -0.707107 -0.707107  27       2\n",
       "2016-06-02 03:00:00+00:00  0.000000  0.000000  28       2\n",
       "2016-02-01 23:00:00+00:00  0.707107  0.707107  29       2\n",
       "2016-02-01 13:00:00+00:00  1.414213  1.414213  30       2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**decode data (denormalized)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TaskID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 03:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 23:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 13:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-02 03:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 23:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 13:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the original input dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeStamps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 03:00:00+00:00</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 23:00:00+00:00</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 13:00:00+00:00</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00+00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00+00:00</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-02 03:00:00+00:00</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 23:00:00+00:00</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 13:00:00+00:00</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            A   B   C  TaskID\n",
       "TimeStamps                                   \n",
       "2015-01-01 01:00:00+00:00   1  11  21       1\n",
       "2015-01-01 02:00:00+00:00   2  12  22       1\n",
       "2015-01-02 03:00:00+00:00   3  13  23       1\n",
       "2015-02-01 23:00:00+00:00   4  14  24       1\n",
       "2015-02-01 13:00:00+00:00   5  15  25       1\n",
       "2016-01-01 01:00:00+00:00   6  16  26       2\n",
       "2016-01-01 02:00:00+00:00   7  17  27       2\n",
       "2016-06-02 03:00:00+00:00   8  18  28       2\n",
       "2016-02-01 23:00:00+00:00   9  19  29       2\n",
       "2016-02-01 13:00:00+00:00  10  20  30       2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2]], device='cuda:0'),\n",
       " tensor([[ 0.0000,  0.0000],\n",
       "         [ 1.4142,  1.4142],\n",
       "         [-0.7071, -0.7071],\n",
       "         [-0.7071, -0.7071]], device='cuda:0'),\n",
       " tensor([[28.],\n",
       "         [25.],\n",
       "         [22.],\n",
       "         [27.]], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.one_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives an example on how to add a new task, that is normalized based on the first year. E.g. when the features are numerical weather predictions. As those are themselves forecasts, we can always extract the past and use the data for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df, to = get_test_data_task_normalization()\n",
    "original_df[\"TaskID\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeStamps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00+00:00</th>\n",
       "      <td>-1.414213</td>\n",
       "      <td>-1.414213</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00+00:00</th>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 03:00:00+00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 23:00:00+00:00</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 13:00:00+00:00</th>\n",
       "      <td>1.414213</td>\n",
       "      <td>1.414213</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  A         B   C  TaskID\n",
       "TimeStamps                                               \n",
       "2015-01-01 01:00:00+00:00 -1.414213 -1.414213  21       3\n",
       "2015-01-01 02:00:00+00:00 -0.707107 -0.707107  22       3\n",
       "2015-01-02 03:00:00+00:00  0.000000  0.000000  23       3\n",
       "2015-02-01 23:00:00+00:00  0.707107  0.707107  24       3\n",
       "2015-02-01 13:00:00+00:00  1.414213  1.414213  25       3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment=None\n",
    "# setups task normalization\n",
    "to_new = to.new(original_df, pre_process=FilterYear(2016, drop=True))\n",
    "to_new.process()\n",
    "to_new.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.440892e-17</td>\n",
       "      <td>-4.440892e-17</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.118034e+00</td>\n",
       "      <td>1.118034e+00</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.414213e+00</td>\n",
       "      <td>-1.414213e+00</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.071067e-01</td>\n",
       "      <td>-7.071067e-01</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.071067e-01</td>\n",
       "      <td>7.071067e-01</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.414213e+00</td>\n",
       "      <td>1.414213e+00</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A             B          C  TaskID\n",
       "count  5.000000e+00  5.000000e+00   5.000000     5.0\n",
       "mean  -4.440892e-17 -4.440892e-17  23.000000     3.0\n",
       "std    1.118034e+00  1.118034e+00   1.581139     0.0\n",
       "min   -1.414213e+00 -1.414213e+00  21.000000     3.0\n",
       "25%   -7.071067e-01 -7.071067e-01  22.000000     3.0\n",
       "50%    0.000000e+00  0.000000e+00  23.000000     3.0\n",
       "75%    7.071067e-01  7.071067e-01  24.000000     3.0\n",
       "max    1.414213e+00  1.414213e+00  25.000000     3.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_new.items.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the second year based on the mean and std of the first year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_new = to.new(original_df, pre_process=FilterYear(2016, drop=False))\n",
    "to_new.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data has larger values in the second year, the normalization is quite off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeStamps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00+00:00</th>\n",
       "      <td>2.121320</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00+00:00</th>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-02 03:00:00+00:00</th>\n",
       "      <td>3.535534</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 23:00:00+00:00</th>\n",
       "      <td>4.242640</td>\n",
       "      <td>4.242640</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01 13:00:00+00:00</th>\n",
       "      <td>4.949747</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  A         B   C  TaskID\n",
       "TimeStamps                                               \n",
       "2016-01-01 01:00:00+00:00  2.121320  2.121320  26       3\n",
       "2016-01-01 02:00:00+00:00  2.828427  2.828427  27       3\n",
       "2016-06-02 03:00:00+00:00  3.535534  3.535534  28       3\n",
       "2016-02-01 23:00:00+00:00  4.242640  4.242640  29       3\n",
       "2016-02-01 13:00:00+00:00  4.949747  4.949747  30       3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_new.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be seen in the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>TaskID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.535534</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.118034</td>\n",
       "      <td>1.118034</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.121320</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.535534</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.242640</td>\n",
       "      <td>4.242640</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.949747</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              A         B          C  TaskID\n",
       "count  5.000000  5.000000   5.000000     5.0\n",
       "mean   3.535534  3.535534  28.000000     3.0\n",
       "std    1.118034  1.118034   1.581139     0.0\n",
       "min    2.121320  2.121320  26.000000     3.0\n",
       "25%    2.828427  2.828427  27.000000     3.0\n",
       "50%    3.535534  3.535534  28.000000     3.0\n",
       "75%    4.242640  4.242640  29.000000     3.0\n",
       "max    4.949747  4.949747  30.000000     3.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_new.items.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a dataloader and verify if we can display the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TaskID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_new.dataloaders(bs=4).show_batch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
