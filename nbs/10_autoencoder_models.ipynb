{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models.autoencoders\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from fastrenewables.tabular.model import *\n",
    "from fastrenewables.timeseries.model import *\n",
    "from fastai.tabular.all import *\n",
    "from torch.autograd import Variable\n",
    "from sklearn.datasets import make_regression\n",
    "from fastai.learner import *\n",
    "from fastrenewables.utils_pytorch import *\n",
    "from fastrenewables.losses import VAEReconstructionLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        z = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "        \n",
    "    \n",
    "    def decode(self, categorical_data, continuous_data, as_np=False):\n",
    "        x = self.decoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(x)\n",
    "        else: return x\n",
    "        \n",
    "    def forward(self, categorical_data, continuous_data):\n",
    "        x = self.encode(categorical_data, continuous_data)\n",
    "        x = self.decode(categorical_data, x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some data that we wann to compress through an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2000\n",
    "X, y = make_regression(n_samples=N, n_features=20, n_informative=15)\n",
    "df = pd.DataFrame(X)\n",
    "x_names = [str(c) for c in df.columns]\n",
    "df.columns = x_names\n",
    "df[\"y\"] = (y - y.min())/(y.max()-y.min())\n",
    "dls = TabularDataLoaders.from_df(df, cont_names=x_names, y_names=x_names, deivce=\"cpu\", procs=Normalize, bs=N//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2000.000000\n",
       "mean        0.518202\n",
       "std         0.160907\n",
       "min         0.000000\n",
       "25%         0.408547\n",
       "50%         0.520172\n",
       "75%         0.626163\n",
       "max         1.000000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 20]), torch.Size([200, 20]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.one_batch()[1].shape, dls.one_batch()[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = get_c(dls)\n",
    "ann_structure = [num_features, num_features*5, 5]\n",
    "ae = Autoencoder(MultiLayerPerceptron(ann_structure), MultiLayerPerceptron(ann_structure[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, ae, metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.417418</td>\n",
       "      <td>1.028195</td>\n",
       "      <td>1.013999</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.142217</td>\n",
       "      <td>0.976013</td>\n",
       "      <td>0.987934</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.022015</td>\n",
       "      <td>0.859396</td>\n",
       "      <td>0.927036</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.950629</td>\n",
       "      <td>0.816512</td>\n",
       "      <td>0.903610</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.901302</td>\n",
       "      <td>0.787643</td>\n",
       "      <td>0.887493</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.865102</td>\n",
       "      <td>0.772275</td>\n",
       "      <td>0.878792</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.838625</td>\n",
       "      <td>0.774557</td>\n",
       "      <td>0.880089</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.817726</td>\n",
       "      <td>0.765040</td>\n",
       "      <td>0.874666</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.800567</td>\n",
       "      <td>0.758330</td>\n",
       "      <td>0.870822</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.786278</td>\n",
       "      <td>0.760223</td>\n",
       "      <td>0.871907</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast based on latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a model that is a wrapper for an autoencoder to forecast a regression or classification based on the latent space from an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderForecast(nn.Module):\n",
    "    def __init__(self, autoencoder, forecast_model):\n",
    "        super().__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.forecast_model = forecast_model\n",
    "        \n",
    "    def forward(self, categorical_data, continuous_data):\n",
    "        \n",
    "        latent_space = self.autoencoder.encode(categorical_data, continuous_data)\n",
    "        yhat = self.forecast_model(categorical_data, latent_space)\n",
    "        \n",
    "        return yhat\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the data loader that has the target feature as output in the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (encoder): (\n",
      "  (Identity())\n",
      "  (ModuleList())\n",
      "  (Dropout(p=0.0, inplace=False))\n",
      "  (BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Requires grad: False\n",
      "   (layers): (\n",
      "    Sequential (0): (\n",
      "      (Linear(in_features=20, out_features=100, bias=False)) Requires grad: False\n",
      "      (ReLU(inplace=True))\n",
      "      (BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Requires grad: False\n",
      "    )\n",
      "    Sequential (1): (\n",
      "      (Linear(in_features=100, out_features=5, bias=True)) Requires grad: False\n",
      "    )\n",
      "  )\n",
      ")\n",
      " (decoder): (\n",
      "  (Identity())\n",
      "  (ModuleList())\n",
      "  (Dropout(p=0.0, inplace=False))\n",
      "  (BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Requires grad: False\n",
      "   (layers): (\n",
      "    Sequential (0): (\n",
      "      (Linear(in_features=5, out_features=100, bias=False)) Requires grad: False\n",
      "      (ReLU(inplace=True))\n",
      "      (BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Requires grad: False\n",
      "    )\n",
      "    Sequential (1): (\n",
      "      (Linear(in_features=100, out_features=20, bias=True)) Requires grad: False\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print_requires_grad(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TabularDataLoaders.from_df(df, cont_names=x_names, y_names=\"y\", deivce=\"cpu\",procs=Normalize, bs=N//10)\n",
    "mlp_regression = MultiLayerPerceptron([ann_structure[-1], get_c(dls)])\n",
    "model = AutoencoderForecast(learn.model, mlp_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AutoencoderForecast (Input shape: 200 x torch.Size([200, 20]))\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     []                  \n",
       "Identity                                                       \n",
       "BatchNorm1d                               40         False     \n",
       "____________________________________________________________________________\n",
       "                     200 x 100           \n",
       "Linear                                    2000       False     \n",
       "ReLU                                                           \n",
       "BatchNorm1d                               200        False     \n",
       "____________________________________________________________________________\n",
       "                     200 x 5             \n",
       "Linear                                    505        False     \n",
       "ReLU                                                           \n",
       "Identity                                                       \n",
       "BatchNorm1d                               10         True      \n",
       "____________________________________________________________________________\n",
       "                     200 x 1             \n",
       "Linear                                    6          True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 2,761\n",
       "Total trainable params: 16\n",
       "Total non-trainable params: 2,745\n",
       "\n",
       "Optimizer used: <function Adam at 0x7fb6c4cf3820>\n",
       "Loss function: FlattenedLoss of MSELoss()\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.326834</td>\n",
       "      <td>0.217491</td>\n",
       "      <td>0.466360</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.235854</td>\n",
       "      <td>0.111830</td>\n",
       "      <td>0.334410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.181198</td>\n",
       "      <td>0.065184</td>\n",
       "      <td>0.255312</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.142332</td>\n",
       "      <td>0.039793</td>\n",
       "      <td>0.199481</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.114708</td>\n",
       "      <td>0.032569</td>\n",
       "      <td>0.180468</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5, lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "        \n",
    "    def forward(self, input, dims):\n",
    "        return input.view(*dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(Autoencoder):\n",
    "    def __init__(self, encoder, decoder, h_dim, z_dim):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.flatten = Flatten()\n",
    "        self.unflatten = UnFlatten()\n",
    "        \n",
    "        self.hidden2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hidden2logvar = nn.Linear(h_dim, z_dim)\n",
    "        self.latent_dimensions = None\n",
    "        self._mu, self._logvar = None, None\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        \n",
    "        x_hidden = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        self.latent_dimensions = x_hidden.shape\n",
    "        \n",
    "        x_hidden = self.flatten(x_hidden)\n",
    "        \n",
    "        mu, logvar = self.hidden2mu(x_hidden), self.hidden2logvar(x_hidden)\n",
    "        \n",
    "        # required for vae loss\n",
    "        self._mu, self._logvar = mu, logvar\n",
    "        \n",
    "        z = self.reparam(mu, logvar)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "\n",
    "    def decode(self, categorical_data, continuous_data, as_np=False, latent_dimensions=None):\n",
    "        \n",
    "        if not latent_dimensions and not self.latent_dimensions:\n",
    "            raise ValueError(\"latent_dimensions are not set to unflatten data.\")\n",
    "        if not latent_dimensions:\n",
    "            latent_dimensions = self.latent_dimensions\n",
    "            \n",
    "        x = self.unflatten(continuous_data, latent_dimensions)\n",
    "        \n",
    "        x = self.decoder(categorical_data, x)\n",
    "        \n",
    "        if as_np: return to_np(x)\n",
    "        else: return x\n",
    "        \n",
    "    def get_posteriors(self, categorical_data, continuous_data):\n",
    "\n",
    "        return self.encode(continuous_data, categorical_data)\n",
    "\n",
    "    def get_z(self, categorical_data, continuous_data):\n",
    "        \"\"\"Encode a batch of data points, x, into their z representations.\"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(categorical_data, continuous_data)\n",
    "        \n",
    "        return self.reparam(mu, logvar)\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        \"\"\"Reparameterisation trick to sample z values.\n",
    "        This is stochastic during training, and returns the mode during evaluation.\"\"\"\n",
    "\n",
    "        if self.training:\n",
    "            # convert logarithmic variance to standard deviation representation\n",
    "            std = torch.exp(logvar / 2)\n",
    "            \n",
    "            # create normal distribution as large as the data\n",
    "            eps = torch.randn_like(std)\n",
    "            # scale by learned mean and standard deviation\n",
    "            return mu + eps*std\n",
    "        else:\n",
    "            return mu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TabularDataLoaders.from_df(df, cont_names=x_names, y_names=x_names, deivce=\"cpu\", procs=Normalize, bs=N//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = get_c(dls)\n",
    "ann_structure = [num_features, num_features*5, 5]\n",
    "ae = VariationalAutoencoder(MultiLayerPerceptron(ann_structure), \n",
    "                            MultiLayerPerceptron(ann_structure[::-1]), \n",
    "                            ann_structure[-1], ann_structure[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, ae, loss_func=VAEReconstructionLoss(ae), metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.603892</td>\n",
       "      <td>1.039223</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.352651</td>\n",
       "      <td>1.030828</td>\n",
       "      <td>0.993362</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.233508</td>\n",
       "      <td>1.002076</td>\n",
       "      <td>0.987172</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.165651</td>\n",
       "      <td>0.984741</td>\n",
       "      <td>0.977986</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.121756</td>\n",
       "      <td>0.970083</td>\n",
       "      <td>0.971174</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.090632</td>\n",
       "      <td>0.965951</td>\n",
       "      <td>0.966367</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.068473</td>\n",
       "      <td>0.957949</td>\n",
       "      <td>0.961562</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.051438</td>\n",
       "      <td>0.955730</td>\n",
       "      <td>0.956740</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.037688</td>\n",
       "      <td>0.954759</td>\n",
       "      <td>0.954648</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.027215</td>\n",
       "      <td>0.944164</td>\n",
       "      <td>0.947959</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast based on latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TabularDataLoaders.from_df(df, cont_names=x_names, y_names=\"y\", deivce=\"cpu\",procs=Normalize, bs=N//10)\n",
    "mlp_regression = MultiLayerPerceptron([ann_structure[-1], get_c(dls)])\n",
    "model = AutoencoderForecast(learn.model, mlp_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AutoencoderForecast (Input shape: 200 x torch.Size([200, 20]))\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     []                  \n",
       "Identity                                                       \n",
       "BatchNorm1d                               40         False     \n",
       "____________________________________________________________________________\n",
       "                     200 x 100           \n",
       "Linear                                    2000       False     \n",
       "ReLU                                                           \n",
       "BatchNorm1d                               200        False     \n",
       "____________________________________________________________________________\n",
       "                     200 x 5             \n",
       "Linear                                    505        False     \n",
       "ReLU                                                           \n",
       "Flatten                                                        \n",
       "Linear                                    30         False     \n",
       "Linear                                    30         False     \n",
       "Identity                                                       \n",
       "BatchNorm1d                               10         True      \n",
       "____________________________________________________________________________\n",
       "                     200 x 1             \n",
       "Linear                                    6          True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 2,821\n",
       "Total trainable params: 16\n",
       "Total non-trainable params: 2,805\n",
       "\n",
       "Optimizer used: <function Adam at 0x7fb6c4cf3820>\n",
       "Loss function: FlattenedLoss of MSELoss()\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, model, metrics=rmse)\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.243670</td>\n",
       "      <td>0.117833</td>\n",
       "      <td>0.343267</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.176788</td>\n",
       "      <td>0.073938</td>\n",
       "      <td>0.271916</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.134227</td>\n",
       "      <td>0.046644</td>\n",
       "      <td>0.215971</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.106050</td>\n",
       "      <td>0.029965</td>\n",
       "      <td>0.173103</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.085935</td>\n",
       "      <td>0.022111</td>\n",
       "      <td>0.148699</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5, lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks that the autoencoders also work with temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = 24\n",
    "n_features = 10\n",
    "n_samples = 3\n",
    "latent_dim  = 2 \n",
    "ann_structure = [10, latent_dim]\n",
    "\n",
    "x = torch.randn(( n_samples, n_features,ts_length), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_tcn = Autoencoder(TemporalCNN(ann_structure), TemporalCNN(ann_structure[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = ae_tcn(None, x)\n",
    "\n",
    "test_eq(True, yhat.requires_grad)\n",
    "test_eq([n_samples, n_features, ts_length], list(yhat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_tcn = VariationalAutoencoder(TemporalCNN(ann_structure), \n",
    "                                TemporalCNN(ann_structure[::-1]),\n",
    "                               ann_structure[-1]*ts_length, ann_structure[-1]*ts_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = vae_tcn(None, x)\n",
    "\n",
    "test_eq(True, yhat.requires_grad)\n",
    "test_eq([n_samples, n_features, ts_length], list(yhat.shape))\n",
    "test_eq([n_samples, latent_dim*ts_length], list(vae_tcn._mu.shape))\n",
    "test_eq([n_samples, latent_dim*ts_length], list(vae_tcn._logvar.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
