{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models.autoencoders\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from fastrenewables.tabular.model import *\n",
    "from fastrenewables.timeseries.model import *\n",
    "from fastai.tabular.all import *\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_structure = [10,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        z = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "        \n",
    "    \n",
    "    def decode(self, categorical_data, continuous_data, as_np=False):\n",
    "        x = self.decoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(x)\n",
    "        else: return x\n",
    "        \n",
    "    def forward(self, categorical_data, continuous_data):\n",
    "        x = self.encode(categorical_data, continuous_data)\n",
    "        x = self.decode(categorical_data, x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = Autoencoder(MultiLayerPerceptron(ann_structure), MultiLayerPerceptron(ann_structure[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (final_activation): Identity()\n",
       "  (embeds): ModuleList()\n",
       "  (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): LinBnDrop(\n",
       "      (0): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10), requires_grad=True)\n",
    "yhat = ae(None, x)\n",
    "yhat.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2488,  0.7525, -0.8032,  0.1610, -0.7692, -1.1662, -0.3459, -0.8292,\n",
       "         -0.9420,  0.3740],\n",
       "        [ 0.2194, -0.1126, -0.9293,  0.4079, -0.3220,  1.0201,  0.1839,  0.1158,\n",
       "          0.0490, -0.7282],\n",
       "        [ 0.7240,  0.1649, -0.1558,  1.1118, -0.8921, -0.0425, -0.0862, -0.7931,\n",
       "         -0.4248,  0.2214]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): TemporalCNN(\n",
       "    (bn_cont): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layers): TemporalConvNet(\n",
       "      (temporal_blocks): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv1d(10, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp1): Chomp1d()\n",
       "          (act_func1): Identity()\n",
       "          (dropout1): Dropout2d(p=0.0, inplace=False)\n",
       "          (conv2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp2): Chomp1d()\n",
       "          (act_func2): Identity()\n",
       "          (dropout2): Dropout2d(p=0.0, inplace=False)\n",
       "          (net): Sequential(\n",
       "            (0): Conv1d(10, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (1): Chomp1d()\n",
       "            (2): Identity()\n",
       "            (3): Dropout2d(p=0.0, inplace=False)\n",
       "            (4): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (5): Chomp1d()\n",
       "            (6): Identity()\n",
       "            (7): Dropout2d(p=0.0, inplace=False)\n",
       "          )\n",
       "          (downsample): Conv1d(10, 2, kernel_size=(1,), stride=(1,))\n",
       "          (act_func3): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TemporalCNN(\n",
       "    (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layers): TemporalConvNet(\n",
       "      (temporal_blocks): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv1d(2, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp1): Chomp1d()\n",
       "          (act_func1): Identity()\n",
       "          (dropout1): Dropout2d(p=0.0, inplace=False)\n",
       "          (conv2): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp2): Chomp1d()\n",
       "          (act_func2): Identity()\n",
       "          (dropout2): Dropout2d(p=0.0, inplace=False)\n",
       "          (net): Sequential(\n",
       "            (0): Conv1d(2, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (1): Chomp1d()\n",
       "            (2): Identity()\n",
       "            (3): Dropout2d(p=0.0, inplace=False)\n",
       "            (4): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (5): Chomp1d()\n",
       "            (6): Identity()\n",
       "            (7): Dropout2d(p=0.0, inplace=False)\n",
       "          )\n",
       "          (downsample): Conv1d(2, 10, kernel_size=(1,), stride=(1,))\n",
       "          (act_func3): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_tcn = Autoencoder(TemporalCNN(ann_structure), TemporalCNN(ann_structure[::-1]))\n",
    "ae_tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([3, 10, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10,2), requires_grad=True)\n",
    "yhat = ae_tcn(None, x)\n",
    "yhat.requires_grad, yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0477,  0.0091],\n",
       "        [ 0.1478,  0.0880],\n",
       "        [-0.2532, -0.1563],\n",
       "        [-0.2404, -0.1595],\n",
       "        [ 0.4216,  0.1389],\n",
       "        [ 0.4037,  0.1737],\n",
       "        [-0.3920, -0.1086],\n",
       "        [ 0.1082, -0.0079],\n",
       "        [-0.3735, -0.1348],\n",
       "        [-0.0016, -0.0938]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "#     def __init__(self, size):\n",
    "#         self.size = size\n",
    "        \n",
    "    def forward(self, input, dims):\n",
    "        return input.view(*dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(Autoencoder):\n",
    "    def __init__(self, encoder, decoder, h_dim, z_dim):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.flatten = Flatten()\n",
    "        self.unflatten = UnFlatten()\n",
    "        \n",
    "        self.hidden2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hidden2logvar = nn.Linear(h_dim, z_dim)\n",
    "        self.latent_dimensions = None\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        \n",
    "        x_hidden = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        self.latent_dimensions = x_hidden.shape\n",
    "        \n",
    "        x_hidden = self.flatten(x_hidden)\n",
    "        \n",
    "        mu, logvar = self.hidden2mu(x_hidden), self.hidden2logvar(x_hidden)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "        \n",
    "    def decode(self, categorical_data, continuous_data, as_np=False, latent_dimensions=None):\n",
    "        \n",
    "        if not latent_dimensions and not self.latent_dimensions:\n",
    "            raise ValueError(\"latent_dimensions are not set to unflatten data.\")\n",
    "        if not latent_dimensions:\n",
    "            latent_dimensions = self.latent_dimensions\n",
    "            \n",
    "        x = self.unflatten(continuous_data, latent_dimensions)\n",
    "        \n",
    "        x = self.decoder(categorical_data, x)\n",
    "        \n",
    "        if as_np: return to_np(x)\n",
    "        else: return x\n",
    "        \n",
    "    def get_posteriors(self, categorical_data, continuous_data):\n",
    "\n",
    "        return self.encode(continuous_data, categorical_data)\n",
    "\n",
    "    def get_z(self, categorical_data, continuous_data):\n",
    "        \"\"\"Encode a batch of data points, x, into their z representations.\"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(categorical_data, continuous_data)\n",
    "        return self.reparam(mu, logvar)\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        \"\"\"Reparameterisation trick to sample z values.\n",
    "        This is stochastic during training, and returns the mode during evaluation.\"\"\"\n",
    "\n",
    "        if self.training:\n",
    "            # convert logarithmic variance to standard deviation representation\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            # create normal distribution as large as the data\n",
    "#             eps = Variable(std.data.new(std.size()).normal_())\n",
    "            eps = torch.randn_like(std)\n",
    "            # scale by learned mean and standard deviation\n",
    "            return mu + eps*std\n",
    "        else:\n",
    "            return mu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10), requires_grad=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = MultiLayerPerceptron(ann_structure)\n",
    "dec = MultiLayerPerceptron(ann_structure[::-1])\n",
    "\n",
    "vae = VariationalAutoencoder(enc, dec, ann_structure[-1], ann_structure[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6188,  0.3713,  1.0308, -2.3945,  1.4508,  0.5034,  0.6243, -0.4527,\n",
       "          0.3466,  1.2480],\n",
       "        [-0.5371, -0.0545, -0.1983,  0.4750, -0.2552, -0.8372,  0.4712, -0.5721,\n",
       "          0.2953, -1.6316],\n",
       "        [-0.7446,  0.3116,  0.0760,  0.2557, -0.3510, -0.3959,  0.5713, -0.6517,\n",
       "          0.4867, -1.4144]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.training = True\n",
    "vae(None, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_tcn = VariationalAutoencoder(TemporalCNN(ann_structure), \n",
    "                                TemporalCNN(ann_structure[::-1]),\n",
    "                               ann_structure[-1]*ts_length, ann_structure[-1]*ts_length)\n",
    "# ae_tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([3, 10, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10,ts_length), requires_grad=True)\n",
    "yhat = ae_tcn(None, x)\n",
    "yhat.requires_grad, yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6324,  0.4716],\n",
       "         [-0.1380,  0.1203],\n",
       "         [-0.0890,  0.6074],\n",
       "         [ 0.3778, -0.5755],\n",
       "         [-0.1370,  0.2466],\n",
       "         [ 0.0231,  0.0706],\n",
       "         [ 0.4997, -0.6765],\n",
       "         [-0.3776,  0.7519],\n",
       "         [-0.3856,  0.3849],\n",
       "         [ 0.5312, -0.6880]],\n",
       "\n",
       "        [[-0.6352, -0.9405],\n",
       "         [ 0.9391, -0.8131],\n",
       "         [ 1.7122, -0.7292],\n",
       "         [-0.6994,  0.7353],\n",
       "         [-0.8056,  0.0767],\n",
       "         [ 0.1449,  0.0605],\n",
       "         [-0.8067,  0.9382],\n",
       "         [ 1.1164, -0.6610],\n",
       "         [-0.5468, -0.5982],\n",
       "         [-0.4496,  1.0455]],\n",
       "\n",
       "        [[ 0.4085,  1.0145],\n",
       "         [ 0.0333, -0.5072],\n",
       "         [-0.0504, -1.4090],\n",
       "         [-0.2253,  0.2217],\n",
       "         [ 0.2016,  0.7672],\n",
       "         [-0.0300, -0.1834],\n",
       "         [-0.2160,  0.3513],\n",
       "         [ 0.1610, -0.7039],\n",
       "         [ 0.1965,  0.5474],\n",
       "         [-0.3160, -0.0773]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "**should work for tcn and mlp**\n",
    "- aletoric uncertainty layer/wrapper \n",
    "    wrapper.forward(x)\n",
    "        mu = model(x)\n",
    "        std = softmax(x)\n",
    "        return mu, std\n",
    "        \n",
    "- aletoric uncertainty loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
