{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models.autoencoders\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from fastrenewables.tabular.model import *\n",
    "from fastrenewables.timeseries.model import *\n",
    "from fastai.tabular.all import *\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_structure = [10,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        z = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "        \n",
    "    \n",
    "    def decode(self, categorical_data, continuous_data, as_np=False):\n",
    "        x = self.decoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(x)\n",
    "        else: return x\n",
    "        \n",
    "    def forward(self, categorical_data, continuous_data):\n",
    "        x = self.encode(categorical_data, continuous_data)\n",
    "        x = self.decode(categorical_data, x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = Autoencoder(MultiLayerPerceptron(ann_structure), MultiLayerPerceptron(ann_structure[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (final_activation): Identity()\n",
       "  (embeds): ModuleList()\n",
       "  (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): LinBnDrop(\n",
       "      (0): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10), requires_grad=True)\n",
    "yhat = ae(None, x)\n",
    "yhat.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9132, -0.1884, -0.2194, -0.2023,  1.1451, -0.1748,  1.5637, -1.6007,\n",
       "         -0.0047, -1.0678],\n",
       "        [ 1.1966, -0.2939,  1.4584,  0.7007,  0.5463,  0.9522, -0.0652,  0.3513,\n",
       "         -0.1976, -1.0404],\n",
       "        [-0.5589,  1.6906,  0.7701,  1.5558,  0.2138, -0.0460,  0.2347, -0.7715,\n",
       "         -0.7836,  0.4259]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): TemporalCNN(\n",
       "    (bn_cont): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layers): TemporalConvNet(\n",
       "      (temporal_blocks): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv1d(10, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp1): Chomp1d()\n",
       "          (act_func1): Identity()\n",
       "          (dropout1): Dropout2d(p=0.0, inplace=False)\n",
       "          (conv2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp2): Chomp1d()\n",
       "          (act_func2): Identity()\n",
       "          (dropout2): Dropout2d(p=0.0, inplace=False)\n",
       "          (net): Sequential(\n",
       "            (0): Conv1d(10, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (1): Chomp1d()\n",
       "            (2): Identity()\n",
       "            (3): Dropout2d(p=0.0, inplace=False)\n",
       "            (4): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (5): Chomp1d()\n",
       "            (6): Identity()\n",
       "            (7): Dropout2d(p=0.0, inplace=False)\n",
       "          )\n",
       "          (downsample): Conv1d(10, 2, kernel_size=(1,), stride=(1,))\n",
       "          (act_func3): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TemporalCNN(\n",
       "    (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layers): TemporalConvNet(\n",
       "      (temporal_blocks): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv1d(2, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp1): Chomp1d()\n",
       "          (act_func1): Identity()\n",
       "          (dropout1): Dropout2d(p=0.0, inplace=False)\n",
       "          (conv2): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp2): Chomp1d()\n",
       "          (act_func2): Identity()\n",
       "          (dropout2): Dropout2d(p=0.0, inplace=False)\n",
       "          (net): Sequential(\n",
       "            (0): Conv1d(2, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (1): Chomp1d()\n",
       "            (2): Identity()\n",
       "            (3): Dropout2d(p=0.0, inplace=False)\n",
       "            (4): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (5): Chomp1d()\n",
       "            (6): Identity()\n",
       "            (7): Dropout2d(p=0.0, inplace=False)\n",
       "          )\n",
       "          (downsample): Conv1d(2, 10, kernel_size=(1,), stride=(1,))\n",
       "          (act_func3): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_tcn = Autoencoder(TemporalCNN(ann_structure), TemporalCNN(ann_structure[::-1]))\n",
    "ae_tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([3, 10, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10,2), requires_grad=True)\n",
    "yhat = ae_tcn(None, x)\n",
    "yhat.requires_grad, yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1177, -0.6364],\n",
       "        [ 0.3726,  0.0438],\n",
       "        [-0.1339,  0.5839],\n",
       "        [ 0.1305,  0.5801],\n",
       "        [-0.4334,  0.1272],\n",
       "        [-0.1354, -0.2221],\n",
       "        [-0.4284,  0.4048],\n",
       "        [-0.3801,  0.1147],\n",
       "        [ 0.5937, -0.0437],\n",
       "        [ 0.2090, -0.1368]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "#     def __init__(self, size):\n",
    "#         self.size = size\n",
    "        \n",
    "    def forward(self, input, dims):\n",
    "        return input.view(*dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(Autoencoder):\n",
    "    def __init__(self, encoder, decoder, h_dim, z_dim):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.flatten = Flatten()\n",
    "        self.unflatten = UnFlatten()\n",
    "        \n",
    "        self.hidden2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hidden2logvar = nn.Linear(h_dim, z_dim)\n",
    "        self.latent_dimensions = None\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        \n",
    "        x_hidden = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        self.latent_dimensions = x_hidden.shape\n",
    "        \n",
    "        x_hidden = self.flatten(x_hidden)\n",
    "        \n",
    "        mu, logvar = self.hidden2mu(x_hidden), self.hidden2logvar(x_hidden)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "        \n",
    "    def decode(self, categorical_data, continuous_data, as_np=False, latent_dimensions=None):\n",
    "        \n",
    "        if not latent_dimensions and not self.latent_dimensions:\n",
    "            raise ValueError(\"latent_dimensions are not set to unflatten data.\")\n",
    "        if not latent_dimensions:\n",
    "            latent_dimensions = self.latent_dimensions\n",
    "            \n",
    "        x = self.unflatten(continuous_data, latent_dimensions)\n",
    "        \n",
    "        x = self.decoder(categorical_data, x)\n",
    "        \n",
    "        if as_np: return to_np(x)\n",
    "        else: return x\n",
    "        \n",
    "    def get_posteriors(self, categorical_data, continuous_data):\n",
    "\n",
    "        return self.encode(continuous_data, categorical_data)\n",
    "\n",
    "    def get_z(self, categorical_data, continuous_data):\n",
    "        \"\"\"Encode a batch of data points, x, into their z representations.\"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(categorical_data, continuous_data)\n",
    "        return self.reparam(mu, logvar)\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        \"\"\"Reparameterisation trick to sample z values.\n",
    "        This is stochastic during training, and returns the mode during evaluation.\"\"\"\n",
    "\n",
    "        if self.training:\n",
    "            # convert logarithmic variance to standard deviation representation\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            # create normal distribution as large as the data\n",
    "#             eps = Variable(std.data.new(std.size()).normal_())\n",
    "            eps = torch.randn_like(std)\n",
    "            # scale by learned mean and standard deviation\n",
    "            return mu + eps*std\n",
    "        else:\n",
    "            return mu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10), requires_grad=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = MultiLayerPerceptron(ann_structure)\n",
    "dec = MultiLayerPerceptron(ann_structure[::-1])\n",
    "\n",
    "vae = VariationalAutoencoder(enc, dec, ann_structure[-1], ann_structure[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5253,  0.3163,  0.7156, -0.6541, -0.1933, -0.1231,  0.3033,  0.6065,\n",
       "          0.2843,  0.4410],\n",
       "        [-0.2915,  0.2880, -1.1070,  0.0281,  1.3649, -0.6017,  0.3194, -0.6601,\n",
       "         -0.3117, -1.2946],\n",
       "        [ 0.0179,  0.4681,  0.3269,  0.1630,  0.0256, -0.2277,  1.0325,  0.3727,\n",
       "         -0.2203,  0.4150]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.training = True\n",
    "vae(None, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_tcn = VariationalAutoencoder(TemporalCNN(ann_structure), \n",
    "                                TemporalCNN(ann_structure[::-1]),\n",
    "                               ann_structure[-1]*ts_length, ann_structure[-1]*ts_length)\n",
    "# ae_tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([3, 10, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10,ts_length), requires_grad=True)\n",
    "yhat = ae_tcn(None, x)\n",
    "yhat.requires_grad, yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3432,  0.2241],\n",
       "         [-0.7341, -0.0931],\n",
       "         [-1.1590, -0.2387],\n",
       "         [ 0.4893, -0.0063],\n",
       "         [-0.1362,  0.0840],\n",
       "         [ 0.3904,  0.0279],\n",
       "         [-0.0910, -0.2486],\n",
       "         [ 0.1789,  0.2125],\n",
       "         [-0.0817,  0.3451],\n",
       "         [ 0.3341, -0.1267]],\n",
       "\n",
       "        [[ 0.3429, -1.0165],\n",
       "         [ 0.2852,  0.6506],\n",
       "         [ 0.6184,  1.8768],\n",
       "         [-0.8920, -0.6352],\n",
       "         [ 1.3292, -0.6065],\n",
       "         [-0.4793, -0.6716],\n",
       "         [-1.3048,  0.7625],\n",
       "         [ 0.6786, -0.9692],\n",
       "         [ 1.2184, -0.3211],\n",
       "         [-1.3480,  0.3064]],\n",
       "\n",
       "        [[ 0.2466, -0.4678],\n",
       "         [-0.3391, -0.1414],\n",
       "         [-0.4986, -0.2558],\n",
       "         [ 0.0832,  0.6815],\n",
       "         [ 0.1803, -1.3973],\n",
       "         [ 0.1181,  0.3336],\n",
       "         [-0.2996,  1.3271],\n",
       "         [ 0.2207, -0.7973],\n",
       "         [ 0.1890, -1.0958],\n",
       "         [-0.0813,  1.3002]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "**should work for tcn and mlp**\n",
    "- aletoric uncertainty layer/wrapper \n",
    "    wrapper.forward(x)\n",
    "        mu = model(x)\n",
    "        std = softmax(x)\n",
    "        return mu, std\n",
    "        \n",
    "- aletoric uncertainty loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
