{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from fastai.optimizer import Adam, RMSProp\n",
    "from fastai.tabular.model import TabularModel\n",
    "from torch import nn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from data.synthetic_wind_data import get_synthetic_wind_data\n",
    "# from utils_plots import data_to_plot, plot_hists, wind_vs_power\n",
    "from fastai.callback.all import *\n",
    "\n",
    "# from gan import GANLearner\n",
    "from fastai.vision.gan import *\n",
    "\n",
    "rng_seed = 42\n",
    "torch.manual_seed(rng_seed)\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "\n",
    "from fastai.tabular.all import TabularDataLoaders\n",
    "from fastai.tabular.all import TabularProc, Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizePerTask(TabularProc):\n",
    "    \"Normalize per TaskId\"\n",
    "    order = 1\n",
    "\n",
    "    def __init__(self, cols_to_ignore=[]):\n",
    "        self.cols_to_ignore = cols_to_ignore\n",
    "\n",
    "    def setups(self, to: Tabular):\n",
    "        self.rel_cols = [c for c in to.cont_names if c not in self.cols_to_ignore]\n",
    "        self.means = getattr(to, \"train\", to)[self.rel_cols].mean()\n",
    "        self.stds = getattr(to, \"train\", to)[self.rel_cols].std(ddof=0) + 1e-7\n",
    "\n",
    "    def encodes(self, to):\n",
    "        to.loc[:, self.rel_cols] = (to.loc[:, self.rel_cols] - self.means) / self.stds\n",
    "\n",
    "    def decodes(self, to):\n",
    "        to.loc[:, self.rel_cols] = to.loc[:, self.rel_cols] * self.stds + self.means\n",
    "\n",
    "        \n",
    "def get_synthetic_wind_data(data_path, file_name, config):\n",
    "\n",
    "    df = pd.concat([pd.read_hdf(data_path + f, key=\"powerdata\") for f in file_name], axis=0)\n",
    "    drop_list = [\"TestFlag\"]\n",
    "    cat_names = []\n",
    "    cont_names = [column for column in df.columns if column not in drop_list]\n",
    "    # cont_names = [\n",
    "    #     \"WindSpeed58m\",\n",
    "    #     \"WindSpeed60m\",\n",
    "    #     \"PowerGeneration\",\n",
    "    # ]\n",
    "    cont_names = [\n",
    "        \"T_HAG_2_M\",\n",
    "        \"RELHUM_HAG_2_M\",\n",
    "        \"PS_SFC_0_M\",\n",
    "        \"ASWDIFDS_SFC_0_M\",\n",
    "        \"ASWDIRS_SFC_0_M\",\n",
    "        \"WindSpeed58m\",\n",
    "        \"WindSpeed60m\",\n",
    "#         \"PowerGeneration\",\n",
    "    ]\n",
    "    y_names = [\"PowerGeneration\"]\n",
    "    # y_names = []\n",
    "\n",
    "    all_names = cont_names + y_names\n",
    "\n",
    "    dls = TabularDataLoaders.from_df(\n",
    "        df=df,\n",
    "        cat_names=cat_names,\n",
    "        cont_names=all_names,\n",
    "        y_names=all_names,\n",
    "        # procs=[Normalize],\n",
    "        bs=config[\"batch_size\"],\n",
    "        procs=[NormalizePerTask(cols_to_ignore=[\"PowerGeneration\"])],\n",
    "#         procs=[NormalizePerTask()],\n",
    "    )\n",
    "\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/scribbler/data/DAF_ICON_Synthetic_Wind_Power_processed/\"\n",
    "file_name = [\"00011.h5\",\"01303.h5\",\"02559.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00011.h5  01303.h5  02559.h5  03651.h5  05347.h5  06344.h5\r\n",
      "00090.h5  01346.h5  02564.h5  03668.h5  05349.h5  07341.h5\r\n",
      "00161.h5  01357.h5  02573.h5  03730.h5  05371.h5  07351.h5\r\n",
      "00164.h5  01358.h5  02597.h5  03761.h5  05397.h5  07367.h5\r\n",
      "00183.h5  01379.h5  02601.h5  03811.h5  05404.h5  07368.h5\r\n",
      "00197.h5  01420.h5  02638.h5  03821.h5  05412.h5  07369.h5\r\n",
      "00198.h5  01443.h5  02667.h5  03897.h5  05426.h5  07370.h5\r\n",
      "00232.h5  01468.h5  02712.h5  03925.h5  05440.h5  07374.h5\r\n",
      "00282.h5  01490.h5  02794.h5  03946.h5  05480.h5  07389.h5\r\n",
      "00298.h5  01503.h5  02812.h5  03987.h5  05490.h5  07391.h5\r\n",
      "00303.h5  01544.h5  02856.h5  04024.h5  05516.h5  07392.h5\r\n",
      "00342.h5  01550.h5  02878.h5  04032.h5  05538.h5  07393.h5\r\n",
      "00427.h5  01580.h5  02897.h5  04036.h5  05546.h5  07394.h5\r\n",
      "00430.h5  01587.h5  02907.h5  04039.h5  05629.h5  07395.h5\r\n",
      "00433.h5  01605.h5  02925.h5  04094.h5  05705.h5  07396.h5\r\n",
      "00460.h5  01612.h5  02928.h5  04104.h5  05779.h5  07403.h5\r\n",
      "00591.h5  01639.h5  02932.h5  04177.h5  05792.h5  07410.h5\r\n",
      "00596.h5  01684.h5  02961.h5  04271.h5  05797.h5  07412.h5\r\n",
      "00603.h5  01691.h5  02985.h5  04279.h5  05800.h5  07416.h5\r\n",
      "00619.h5  01694.h5  03015.h5  04280.h5  05825.h5  10510.h5\r\n",
      "00642.h5  01757.h5  03023.h5  04336.h5  05839.h5  13674.h5\r\n",
      "00656.h5  01759.h5  03028.h5  04371.h5  05851.h5  13676.h5\r\n",
      "00662.h5  01766.h5  03032.h5  04393.h5  05856.h5  13693.h5\r\n",
      "00691.h5  01803.h5  03086.h5  04464.h5  05871.h5  13701.h5\r\n",
      "00701.h5  01832.h5  03093.h5  04466.h5  05877.h5  13711.h5\r\n",
      "00704.h5  01869.h5  03098.h5  04501.h5  05906.h5  13901.h5\r\n",
      "00722.h5  01886.h5  03126.h5  04625.h5  05930.h5  13932.h5\r\n",
      "00788.h5  01963.h5  03158.h5  04642.h5  06091.h5  13952.h5\r\n",
      "00840.h5  01975.h5  03166.h5  04745.h5  06096.h5  13965.h5\r\n",
      "00853.h5  01993.h5  03167.h5  04880.h5  06097.h5  15000.h5\r\n",
      "00856.h5  02014.h5  03196.h5  04887.h5  06099.h5  15044.h5\r\n",
      "00867.h5  02023.h5  03231.h5  04911.h5  06101.h5  15120.h5\r\n",
      "00880.h5  02044.h5  03244.h5  04919.h5  06102.h5  15122.h5\r\n",
      "00891.h5  02100.h5  03268.h5  04928.h5  06103.h5  15189.h5\r\n",
      "00953.h5  02171.h5  03287.h5  04931.h5  06106.h5  15190.h5\r\n",
      "00963.h5  02252.h5  03321.h5  04947.h5  06107.h5  15200.h5\r\n",
      "01001.h5  02261.h5  03362.h5  05029.h5  06108.h5  15207.h5\r\n",
      "01011.h5  02290.h5  03366.h5  05078.h5  06159.h5  15214.h5\r\n",
      "01013.h5  02349.h5  03376.h5  05100.h5  06163.h5  15444.h5\r\n",
      "01048.h5  02377.h5  03379.h5  05109.h5  06196.h5  15520.h5\r\n",
      "01078.h5  02429.h5  03402.h5  05142.h5  06197.h5  15547.h5\r\n",
      "01200.h5  02483.h5  03513.h5  05158.h5  06211.h5  \u001b[0m\u001b[01;35mscatter_sample.png\u001b[0m\r\n",
      "01262.h5  02485.h5  03534.h5  05319.h5  06253.h5  \u001b[01;35mtimesersies_sample.png\u001b[0m\r\n",
      "01270.h5  02497.h5  03631.h5  05327.h5  06314.h5\r\n"
     ]
    }
   ],
   "source": [
    "ls {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_samples\": 100000,\n",
    "    # \"n_features\": 24,\n",
    "#     \"n_targets\": 1,\n",
    "    \"batch_size\": 1024,\n",
    "    \"n_noise_samples\": 100,\n",
    "    \"lr\": 1 * 1e-5,\n",
    "    \"epochs\": 100,\n",
    "    \"structure\": [2 ** n for n in range(11, 5, -1)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_synthetic_wind_data(data_path, file_name, config)\n",
    "config[\"n_features\"] = len(dls.cont_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat,x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x==y).sum()==x.shape[0]*x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.core import Callback, TrainEvalCallback\n",
    "from fastai.callback.progress import CSVLogger\n",
    "from fastai.learner import Learner, Metric\n",
    "from fastcore.basics import class2attr\n",
    "from fastcore.foundation import L\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from fastrenewables.tabular.model import MultiLayerPerceptron\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Sigmoid\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMode(Enum):\n",
    "    DISC_REAL=0\n",
    "    DISC_FAKE=1\n",
    "    GEN=2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TrainMode.DISC_REAL: 0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainMode.DISC_REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularGANModule(nn.Module):\n",
    "    def __init__(self, generator, critic, noise_size=100):\n",
    "        super(TabularGANModule, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.critic = critic\n",
    "        self.noise_size = noise_size\n",
    "        self.gen_mode = True  # for forward-fct\n",
    "        self.train_gen = True  # for optimizer handling\n",
    "\n",
    "    def _input_noise(self, bs):\n",
    "        # generate random values, used as input for generator\n",
    "        with torch.no_grad():\n",
    "            return torch.randn(bs, self.noise_size).cuda()\n",
    "\n",
    "    def generate_samples(self, x_cat, x_cont, device=\"cpu\"):\n",
    "        bs = x_cont.shape[0]\n",
    "        noise = self._input_noise(bs).to(device)\n",
    "        gen_data = self.generator(x_cat, noise)\n",
    "        return gen_data\n",
    "\n",
    "    def _requires_grad(self, model, freeze):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = freeze\n",
    "            \n",
    "    def update_train_mode(self, train_mode):\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        # TODO: should not be in the model\n",
    "        if self.train_mode in (TrainMode.DISC_REAL, TrainMode.DISC_FAKE):\n",
    "            self._requires_grad(self.generator, False)\n",
    "            self._requires_grad(self.critic, True)\n",
    "        else:\n",
    "            self._requires_grad(self.generator, True)\n",
    "            self._requires_grad(self.critic, False)\n",
    "            \n",
    "        \n",
    "        if self.train_mode == TrainMode.DISC_REAL:\n",
    "            # take real data\n",
    "            crit_out = self.critic(x_cat, x_cont)\n",
    "        elif self.train_mode in (TrainMode.DISC_FAKE, TrainMode.GEN):\n",
    "            # take synthetic data\n",
    "            gen_data = self.generate_samples(x_cat, x_cont)\n",
    "            crit_out = self.critic(x_cat, gen_data)\n",
    "            \n",
    "#         print(\"crit_out\", crit_out.shape, crit_out[0,0])\n",
    "\n",
    "        return crit_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealLossMetric(Metric):\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.real_loss = learn.real_loss\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return f\"{self.real_loss:.6f}\"\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return class2attr(self, \"Metric\")\n",
    "\n",
    "\n",
    "class FakeLossMetric(Metric):\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.fake_loss = learn.fake_loss\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return f\"{self.fake_loss:.6f}\"\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return class2attr(self, \"Metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.BCELoss()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, model, criterion=nn.BCELoss()):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.real_label = 1.0\n",
    "        self.fake_label = 0.0\n",
    "        self.criterion = criterion\n",
    "\n",
    "    @property\n",
    "    def train_mode(self):\n",
    "        return self.model.train_mode\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        # change t w.r.t train_mode\n",
    "        loss = 0\n",
    "        device = \"cuda:0\"\n",
    "        b_size = y.shape[0]\n",
    "        a_size = y.shape[1]\n",
    "#         print(\"*******\")\n",
    "#         print(y.shape, y[0,0])\n",
    "#         print(\"*******\")\n",
    "#         print(t.shape, t[0,0])\n",
    "#         print(\"*******\")\n",
    "        \n",
    "        \n",
    "        if self.train_mode == TrainMode.DISC_REAL:\n",
    "            label = torch.full((b_size,a_size), self.real_label, dtype=torch.float, device=device)\n",
    "        elif self.train_mode == TrainMode.DISC_FAKE:\n",
    "            label = torch.full((b_size,a_size), self.fake_label, dtype=torch.float, device=device)\n",
    "        elif self.train_mode.GEN == TrainMode.GEN:\n",
    "            label = torch.full((b_size,a_size), self.real_label, dtype=torch.float, device=device)\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "        return self.criterion(y.reshape(-1), label.reshape(-1).detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.core import *\n",
    "\n",
    "class GANTrainer(Callback):\n",
    "    run_after = TrainEvalCallback\n",
    "\n",
    "    def __init__(self, n_gen=1, n_crit=1, clip=None):\n",
    "        super(GANTrainer, self).__init__()\n",
    "        self.n_gen = n_gen\n",
    "        self.n_crit = n_crit\n",
    "        self.clip = clip\n",
    "        \n",
    "\n",
    "    def after_create(self):\n",
    "        self.learn.gen_opt = self.opt_func(self.model.generator.parameters(), lr=self.lr)\n",
    "        self.learn.crit_opt = self.opt_func(self.model.critic.parameters(), lr=self.lr)\n",
    "#         self.c_gen = 0\n",
    "#         self.c_crit = 0\n",
    "        self._set_train_mode(TrainMode.DISC_REAL)\n",
    "        \n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.train_mode.GEN == TrainMode.GEN:\n",
    "            self.learn.opt = self.learn.gen_opt\n",
    "        else:\n",
    "            self.learn.opt = self.learn.crit_opt\n",
    "        # zero grad only after DISC_FAKE? is done in _do_one_batch anyways\n",
    "        self.learn.opt.zero_grad()\n",
    "\n",
    "    def before_step(self):\n",
    "        if not self.train_mode.GEN == TrainMode.GEN and self.clip is not None:\n",
    "            nn.utils.clip_grad_value_(self.learn.model.critic.parameters(), self.clip)\n",
    "            \n",
    "    def _set_train_mode(self, new_mode):\n",
    "        self.train_mode = new_mode\n",
    "        self.model.update_train_mode(self.train_mode)\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.training:\n",
    "            return\n",
    "\n",
    "        # TODO: IS THIS DIRECTLY ACCESSING THE MODELS PARAMS?\n",
    "        if self.train_mode == TrainMode.DISC_REAL:\n",
    "            self._set_train_mode(TrainMode.DISC_FAKE)\n",
    "        elif self.train_mode == TrainMode.DISC_FAKE:\n",
    "            self._set_train_mode(TrainMode.GEN)\n",
    "            self.learn.fake_loss = self.learn.loss.item()\n",
    "        elif self.train_mode.GEN == TrainMode.GEN:\n",
    "            self._set_train_mode(TrainMode.DISC_REAL)\n",
    "            self.learn.real_loss = self.learn.loss.item()\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.learn.opt = self.learn.gen_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLearner(Learner):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls,\n",
    "        generator,\n",
    "        critic,\n",
    "        criterion=nn.BCELoss(),\n",
    "        lr=1e-3,\n",
    "        noise_size=100,\n",
    "        cbs=[CSVLogger()],\n",
    "        metrics=[RealLossMetric, FakeLossMetric],\n",
    "        opt_func=None,\n",
    "        clip=0.01,\n",
    "        n_gen=1,\n",
    "        n_crit=5,\n",
    "    ):\n",
    "        gan_model = TabularGANModule(\n",
    "            generator=generator,\n",
    "            critic=critic,\n",
    "            noise_size=noise_size,\n",
    "        )\n",
    "\n",
    "        gan_loss = GANLoss(gan_model, criterion=criterion)\n",
    "        trainer = GANTrainer(n_gen, n_crit, clip)\n",
    "        cbs = L(cbs) + L(trainer)\n",
    "        metrics = L(metrics)\n",
    "        \n",
    "        super(GANLearner, self).__init__(\n",
    "            dls,\n",
    "            gan_model,\n",
    "            cbs=cbs,\n",
    "            lr=lr,\n",
    "            metrics=metrics,\n",
    "            opt_func=opt_func,\n",
    "            loss_func=gan_loss,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate for optimizers\n",
    "lr = 1e-2\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# learner = GANLearner(dls, generator=generator_model, \n",
    "#                      critic=critic_model, \n",
    "# #                      opt_func=partial(RMSProp, mom=0.5, sqr_mom=0.99),)\n",
    "#                     opt_func=partial(Adam, lr=lr),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.fit(50, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.torch_core import to_np\n",
    "\n",
    "# def data_to_plot(dls, learner):\n",
    "\n",
    "#     # dls.cuda()\n",
    "#     learner.model.cuda()\n",
    "#     real_cat_data = dls.train_ds.cats\n",
    "#     real_cont_data = dls.train_ds.conts\n",
    "#     fake_data = to_np(learner.model.generate_samples(real_cat_data, real_cont_data))\n",
    "#     # fake_data = (\n",
    "#     #         learner.model.generate_samples(real_cat_data, real_cont_data).detach().cpu().numpy()\n",
    "#     # )\n",
    "#     real_data = real_cont_data.to_numpy()\n",
    "#     return real_data, fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real, fake = data_to_plot(dls, learner)\n",
    "# for i in range(real.shape[1]):\n",
    "    \n",
    "#     plt.hist(fake[:,i], label=\"fake\")\n",
    "#     plt.hist(real[:,i], label=\"real\")\n",
    "    \n",
    "#     plt.title(dls.train_ds.cont_names[i])\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 100000,\n",
       " 'batch_size': 1024,\n",
       " 'n_noise_samples': 100,\n",
       " 'lr': 1e-05,\n",
       " 'epochs': 100,\n",
       " 'structure': [2048, 1024, 512, 256, 128, 64],\n",
       " 'n_features': 8}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan_model = TabularGANModule(\n",
    "#             generator=generator_model,\n",
    "#             critic=critic_model,\n",
    "#             noise_size=config[\"n_noise_samples\"],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat,x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic = MultiLayerPerceptron(\n",
    "#             ann_structure=[config[\"n_features\"], 100, 50, 1],\n",
    "#             act_cls=nn.LeakyReLU(),\n",
    "#             final_activation=Sigmoid,\n",
    "#             bn_cont=False\n",
    "#         )\n",
    "\n",
    "# generator = MultiLayerPerceptron(\n",
    "#     [config[\"n_noise_samples\"], 400, 200, 100, 50, config[\"n_features\"]],\n",
    "#     act_cls=nn.LeakyReLU(),\n",
    "# #     final_activation=Sigmoid,\n",
    "#     bn_cont=False\n",
    "# )\n",
    "\n",
    "# import torch.optim as optim# Initialize BCELoss function\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# # Create batch of latent vectors that we will use to visualize\n",
    "# #  the progression of the generator\n",
    "# # fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# # Establish convention for real and fake labels during training\n",
    "# real_label = 1.\n",
    "# fake_label = 0.\n",
    "\n",
    "# # Setup Adam optimizers for both G and D\n",
    "# optimizerD = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "# optimizerG = optim.Adam(critic.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "# critic.zero_grad()\n",
    "# batch_size = x.shape[0]\n",
    "# label = torch.full((batch_size,), real_label, dtype=torch.float)\n",
    "# output = critic(cat, y).view(-1)\n",
    "\n",
    "# errD_real = criterion(output, label)\n",
    "# errD_real.backward()\n",
    "\n",
    "# D_x = output.mean().item()\n",
    "\n",
    "# # fake = gan_model.generate_samples(cat,x, \"cpu\")\n",
    "# bs = x.shape[0]\n",
    "# noise = gan_model._input_noise(bs).to(\"cpu\")\n",
    "# fake = generator(cat, noise)\n",
    "# # # noise\n",
    "# label.fill_(fake_label)\n",
    "# output = critic(cat, fake.detach()).view(-1)\n",
    "# errD_fake = criterion(output, label)\n",
    "# errD_fake.backward()\n",
    "# D_G_z1 = output.mean().item()\n",
    "# errD = errD_real + errD_fake\n",
    "\n",
    "# optimizerD.step()\n",
    "# critic.zero_grad()\n",
    "\n",
    "# generator.zero_grad()\n",
    "# label.fill_(real_label)\n",
    "# fake = generator(cat, noise)\n",
    "# ouput = critic(cat, fake).view(-1)\n",
    "# errG = criterion(output,label)\n",
    "# errG.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GANLearner(Learner):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         dls,\n",
    "#         generator,\n",
    "#         critic,\n",
    "#         criterion=nn.BCELoss(),\n",
    "#         lr=1e-3,\n",
    "#         noise_size=100,\n",
    "#         cbs=[CSVLogger()],\n",
    "#         metrics=[RealLossMetric, FakeLossMetric],\n",
    "#         opt_func=None,\n",
    "#         clip=0.01,\n",
    "#         n_gen=1,\n",
    "#         n_crit=5,\n",
    "#     ):\n",
    "#         gan_model = TabularGANModule(\n",
    "#             generator=generator,\n",
    "#             critic=critic,\n",
    "#             noise_size=noise_size,\n",
    "#         )\n",
    "\n",
    "#         gan_loss = GANLoss(gan_model, criterion=criterion)\n",
    "# #         trainer = GANTrainer(n_gen, n_crit, clip)\n",
    "# #         cbs = L(cbs) + L(trainer)\n",
    "# #         metrics = L(metrics)\n",
    "        \n",
    "#         # Setup Adam optimizers for both G and D\n",
    "#         self.optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "#         self.optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "        \n",
    "#         super(GANLearner, self).__init__(\n",
    "#             dls,\n",
    "#             gan_model,\n",
    "#             cbs=cbs,\n",
    "#             lr=lr,\n",
    "#             metrics=metrics,\n",
    "#             opt_func=opt_func,\n",
    "#             loss_func=gan_loss,\n",
    "#         )\n",
    "        \n",
    "#     def _do_one_batch(self):\n",
    "#         self.pred = self.model(*self.xb)\n",
    "#         self('after_pred')\n",
    "#         if len(self.yb):\n",
    "#             self.loss_grad = self.loss_func(self.pred, *self.yb)\n",
    "#             self.loss = self.loss_grad.clone()\n",
    "#         self('after_loss')\n",
    "#         if not self.training or not len(self.yb): return\n",
    "#         self('before_backward')\n",
    "#         self.loss_grad.backward()\n",
    "#         self._with_events(self.opt.step, 'step', CancelStepException)\n",
    "#         self.opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
