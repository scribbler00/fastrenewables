{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from fastai.optimizer import Adam, RMSProp\n",
    "from fastai.tabular.model import TabularModel\n",
    "from torch import nn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from data.synthetic_wind_data import get_synthetic_wind_data\n",
    "# from utils_plots import data_to_plot, plot_hists, wind_vs_power\n",
    "from fastai.callback.all import *\n",
    "\n",
    "# from gan import GANLearner\n",
    "from fastai.vision.gan import *\n",
    "\n",
    "rng_seed = 42\n",
    "torch.manual_seed(rng_seed)\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "\n",
    "from fastai.tabular.all import TabularDataLoaders\n",
    "from fastai.tabular.all import TabularProc, Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizePerTask(TabularProc):\n",
    "    \"Normalize per TaskId\"\n",
    "    order = 1\n",
    "\n",
    "    def __init__(self, cols_to_ignore=[]):\n",
    "        self.cols_to_ignore = cols_to_ignore\n",
    "\n",
    "    def setups(self, to: Tabular):\n",
    "        self.rel_cols = [c for c in to.cont_names if c not in self.cols_to_ignore]\n",
    "        self.means = getattr(to, \"train\", to)[self.rel_cols].mean()\n",
    "        self.stds = getattr(to, \"train\", to)[self.rel_cols].std(ddof=0) + 1e-7\n",
    "\n",
    "    def encodes(self, to):\n",
    "        to.loc[:, self.rel_cols] = (to.loc[:, self.rel_cols] - self.means) / self.stds\n",
    "\n",
    "    def decodes(self, to):\n",
    "        to.loc[:, self.rel_cols] = to.loc[:, self.rel_cols] * self.stds + self.means\n",
    "\n",
    "        \n",
    "def get_synthetic_wind_data(data_path, file_name, config):\n",
    "\n",
    "    df = pd.concat([pd.read_hdf(data_path + f, key=\"powerdata\") for f in file_name], axis=0)\n",
    "    drop_list = [\"TestFlag\"]\n",
    "    cat_names = []\n",
    "    cont_names = [column for column in df.columns if column not in drop_list]\n",
    "    # cont_names = [\n",
    "    #     \"WindSpeed58m\",\n",
    "    #     \"WindSpeed60m\",\n",
    "    #     \"PowerGeneration\",\n",
    "    # ]\n",
    "    cont_names = [\n",
    "        \"T_HAG_2_M\",\n",
    "        \"RELHUM_HAG_2_M\",\n",
    "        \"PS_SFC_0_M\",\n",
    "        \"ASWDIFDS_SFC_0_M\",\n",
    "        \"ASWDIRS_SFC_0_M\",\n",
    "        \"WindSpeed58m\",\n",
    "        \"WindSpeed60m\",\n",
    "#         \"PowerGeneration\",\n",
    "    ]\n",
    "    y_names = [\"PowerGeneration\"]\n",
    "    # y_names = []\n",
    "\n",
    "    all_names = cont_names + y_names\n",
    "\n",
    "    dls = TabularDataLoaders.from_df(\n",
    "        df=df,\n",
    "        cat_names=cat_names,\n",
    "        cont_names=all_names,\n",
    "        y_names=all_names,\n",
    "        # procs=[Normalize],\n",
    "        bs=config[\"batch_size\"],\n",
    "        procs=[NormalizePerTask(cols_to_ignore=[\"PowerGeneration\"])],\n",
    "#         procs=[NormalizePerTask()],\n",
    "    )\n",
    "\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"/home/scribbler/data/DAF_ICON_Synthetic_Wind_Power_processed/\"\n",
    "# file_name = [\"00011.h5\",\"01303.h5\",\"02559.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_samples\": 100000,\n",
    "    # \"n_features\": 24,\n",
    "#     \"n_targets\": 1,\n",
    "    \"batch_size\": 1024,\n",
    "    \"n_noise_samples\": 100,\n",
    "    \"lr\": 1 * 1e-5,\n",
    "    \"epochs\": 100,\n",
    "    \"structure\": [2 ** n for n in range(11, 5, -1)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls = get_synthetic_wind_data(data_path, file_name, config)\n",
    "# config[\"n_features\"] = len(dls.cont_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat,x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x==y).sum()==x.shape[0]*x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.core import Callback, TrainEvalCallback\n",
    "from fastai.callback.progress import CSVLogger\n",
    "from fastai.learner import Learner, Metric\n",
    "from fastcore.basics import class2attr\n",
    "from fastcore.foundation import L\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from fastrenewables.tabular.model import MultiLayerPerceptron\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Sigmoid\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMode(Enum):\n",
    "    DISC_REAL=0\n",
    "    DISC_FAKE=1\n",
    "    GEN=2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TrainMode.DISC_REAL: 0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainMode.DISC_REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularGANModule(nn.Module):\n",
    "    def __init__(self, generator, critic, noise_size=100):\n",
    "        super(TabularGANModule, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.critic = critic\n",
    "        self.noise_size = noise_size\n",
    "        self.gen_mode = True  # for forward-fct\n",
    "        self.train_gen = True  # for optimizer handling\n",
    "\n",
    "    def _input_noise(self, bs):\n",
    "        # generate random values, used as input for generator\n",
    "        with torch.no_grad():\n",
    "            return torch.randn(bs, self.noise_size).cuda()\n",
    "\n",
    "    def generate_samples(self, x_cat, x_cont, device=\"cpu\"):\n",
    "        bs = x_cont.shape[0]\n",
    "        noise = self._input_noise(bs).to(device)\n",
    "        gen_data = self.generator(x_cat, noise)\n",
    "        return gen_data\n",
    "\n",
    "    def _requires_grad(self, model, freeze):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = freeze\n",
    "            \n",
    "    def update_train_mode(self, train_mode):\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        # TODO: should not be in the model\n",
    "        if self.train_mode in (TrainMode.DISC_REAL, TrainMode.DISC_FAKE):\n",
    "            self._requires_grad(self.generator, False)\n",
    "            self._requires_grad(self.critic, True)\n",
    "        else:\n",
    "            self._requires_grad(self.generator, True)\n",
    "            self._requires_grad(self.critic, False)\n",
    "            \n",
    "        \n",
    "        if self.train_mode == TrainMode.DISC_REAL:\n",
    "            # take real data\n",
    "            crit_out = self.critic(x_cat, x_cont)\n",
    "        elif self.train_mode in (TrainMode.DISC_FAKE, TrainMode.GEN):\n",
    "            # take synthetic data\n",
    "            gen_data = self.generate_samples(x_cat, x_cont)\n",
    "            crit_out = self.critic(x_cat, gen_data)\n",
    "            \n",
    "#         print(\"crit_out\", crit_out.shape, crit_out[0,0])\n",
    "\n",
    "        return crit_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealLossMetric(Metric):\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.real_loss = learn.real_loss\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return f\"{self.real_loss:.6f}\"\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return class2attr(self, \"Metric\")\n",
    "\n",
    "\n",
    "class FakeLossMetric(Metric):\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.fake_loss = learn.fake_loss\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return f\"{self.fake_loss:.6f}\"\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return class2attr(self, \"Metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, model, criterion=nn.BCELoss()):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.real_label = 1.0\n",
    "        self.fake_label = 0.0\n",
    "        self.criterion = criterion\n",
    "\n",
    "    @property\n",
    "    def train_mode(self):\n",
    "        return self.model.train_mode\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        # change t w.r.t train_mode\n",
    "        loss = 0\n",
    "        device = \"cuda:0\"\n",
    "        b_size = y.shape[0]\n",
    "        a_size = y.shape[1]\n",
    "#         print(\"*******\")\n",
    "#         print(y.shape, y[0,0])\n",
    "#         print(\"*******\")\n",
    "#         print(t.shape, t[0,0])\n",
    "#         print(\"*******\")\n",
    "        \n",
    "        \n",
    "        if self.train_mode == TrainMode.DISC_REAL:\n",
    "            label = torch.full((b_size,a_size), self.real_label, dtype=torch.float, device=device)\n",
    "        elif self.train_mode == TrainMode.DISC_FAKE:\n",
    "            label = torch.full((b_size,a_size), self.fake_label, dtype=torch.float, device=device)\n",
    "        elif self.train_mode.GEN == TrainMode.GEN:\n",
    "            label = torch.full((b_size,a_size), self.real_label, dtype=torch.float, device=device)\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "        return self.criterion(y.reshape(-1), label.reshape(-1).detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.core import *\n",
    "\n",
    "class GANTrainer(Callback):\n",
    "    run_after = TrainEvalCallback\n",
    "\n",
    "    def __init__(self, n_gen=1, n_crit=1, clip=None):\n",
    "        super(GANTrainer, self).__init__()\n",
    "        self.n_gen = n_gen\n",
    "        self.n_crit = n_crit\n",
    "        self.clip = clip\n",
    "        \n",
    "\n",
    "    def after_create(self):\n",
    "        self.learn.gen_opt = self.opt_func(self.model.generator.parameters(), lr=self.lr)\n",
    "        self.learn.crit_opt = self.opt_func(self.model.critic.parameters(), lr=self.lr)\n",
    "#         self.c_gen = 0\n",
    "#         self.c_crit = 0\n",
    "        self._set_train_mode(TrainMode.DISC_REAL)\n",
    "        \n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.train_mode.GEN == TrainMode.GEN:\n",
    "            self.learn.opt = self.learn.gen_opt\n",
    "        else:\n",
    "            self.learn.opt = self.learn.crit_opt\n",
    "        # zero grad only after DISC_FAKE? is done in _do_one_batch anyways\n",
    "        self.learn.opt.zero_grad()\n",
    "\n",
    "    def before_step(self):\n",
    "        if not self.train_mode.GEN == TrainMode.GEN and self.clip is not None:\n",
    "            nn.utils.clip_grad_value_(self.learn.model.critic.parameters(), self.clip)\n",
    "            \n",
    "    def _set_train_mode(self, new_mode):\n",
    "        self.train_mode = new_mode\n",
    "        self.model.update_train_mode(self.train_mode)\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.training:\n",
    "            return\n",
    "\n",
    "        # TODO: IS THIS DIRECTLY ACCESSING THE MODELS PARAMS?\n",
    "        if self.train_mode == TrainMode.DISC_REAL:\n",
    "            self._set_train_mode(TrainMode.DISC_FAKE)\n",
    "        elif self.train_mode == TrainMode.DISC_FAKE:\n",
    "            self._set_train_mode(TrainMode.GEN)\n",
    "            self.learn.fake_loss = self.learn.loss.item()\n",
    "        elif self.train_mode.GEN == TrainMode.GEN:\n",
    "            self._set_train_mode(TrainMode.DISC_REAL)\n",
    "            self.learn.real_loss = self.learn.loss.item()\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.learn.opt = self.learn.gen_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLearner(Learner):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dls,\n",
    "        generator,\n",
    "        critic,\n",
    "        criterion=nn.BCELoss(),\n",
    "        lr=1e-3,\n",
    "        noise_size=100,\n",
    "        cbs=[CSVLogger()],\n",
    "        metrics=[RealLossMetric, FakeLossMetric],\n",
    "        opt_func=None,\n",
    "        clip=0.01,\n",
    "        n_gen=1,\n",
    "        n_crit=5,\n",
    "    ):\n",
    "        gan_model = TabularGANModule(\n",
    "            generator=generator,\n",
    "            critic=critic,\n",
    "            noise_size=noise_size,\n",
    "        )\n",
    "\n",
    "        gan_loss = GANLoss(gan_model, criterion=criterion)\n",
    "        trainer = GANTrainer(n_gen, n_crit, clip)\n",
    "        cbs = L(cbs) + L(trainer)\n",
    "        metrics = L(metrics)\n",
    "        \n",
    "        super(GANLearner, self).__init__(\n",
    "            dls,\n",
    "            gan_model,\n",
    "            cbs=cbs,\n",
    "            lr=lr,\n",
    "            metrics=metrics,\n",
    "            opt_func=opt_func,\n",
    "            loss_func=gan_loss,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate for optimizers\n",
    "lr = 1e-2\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# learner = GANLearner(dls, generator=generator_model, \n",
    "#                      critic=critic_model, \n",
    "# #                      opt_func=partial(RMSProp, mom=0.5, sqr_mom=0.99),)\n",
    "#                     opt_func=partial(Adam, lr=lr),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.fit(50, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.torch_core import to_np\n",
    "\n",
    "# def data_to_plot(dls, learner):\n",
    "\n",
    "#     # dls.cuda()\n",
    "#     learner.model.cuda()\n",
    "#     real_cat_data = dls.train_ds.cats\n",
    "#     real_cont_data = dls.train_ds.conts\n",
    "#     fake_data = to_np(learner.model.generate_samples(real_cat_data, real_cont_data))\n",
    "#     # fake_data = (\n",
    "#     #         learner.model.generate_samples(real_cat_data, real_cont_data).detach().cpu().numpy()\n",
    "#     # )\n",
    "#     real_data = real_cont_data.to_numpy()\n",
    "#     return real_data, fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real, fake = data_to_plot(dls, learner)\n",
    "# for i in range(real.shape[1]):\n",
    "    \n",
    "#     plt.hist(fake[:,i], label=\"fake\")\n",
    "#     plt.hist(real[:,i], label=\"real\")\n",
    "    \n",
    "#     plt.title(dls.train_ds.cont_names[i])\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 100000,\n",
       " 'batch_size': 1024,\n",
       " 'n_noise_samples': 100,\n",
       " 'lr': 1e-05,\n",
       " 'epochs': 100,\n",
       " 'structure': [2048, 1024, 512, 256, 128, 64],\n",
       " 'n_features': 8}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan_model = TabularGANModule(\n",
    "#             generator=generator_model,\n",
    "#             critic=critic_model,\n",
    "#             noise_size=config[\"n_noise_samples\"],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat,x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic = MultiLayerPerceptron(\n",
    "#             ann_structure=[config[\"n_features\"], 100, 50, 1],\n",
    "#             act_cls=nn.LeakyReLU(),\n",
    "#             final_activation=Sigmoid,\n",
    "#             bn_cont=False\n",
    "#         )\n",
    "\n",
    "# generator = MultiLayerPerceptron(\n",
    "#     [config[\"n_noise_samples\"], 400, 200, 100, 50, config[\"n_features\"]],\n",
    "#     act_cls=nn.LeakyReLU(),\n",
    "# #     final_activation=Sigmoid,\n",
    "#     bn_cont=False\n",
    "# )\n",
    "\n",
    "# import torch.optim as optim# Initialize BCELoss function\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# # Create batch of latent vectors that we will use to visualize\n",
    "# #  the progression of the generator\n",
    "# # fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# # Establish convention for real and fake labels during training\n",
    "# real_label = 1.\n",
    "# fake_label = 0.\n",
    "\n",
    "# # Setup Adam optimizers for both G and D\n",
    "# optimizerD = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "# optimizerG = optim.Adam(critic.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "# critic.zero_grad()\n",
    "# batch_size = x.shape[0]\n",
    "# label = torch.full((batch_size,), real_label, dtype=torch.float)\n",
    "# output = critic(cat, y).view(-1)\n",
    "\n",
    "# errD_real = criterion(output, label)\n",
    "# errD_real.backward()\n",
    "\n",
    "# D_x = output.mean().item()\n",
    "\n",
    "# # fake = gan_model.generate_samples(cat,x, \"cpu\")\n",
    "# bs = x.shape[0]\n",
    "# noise = gan_model._input_noise(bs).to(\"cpu\")\n",
    "# fake = generator(cat, noise)\n",
    "# # # noise\n",
    "# label.fill_(fake_label)\n",
    "# output = critic(cat, fake.detach()).view(-1)\n",
    "# errD_fake = criterion(output, label)\n",
    "# errD_fake.backward()\n",
    "# D_G_z1 = output.mean().item()\n",
    "# errD = errD_real + errD_fake\n",
    "\n",
    "# optimizerD.step()\n",
    "# critic.zero_grad()\n",
    "\n",
    "# generator.zero_grad()\n",
    "# label.fill_(real_label)\n",
    "# fake = generator(cat, noise)\n",
    "# ouput = critic(cat, fake).view(-1)\n",
    "# errG = criterion(output,label)\n",
    "# errG.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GANLearner(Learner):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         dls,\n",
    "#         generator,\n",
    "#         critic,\n",
    "#         criterion=nn.BCELoss(),\n",
    "#         lr=1e-3,\n",
    "#         noise_size=100,\n",
    "#         cbs=[CSVLogger()],\n",
    "#         metrics=[RealLossMetric, FakeLossMetric],\n",
    "#         opt_func=None,\n",
    "#         clip=0.01,\n",
    "#         n_gen=1,\n",
    "#         n_crit=5,\n",
    "#     ):\n",
    "#         gan_model = TabularGANModule(\n",
    "#             generator=generator,\n",
    "#             critic=critic,\n",
    "#             noise_size=noise_size,\n",
    "#         )\n",
    "\n",
    "#         gan_loss = GANLoss(gan_model, criterion=criterion)\n",
    "# #         trainer = GANTrainer(n_gen, n_crit, clip)\n",
    "# #         cbs = L(cbs) + L(trainer)\n",
    "# #         metrics = L(metrics)\n",
    "        \n",
    "#         # Setup Adam optimizers for both G and D\n",
    "#         self.optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "#         self.optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "        \n",
    "#         super(GANLearner, self).__init__(\n",
    "#             dls,\n",
    "#             gan_model,\n",
    "#             cbs=cbs,\n",
    "#             lr=lr,\n",
    "#             metrics=metrics,\n",
    "#             opt_func=opt_func,\n",
    "#             loss_func=gan_loss,\n",
    "#         )\n",
    "        \n",
    "#     def _do_one_batch(self):\n",
    "#         self.pred = self.model(*self.xb)\n",
    "#         self('after_pred')\n",
    "#         if len(self.yb):\n",
    "#             self.loss_grad = self.loss_func(self.pred, *self.yb)\n",
    "#             self.loss = self.loss_grad.clone()\n",
    "#         self('after_loss')\n",
    "#         if not self.training or not len(self.yb): return\n",
    "#         self('before_backward')\n",
    "#         self.loss_grad.backward()\n",
    "#         self._with_events(self.opt.step, 'step', CancelStepException)\n",
    "#         self.opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
