{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils_pytorch\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import nn\n",
    "from torch.nn import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unfreeze_n_final_layer(model, n, include_embedding=False):\n",
    "    \"\"\"\n",
    "    Remove all but the last 'n' layers from the gradient computation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch.nn.ModuleList/pytorch.nn.Sequential/any\n",
    "        the model whose layers are to be excluded from the gradient computation.\n",
    "    n : interger\n",
    "        the number of layers not to be included for gradient computation.\n",
    "    include_embedding : bool\n",
    "        if True, include all embedding layers to the gradient computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Currently embedding layers are either included or excluded through 'include_embedding'.\n",
    "    \"\"\"\n",
    "    # freeze all parameters by excluding them from gradient computation\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Reinclude the parameters of the last n layers to gradient computation\n",
    "    layers = list(model.children())\n",
    "\n",
    "    new_layers = []\n",
    "    for l in layers:\n",
    "        if type(l) is nn.ModuleList:\n",
    "            unfreeze_n_final_layer(l, n, include_embedding=include_embedding)\n",
    "        elif type(l) is Embedding and include_embedding:\n",
    "            for param in l.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif type(l) is Embedding and not include_embedding:\n",
    "            for param in l.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif hasattr(l, \"weight\") or isinstance(l, nn.Sequential):\n",
    "            new_layers.append(l)\n",
    "\n",
    "    if len(new_layers) > 0:\n",
    "        layers = new_layers\n",
    "\n",
    "        if n > len(layers) or n == -1:\n",
    "            n = len(layers)  # relearn the whole network\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            for param in layers[-i].parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def freeze(layer):\n",
    "    \"\"\"\n",
    "    Exclude a layer from the gradient computation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer : torch.nn\n",
    "        the layer which is to be excluded from the gradient computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unfreeze(layer):\n",
    "    \"\"\"\n",
    "    Include a layer to the gradient computation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer : torch.nn\n",
    "        the layer which is to be included to the gradient computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_requires_grad(\n",
    "    model, include_embedding=True, type_name=\"\", rec_level=0, tabs=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Print which layers of the model are included in the gradient computation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch.nn.ModuleList/pytorch.nn.Sequential/any\n",
    "        the model that is to be analyzed.\n",
    "    include_embedding : bool\n",
    "        currently not used.\n",
    "    type_name : string\n",
    "        currently not used.\n",
    "    rec_level : integer\n",
    "        currently not used.\n",
    "    tabs : string\n",
    "        the amount of space before each print.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    layers = list(model.children())\n",
    "    new_rec_level = rec_level + 1\n",
    "\n",
    "    modules = model._modules\n",
    "    if isinstance(model, nn.ModuleList):\n",
    "        cur_type = \"ModuleList\"\n",
    "    elif isinstance(model, nn.Sequential):\n",
    "        cur_type = \"Sequential\"\n",
    "    else:\n",
    "        cur_type = \"\"\n",
    "    for k, v in modules.items():\n",
    "        if len(v._modules) > 0:\n",
    "            print(f\"{tabs}{cur_type} ({k}): (\")\n",
    "            new_tabs = tabs + \"  \"\n",
    "            print_requires_grad(v, tabs=new_tabs)\n",
    "            print(f\"{tabs})\")\n",
    "        else:\n",
    "            if hasattr(v, \"weight\"):\n",
    "                print(f\"{tabs}({v}) Requires grad: {v.weight.requires_grad}\")\n",
    "            else:\n",
    "                print(f\"{tabs}({v})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
