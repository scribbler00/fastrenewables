{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp timeseries.learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# timeseries.learner\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.tabular.data import *\n",
    "from fastai.tabular.core import *\n",
    "from fastai.tabular.model import *\n",
    "from fastai.basics import *\n",
    "from fastrenewables.tabular.core import *\n",
    "from fastrenewables.tabular.model import *\n",
    "from fastrenewables.timeseries.core import *\n",
    "from fastrenewables.timeseries.data import *\n",
    "from fastrenewables.timeseries.model import *\n",
    "from fastrenewables.losses import VILoss\n",
    "from fastrenewables.utils import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def _tensor_to_device(x, device):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.to(device)\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def _swap_axes_and_flatten(tensr):\n",
    "    if isinstance(tensr, torch.Tensor):\n",
    "        return  torch.swapaxes(tensr, 1, 2).reshape(-1, tensr.shape[1])\n",
    "    elif isinstance(tensr, np.ndarray):\n",
    "        return  np.swapaxes(tensr, 1, 2).reshape(-1, tensr.shape[1])\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert_to_tensor_ts(to, include_index=False, device=\"cpu\", flatten=False):\n",
    "    # to increase speed we direclty predict on all tensors   \n",
    "    if isinstance(to, (TimeseriesDataset, Timeseries)):\n",
    "        with torch.no_grad():\n",
    "            cats, conts, targets = _tensor_to_device(to.cats, device), _tensor_to_device(to.conts, device), _tensor_to_device(to.ys, device)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown type\")\n",
    "        \n",
    "    indexes = to.indexes\n",
    "    \n",
    "    if flatten:\n",
    "        indexes = _swap_axes_and_flatten(indexes).ravel()\n",
    "        conts = _swap_axes_and_flatten(conts)\n",
    "        cats = _swap_axes_and_flatten(cats)\n",
    "        targets = _swap_axes_and_flatten(targets)\n",
    "        \n",
    "    if include_index:\n",
    "        return indexes, cats, conts, targets\n",
    "    else:\n",
    "        return cats, conts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fast_prediction_ts(model, to, flatten, filter, device=\"cpu\"):\n",
    "    \n",
    "    cats, conts, ys = convert_to_tensor_ts(to, include_index=False, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(cats, conts)\n",
    "\n",
    "    preds, targets = to_np(preds), to_np(ys)\n",
    "\n",
    "    if flatten:\n",
    "        preds, targets = preds.reshape(-1), targets.reshape(-1)\n",
    "\n",
    "    if filter:\n",
    "        targets, preds = filter_preds(targets, preds)\n",
    "    \n",
    "        \n",
    "        \n",
    "    return preds, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RenewableTimeseriesLearner(Learner):\n",
    "    \"`Learner` for renewable timerseries data.\"\n",
    "    def predict(self, ds_idx=1, test_dl=None, filter=True, as_df=False, flatten=True):\n",
    "        device = next(self.model.parameters()).device\n",
    "        preds, targets = None, None\n",
    "        if test_dl is not None:\n",
    "            to = test_dl.train_ds\n",
    "        elif ds_idx == 0:\n",
    "            to = self.dls.train_ds\n",
    "        elif ds_idx == 1:\n",
    "            to = self.dls.valid_ds\n",
    "        \n",
    "        preds, targets = fast_prediction_ts(self.model, to, flatten=flatten, filter=filter)\n",
    "        \n",
    "        if as_df:\n",
    "            return pd.DataFrame({\"Prediction\": preds, \"Target\":targets}, index=to.indexes.reshape(-1))\n",
    "        else:\n",
    "            return preds, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"RenewableTimeseriesLearner\" class=\"doc_header\"><code>class</code> <code>RenewableTimeseriesLearner</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>RenewableTimeseriesLearner</code>(**`dls`**, **`model`**, **`loss_func`**=*`None`*, **`opt_func`**=*`Adam`*, **`lr`**=*`0.001`*, **`splitter`**=*`trainable_params`*, **`cbs`**=*`None`*, **`metrics`**=*`None`*, **`path`**=*`None`*, **`model_dir`**=*`'models'`*, **`wd`**=*`None`*, **`wd_bn_bias`**=*`False`*, **`train_bn`**=*`True`*, **`moms`**=*`(0.95, 0.85, 0.95)`*) :: `Learner`\n",
       "\n",
       "`Learner` for renewable timerseries data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(RenewableTimeseriesLearner, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(cat_names = [\"TaskID\", 'Month', 'Day', 'Hour'], cont_names=None, y_names=\"PowerGeneration\", y_block=RegressionBlock(), post_hooks=[]):\n",
    "    \n",
    "\n",
    "    if cont_names is None:\n",
    "        cont_names = ['T_HAG_2_M', 'RELHUM_HAG_2_M', 'PS_SFC_0_M', 'ASWDIFDS_SFC_0_M',\n",
    "           'ASWDIRS_SFC_0_M', 'WindSpeed58m']\n",
    "    \n",
    "    pd.options.mode.chained_assignment=None\n",
    "    kwargs = {\"post_hooks\": post_hooks}\n",
    "    dls = RenewableTimeSeriesDataLoaders.from_files(glob.glob(\"../data/*.h5\"), \n",
    "                                                y_names=y_names, \n",
    "                                                cat_names=cat_names, \n",
    "                                                cont_names=cont_names,\n",
    "                                                pre_procs=[FilterYear(year=2020), \n",
    "                                                             AddSeasonalFeatures(as_cont=False),\n",
    "                                                             FilterInconsistentSamplesPerDay], \n",
    "                                                procs=Categorify, \n",
    "                                                bs=12,\n",
    "                                                y_block=y_block,\n",
    "                                                post_hooks=post_hooks)\n",
    "    return dls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dls \u001b[38;5;241m=\u001b[39m \u001b[43mget_dls\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPowerGeneration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRegressionBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mget_dls\u001b[0;34m(cat_names, cont_names, y_names, y_block, post_hooks)\u001b[0m\n\u001b[1;32m      8\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mchained_assignment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_hooks\u001b[39m\u001b[38;5;124m\"\u001b[39m: post_hooks}\n\u001b[0;32m---> 10\u001b[0m dls \u001b[38;5;241m=\u001b[39m \u001b[43mRenewableTimeSeriesDataLoaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/*.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43my_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcat_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcont_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcont_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mpre_procs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mFilterYear\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2020\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43mAddSeasonalFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_cont\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43mFilterInconsistentSamplesPerDay\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCategorify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43my_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mpost_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_hooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dls\n",
      "File \u001b[0;32m~/anaconda/lib/python3.8/site-packages/fastrenewables/timeseries/data.py:57\u001b[0m, in \u001b[0;36mRenewableTimeSeriesDataLoaders.from_files\u001b[0;34m(cls, files, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_files\u001b[39m(\u001b[38;5;28mcls\u001b[39m, files,  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     55\u001b[0m         dfs \u001b[38;5;241m=\u001b[39m read_files(files)\n\u001b[0;32m---> 57\u001b[0m         dfs \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#         if \"cat_names\" in kwargs.keys():\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#             kwargs[\"cat_names\"] = kwargs[\"cat_names\"] if \"TaskID\" in kwargs[\"cat_names\"] else kwargs[\"cat_names\"] + [\"TaskID\"]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#         else:\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#             kwargs[\"cat_names\"] = [\"TaskID\"]\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_df(dfs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/core/reshape/concat.py:294\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m     92\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/core/reshape/concat.py:351\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    348\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "dls = get_dls(y_names=\"PowerGeneration\", y_block=RegressionBlock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one ouput for the regression task, the PowerGeneration and the MSE loss as default loss. Note that these values are inherited by the `TabularPandas` module from `fastai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Number of targets: {dls.train_ds.c} with loss {dls.train_ds.loss_func}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can easily extend this to multiple targets by providing the required y columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(cont_names = \"WindSpeed58m\", y_names=['T_HAG_2_M', 'RELHUM_HAG_2_M'], y_block=RegressionBlock())\n",
    "f\"Number of targets: {dls.train_ds.c}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the same procedure for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(y_names=[\"TaskID\"], y_block=CategoryBlock())\n",
    "f\"Number of targets: {dls.train_ds.c} with loss {dls.train_ds.loss_func}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with the default values, we have one value for each timestamp. But in most cases we only want to classify a single sample for a timeseries sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dls.one_batch()[-1]\n",
    "y[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome this issue, we can apply `post_hooks` to shorten the target timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(y_names=[\"TaskID\"], y_block=CategoryBlock(), post_hooks=[reduce_target_timeseries_to_element])\n",
    "f\"Number of targets: {dls.train_ds.c} with loss {dls.train_ds.loss_func}.\"\n",
    "y = dls.one_batch()[-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Learner.__init__)\n",
    "def renewable_timeseries_learner(dls, layers=None, emb_szs=None, config=None, \n",
    "                                 n_out=None, y_range=None, \n",
    "                                 embedding_type=EmbeddingType.Normal, \n",
    "                                 input_sequence_length=None,\n",
    "                                 output_sequence_length=None,\n",
    "                                 sequence_transform=None,\n",
    "                                 **kwargs):\n",
    "    \"Get a `Learner` using `dls`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
    "    if config is None: config = tabular_config()\n",
    "    \n",
    "    if n_out is None: \n",
    "        n_out = get_c(dls)\n",
    "#     n_out = dls.train_ds.ys.shape[1]\n",
    "        \n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    \n",
    "    if layers is None: layers = [len(dls.cont_names), 200, 100, n_out]\n",
    "    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n",
    "        \n",
    "    embed_p = kwargs[\"embed_p\"].pop() if \"embed_p\" in kwargs.keys() else 0.1\n",
    "    \n",
    "    emb_module = None\n",
    "    if len(dls.train_ds.cat_names) > 0:\n",
    "        emb_szs = get_emb_sz(dls.train_ds, {} if emb_szs is None else emb_szs)\n",
    "        emb_module = EmbeddingModule(None, embedding_dropout=embed_p, embedding_dimensions=emb_szs)\n",
    "        \n",
    "    model = TemporalCNN(layers, embedding_module=emb_module, \n",
    "                        input_sequence_length=input_sequence_length,\n",
    "                        output_sequence_length=output_sequence_length, \n",
    "                        sequence_transform=sequence_transform,\n",
    "                        **config)\n",
    "    \n",
    "    if embedding_type==EmbeddingType.Bayes and \"loss_func\" not in kwargs.keys():\n",
    "        base_loss = getattr(dls.train_ds, 'loss_func', None)\n",
    "        assert base_loss is not None, \"Could not infer loss function from the data, please pass a loss function.\"\n",
    "        loss_func=VILoss(model=model, base_loss=base_loss, kl_weight=0.1)\n",
    "        kwargs[\"loss_func\"] = loss_func\n",
    "    \n",
    "    return RenewableTimeseriesLearner(dls, model, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(renewable_timeseries_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.tabular.data import *\n",
    "from fastai.tabular.core import *\n",
    "from fastai.tabular.model import *\n",
    "from fastai.basics import *\n",
    "from fastrenewables.tabular.core import *\n",
    "from fastrenewables.tabular.data import *\n",
    "from fastrenewables.tabular.model import *\n",
    "from fastrenewables.losses import VILoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(y_names=\"PowerGeneration\", y_block=RegressionBlock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = renewable_timeseries_learner(dls, metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats, conts, ys = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model(cats, conts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = learner.predict(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = learner.predict(0, as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(targets[0:200], label=\"Target\")\n",
    "plt.plot(preds[0:200], label=\"Prediction\")\n",
    "plt.legend()\n",
    "# result_df[500:1000].plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a clasification dataloader, where we aim to forecast the task ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(cat_names = ['Month', 'Day', 'Hour'], y_names=[\"TaskID\"], \n",
    "              y_block=CategoryBlock(), \n",
    "              post_hooks=[reduce_target_timeseries_to_element])\n",
    "f\"Number of classes {dls.c}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# check that the convert function are working\n",
    "ts_data_cont = dls.train_ds.conts\n",
    "ts_data_cat = dls.train_ds.cats\n",
    "index, cats, conts, ys = convert_to_tensor_ts(dls.train_ds, include_index=True, flatten=True)\n",
    "test_eq(cats[1,0], ts_data_cat[0,0,1])\n",
    "test_eq(cats[1,1], ts_data_cat[0,1,1])\n",
    "test_eq(cats[1,2], ts_data_cat[0,2,1])\n",
    "test_eq(conts[1,0], ts_data_cont[0,0,1])\n",
    "test_eq(conts[1,1], ts_data_cont[0,1,1])\n",
    "test_eq(conts[1,2], ts_data_cont[0,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either let the `renewable_timeseries_learner` handle the correct output shape by providing the `input_sequence_length` and `output_sequence_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence_length=dls.input_sequence_length\n",
    "output_sequence_length=dls.output_sequence_length\n",
    "input_sequence_length, output_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = renewable_timeseries_learner(dls, metrics=accuracy, \n",
    "               input_sequence_length=input_sequence_length,\n",
    "               output_sequence_length=output_sequence_length)\n",
    "learner.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or alternatively, we can provide a custom layer converts the results of the `CNN` into the required shape for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs_last_cnn_layer = 10\n",
    "sequence_transform=MultiLayerPerceptron([input_sequence_length*num_outputs_last_cnn_layer, dls.c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = renewable_timeseries_learner(dls, n_out=10, metrics=accuracy, \n",
    "               input_sequence_length=input_sequence_length,\n",
    "               sequence_transform=sequence_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
