{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gan.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gan.model\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# export\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def flatten_ts(x):\n",
    "    \"\"\"assumes matrix of shape (n_samples, n_features, ts_length)\"\"\"\n",
    "    if len(x.shape) in [1,2]:\n",
    "        return x\n",
    "\n",
    "    n_samples, n_features, ts_length = x.shape\n",
    "\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = x.swapaxes(1,2)\n",
    "    else:\n",
    "        x = x.permute(0,2,1)\n",
    "    x = x.reshape(n_samples*ts_length, n_features)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def LinBnAct(si, so, use_bn, act_cls):\n",
    "    layers = [nn.Linear(si,so)]\n",
    "    if use_bn:\n",
    "        layers += [nn.BatchNorm1d(so)]\n",
    "    if act_cls is not None:\n",
    "        layers += [act_cls]\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class GANMLP(torch.nn.Module):\n",
    "    def __init__(self, ann_structure, use_bn=True, bn_cont=False, act_cls=torch.nn.ReLU(), embedding_module=None, final_activation=None):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(GANMLP, self).__init__()\n",
    "        \n",
    "        n_cont = ann_structure[0]\n",
    "        if embedding_module is not None:\n",
    "            emb_sz = []\n",
    "            ann_structure[0] = ann_structure[0] + embedding_module.no_of_embeddings\n",
    "\n",
    "        self.embedding_module = embedding_module\n",
    "        \n",
    "        layers = []\n",
    "        for idx in range(1, len(ann_structure)):\n",
    "            cur_use_bn = use_bn\n",
    "            cur_act_cls = act_cls\n",
    "            if idx == 1 and not bn_cont:\n",
    "                cur_use_bn = False\n",
    "            if idx == len(ann_structure)-1:\n",
    "                cur_act_cls=None\n",
    "                cur_use_bn = False\n",
    "                \n",
    "            layer = LinBnAct(ann_structure[idx-1], ann_structure[idx], cur_use_bn , cur_act_cls )\n",
    "            layers.append(layer)\n",
    "        if final_activation is not None:\n",
    "            layers.append(final_activation)\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, cat, continuous_data):\n",
    "        if self.embedding_module is not None:\n",
    "            cat = self.embedding_module(cat)\n",
    "            continuous_data = torch.cat([cat, continuous_data], 1)\n",
    "        \n",
    "        return self.layers(continuous_data)\n",
    "    \n",
    "    \n",
    "class AuxiliaryDiscriminator(torch.nn.Module):\n",
    "    def __init__(self, basic_discriminator, n_classes, input_size, model_type='mlp'):\n",
    "        super(AuxiliaryDiscriminator, self).__init__()\n",
    "        self.basic_discriminator = basic_discriminator\n",
    "        self.n_classes = n_classes\n",
    "        self.input_size = input_size\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        if self.model_type == 'mlp':\n",
    "            self.adv_layer = nn.Sequential(nn.Linear(self.input_size, 1), nn.Sigmoid())\n",
    "            self.aux_layer = nn.Sequential(nn.Linear(self.input_size, self.n_classes), nn.Softmax(dim=1))\n",
    "        elif self.model_type == 'tcn':\n",
    "            self.adv_layer = nn.Sequential(nn.Linear(self.input_size, 1), nn.Sigmoid())\n",
    "            self.aux_layer = nn.Sequential(nn.Linear(self.input_size, self.n_classes), nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, cats, conts):\n",
    "        out = self.basic_discriminator(None, conts)\n",
    "        if self.model_type == 'tcn':\n",
    "            out = out.flatten(1, 2)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return (validity, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5461,  0.6540, -0.2578],\n",
       "        [ 0.0379,  0.8133, -0.1649]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "GANMLP([1,2,3]).forward(None, torch.tensor([1.0, 2.0]).reshape(-1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
