{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp timeseries.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# timeseries.model\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.tabular.data import *\n",
    "from fastai.tabular.core import *\n",
    "from fastrenewables.timeseries.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "import numpy as np\n",
    "import warnings\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.utils import weight_norm\n",
    "from fastcore.foundation import defaults\n",
    "from fastai.tabular.model import *\n",
    "from fastai.layers import *\n",
    "from fastrenewables.tabular.model import *\n",
    "from fastrenewables.utils_blitz import set_train_mode\n",
    "from torch.nn import BatchNorm1d\n",
    "from enum import Enum\n",
    "\n",
    "from blitz.utils import variational_estimator\n",
    "import torch.nn.functional as F\n",
    "from fastcore.foundation import L\n",
    "from fastai.torch_core import params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"Removes excess padding.\"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, : -self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4, 5, 6]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chomp1d(1)(torch.tensor([1,2,3,4,5,6,7,8]).reshape(1,1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4, 5, 6]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chomp1d(2)(torch.tensor([1,2,3,4,5,6,7,8]).reshape(1,1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??ConvLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BasicTemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Extends fastai `ConvLayer` (CNN+normalization) to include results from an embedding layer,\n",
    "        as proposed in Task-TCN (https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/main.pdf) for \n",
    "        MTL-Architectures.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_inputs,\n",
    "        n_outputs,\n",
    "        kernel_size=3,\n",
    "        act_func=nn.ReLU,\n",
    "        embedding_size=None,\n",
    "        transpose=False,\n",
    "    ):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            n_inputs ([type]): [description]\n",
    "            n_outputs ([type]): [description]\n",
    "            kernel_size (int, optional): [description]. Defaults to 3.\n",
    "            act_func ([type], optional): [description]. Defaults to nn.ReLU.\n",
    "            embedding_size ([type], optional): [description]. Defaults to None.\n",
    "            transpose (bool, optional): [description]. Defaults to False.\n",
    "        \"\"\"\n",
    "        super(BasicTemporalBlock, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.act_func = act_func\n",
    "\n",
    "        self.conv = ConvLayer(\n",
    "            n_inputs,\n",
    "            n_outputs,\n",
    "            ks=kernel_size,\n",
    "            stride=1,\n",
    "            padding=kernel_size // 2,\n",
    "            norm_type=NormType.Weight,\n",
    "            bias=False,\n",
    "            ndim=1,\n",
    "            act_cls=self.act_func,\n",
    "            transpose=transpose,\n",
    "        )\n",
    "\n",
    "        if self.embedding_size is not None:\n",
    "            self.embedding_transform = (\n",
    "                nn.Conv1d(self.embedding_size, n_outputs, 1)\n",
    "                if (self.embedding_size != n_outputs)\n",
    "                and (self.embedding_size is not None)\n",
    "                else None\n",
    "            )\n",
    "            self.emb_act_func = self.act_func()\n",
    "        else:\n",
    "            self.embedding_transform = None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # ConvLayer uses fastai init strategy\n",
    "        # use fastais init strategy\n",
    "        if self.embedding_transform is not None:\n",
    "            init_linear(self.embedding_transform, act_func=self.act_func, init=\"auto\")\n",
    "\n",
    "    def forward(self, categorical, continous=None):\n",
    "        res = self.conv(continous)\n",
    "\n",
    "        res = (\n",
    "            res\n",
    "            if self.embedding_transform is None\n",
    "            else self.emb_act_func(self.embedding_transform(categorical) + res)\n",
    "        )\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1],[1],[1]]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(torch.Size([1, 1, 1]), BasicTemporalBlock(3,1)(torch.empty(1), a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        (Single) Residual block of a TCN.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_inputs,\n",
    "        n_outputs,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dilation,\n",
    "        padding,\n",
    "        act_func=nn.ReLU,\n",
    "        embedding_size=None,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.act_func = Identity if act_func is None else act_func\n",
    "\n",
    "        self.conv1 = weight_norm(\n",
    "            nn.Conv1d(\n",
    "                n_inputs,\n",
    "                n_outputs,\n",
    "                kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                dilation=dilation,\n",
    "            )\n",
    "        )\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.act_func1 = self.act_func()\n",
    "        # equivalent to keras spatial dropout if input is of form (batch_size, channels, time_series_length)\n",
    "        self.dropout1 = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(\n",
    "            nn.Conv1d(\n",
    "                n_outputs,\n",
    "                n_outputs,\n",
    "                kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                dilation=dilation,\n",
    "            )\n",
    "        )\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.act_func2 = self.act_func()\n",
    "        # equivalent to keras spatial dropout if input is of form (batch_size, channels, time_series_length)\n",
    "        self.dropout2 = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.chomp1,\n",
    "            self.act_func1,\n",
    "            self.dropout1,\n",
    "            self.conv2,\n",
    "            self.chomp2,\n",
    "            self.act_func2,\n",
    "            self.dropout2,\n",
    "        )\n",
    "\n",
    "        self.downsample = (\n",
    "            nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        )\n",
    "\n",
    "        self.act_func3 = self.act_func()\n",
    "\n",
    "        if self.embedding_size is not None:\n",
    "            self.embedding_transform = (\n",
    "                nn.Conv1d(self.embedding_size, n_outputs, 1)\n",
    "                if (self.embedding_size != n_outputs)\n",
    "                and (self.embedding_size is not None)\n",
    "                else None\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.embedding_transform = None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the convolution layers\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        # use fastais init strategy\n",
    "        init_linear(self.conv1, act_func=self.act_func, init=\"auto\")\n",
    "        init_linear(self.conv2, act_func=self.act_func, init=\"auto\")\n",
    "        if self.downsample is not None:\n",
    "            init_linear(self.downsample, act_func=self.act_func, init=\"auto\")\n",
    "        if self.embedding_transform is not None:\n",
    "            init_linear(self.embedding_transform, act_func=self.act_func, init=\"auto\")\n",
    "\n",
    "    def forward(self, categorical, continous=None):\n",
    "        out = self.net(continous)\n",
    "        res = continous if self.downsample is None else self.downsample(continous)\n",
    "        res = out + res\n",
    "\n",
    "        res = (\n",
    "            res\n",
    "            if self.embedding_transform is None\n",
    "            else self.embedding_transform(categorical) + res\n",
    "        )\n",
    "\n",
    "        return self.act_func3(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 24])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((3, 3, 24)).float();a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualBlock(3, 1, 3, 1, 1, padding=2)\n",
    "test_eq(torch.Size([3, 1, 24]), model(None, a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TemporalConvNet(nn.Module):\n",
    "    \"\"\"Wrapper module that is capable of creating a simple CNN or a TCN.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs,\n",
    "        num_channels,\n",
    "        kernel_size=3,\n",
    "        dropout=0.0,\n",
    "        cnn_type=\"tcn\",\n",
    "        embedding_size=None,\n",
    "        final_activation=Identity,\n",
    "        act_func=nn.ReLU,\n",
    "        add_embedding_at_layer=L(),\n",
    "        transpose=False,\n",
    "    ):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            num_inputs ([type]): [description]\n",
    "            num_channels ([type]): [description]\n",
    "            kernel_size (int, optional): [description]. Defaults to 3.\n",
    "            dropout (float, optional): [description]. Defaults to 0.0.\n",
    "            cnn_type (str, optional): [description]. Defaults to \"tcn\".\n",
    "            embedding_size ([type], optional): [description]. Defaults to None.\n",
    "            final_activation ([type], optional): [description]. Defaults to Identity.\n",
    "            act_func ([type], optional): [description]. Defaults to nn.ReLU.\n",
    "            add_embedding_at_layer ([type], optional): [description]. Defaults to L().\n",
    "            transpose (bool, optional): [description]. Defaults to False.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: [description]\n",
    "        \"\"\"\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.cnn_type = cnn_type\n",
    "        layers = []\n",
    "\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i - 1]\n",
    "            out_channels = num_channels[i]\n",
    "\n",
    "            if self.cnn_type == \"tcn\":\n",
    "                dilation_size = 2 ** i\n",
    "                cur_layer = ResidualBlock(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size,\n",
    "                    stride=1,\n",
    "                    dilation=dilation_size,\n",
    "                    padding=(kernel_size - 1) * dilation_size,\n",
    "                    dropout=dropout,\n",
    "                    embedding_size=self.embedding_size\n",
    "                    if i in add_embedding_at_layer\n",
    "                    else None,\n",
    "                    act_func=act_func if i < num_levels - 1 else final_activation,\n",
    "                )\n",
    "            elif self.cnn_type == \"cnn\":\n",
    "                cur_layer = BasicTemporalBlock(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    embedding_size=self.embedding_size\n",
    "                    if i in add_embedding_at_layer\n",
    "                    else None,\n",
    "                    act_func=act_func if i < num_levels - 1 else final_activation,\n",
    "                    transpose=transpose,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Expected cnn/tcn as input for cnn_type.\")\n",
    "\n",
    "            layers += [cur_layer]\n",
    "\n",
    "        self.temporal_blocks = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, categorical, continous=None):\n",
    "        \"\"\"The categorical data can either be encoded via an embedding layer or directly applied.\"\"\"\n",
    "        x = continous\n",
    "        for layer in self.temporal_blocks:\n",
    "            x = layer(categorical, x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_shape = TemporalConvNet(3,[1,1])(torch.empty(1), a).shape\n",
    "test_eq(torch.Size([3, 1, 24]), resulting_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@variational_estimator\n",
    "class TemporalCNN(nn.Module):\n",
    "    \"\"\"Module to create a CNN based architecture for timerseries such as a simple CNN, a TCN, or a Task-TCN.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnn_structure,\n",
    "        kernel_size=3,\n",
    "        dropout=0.0,\n",
    "        embedding_module=None,\n",
    "        batch_norm_cont=True,\n",
    "        cnn_type=\"tcn\",\n",
    "        y_ranges=None,\n",
    "        final_activation=Identity,\n",
    "        act_func=nn.ReLU,\n",
    "        add_embedding_at_layer=[],\n",
    "        input_sequence_length=None,\n",
    "        output_sequence_length=None,\n",
    "        transpose=False,\n",
    "        sequence_transform = None\n",
    "    ):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            cnn_structure ([type]): [description]\n",
    "            kernel_size (int, optional): [description]. Defaults to 3.\n",
    "            dropout (float, optional): [description]. Defaults to 0.0.\n",
    "            embedding_module ([type], optional): [description]. Defaults to None.\n",
    "            batch_norm_cont (bool, optional): [description]. Defaults to True.\n",
    "            cnn_type (str, optional): [description]. Defaults to \"tcn\".\n",
    "            y_ranges ([type], optional): [description]. Defaults to None.\n",
    "            final_activation ([type], optional): [description]. Defaults to Identity.\n",
    "            act_func ([type], optional): [description]. Defaults to nn.ReLU.\n",
    "            add_embedding_at_layer (list, optional): [description]. Defaults to [].\n",
    "            input_sequence_length ([type], optional): [description]. Defaults to None.\n",
    "            output_sequence_length ([type], optional): [description]. Defaults to None.\n",
    "            transpose (bool, optional): [description]. Defaults to False.\n",
    "        \"\"\"\n",
    "        super(TemporalCNN, self).__init__()\n",
    "\n",
    "        self.embedding_module = embedding_module\n",
    "        self.cnn_structure = cnn_structure\n",
    "        self.cnn_type = cnn_type.lower()\n",
    "        self.batch_norm_cont = batch_norm_cont\n",
    "        self.y_ranges = y_ranges\n",
    "        self.dropout = dropout\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_sequence_length = input_sequence_length\n",
    "        self.output_sequence_length = output_sequence_length\n",
    "        self.transpose = transpose\n",
    "\n",
    "        self.bn_cont = (\n",
    "            BatchNorm1d(self.cnn_structure[0]) if self.batch_norm_cont else None\n",
    "        )\n",
    "        self.embedding_size = (\n",
    "            self.embedding_module.no_of_embeddings\n",
    "            if self.embedding_module is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        num_channels = self.cnn_structure[1:]\n",
    "        num_inputs = self.cnn_structure[0]\n",
    "\n",
    "        self.layers = TemporalConvNet(\n",
    "            num_inputs,\n",
    "            num_channels=num_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            dropout=self.dropout,\n",
    "            cnn_type=self.cnn_type,\n",
    "            embedding_size=self.embedding_size,\n",
    "            final_activation=final_activation,\n",
    "            act_func=act_func,\n",
    "            add_embedding_at_layer=add_embedding_at_layer,\n",
    "            transpose=self.transpose,\n",
    "        )\n",
    "        \n",
    "        self.custom_output_sequence=False\n",
    "        if output_sequence_length != None and input_sequence_length != None and sequence_transform is None:\n",
    "            # TODO: can this replace with a 1D-CNN with kernel size 1?\n",
    "            self.sequence_transform = nn.Linear(\n",
    "                self.cnn_structure[-1] * self.input_sequence_length,\n",
    "                self.cnn_structure[-1] * self.output_sequence_length,\n",
    "            )\n",
    "        elif sequence_transform is not None:\n",
    "            self.sequence_transform = sequence_transform\n",
    "            self.custom_output_sequence=True\n",
    "        else:\n",
    "            self.sequence_transform = None\n",
    "            self.custom_output_sequence=False\n",
    "        \n",
    "            \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        categorical_data,\n",
    "        continuous_data,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        categorical_data : pytorch.Tensor\n",
    "            categorical input data. only used when an embedding module is available.\n",
    "        continuous_data : pytorch.Tensor\n",
    "            continuous input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            concatenated outputs of all separate subnetworks.\n",
    "        \"\"\"\n",
    "        if self.batch_norm_cont:\n",
    "            # expecting (batch_size, n_features, timeseries_length)\n",
    "            continuous_data = self.bn_cont(continuous_data)\n",
    "\n",
    "        if self.embedding_module is not None:\n",
    "            categorical_data = self._forward_embedding_module(\n",
    "                categorical_data,\n",
    "            )\n",
    "\n",
    "        x = self.layers(categorical_data, continuous_data)\n",
    "\n",
    "        if self.custom_output_sequence:\n",
    "            x = x.reshape(-1, self.cnn_structure[-1] * self.input_sequence_length)\n",
    "            x = self.sequence_transform(categorical_data, x)\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        elif self.sequence_transform is not None:\n",
    "            x = x.reshape(-1, self.cnn_structure[-1] * self.input_sequence_length)\n",
    "            x = self.sequence_transform(x)\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        if self.y_ranges is not None:\n",
    "            y_range = self.y_ranges[0]\n",
    "            x = (y_range[1] - y_range[0]) * torch.sigmoid(x) + y_range[0]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _forward_embedding_module(self, categorical_data):\n",
    "        \"\"\"\n",
    "        Apply the embedding layer and return result.\n",
    "        ----------\n",
    "        categorical_data : pytorch.Tensor\n",
    "            categorical input data. only used when an embedding module is available.\n",
    "        continuous_data : pytorch.Tensor\n",
    "            continuous input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            combined tensors of continuous data and the output of the embedding layer.\n",
    "        \"\"\"\n",
    "        # check if all columns have the same value\n",
    "        if (\n",
    "            categorical_data[:, :, 0].reshape((-1, categorical_data.shape[1], 1))\n",
    "            - categorical_data\n",
    "        ).sum() == 0:\n",
    "            return self._forward_embedding_module_same(categorical_data)\n",
    "        else:\n",
    "            if self.embedding_module.embedding_type == EmbeddingType.Bayes:\n",
    "                warnings.warn(\n",
    "                    \"Mixed types not supported for bayesian embedding. Fallback to sampling per time step.\"\n",
    "                )\n",
    "\n",
    "            return self._forward_embedding_module_different(categorical_data)\n",
    "\n",
    "    def _forward_embedding_module_same(self, categorical_data):\n",
    "        \"\"\"\n",
    "        If all columns have the same value, apply the embedding method only to the first column\n",
    "        Parameters\n",
    "        ----------\n",
    "        categorical_data : pytorch.Tensor\n",
    "            categorical input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            resulting tensor of the embedding module.\n",
    "        \"\"\"\n",
    "        timesteps = categorical_data.shape[2]\n",
    "        batch_size = categorical_data.shape[0]\n",
    "\n",
    "        # Assume that all columns have the same value\n",
    "        # In case of an bayes embedding all should have the same value\n",
    "        categorical_data = categorical_data[:, :, 0]\n",
    "        categorical_data = self.embedding_module(categorical_data)\n",
    "        emb_dim = categorical_data.shape[1]\n",
    "        ones = torch.ones((batch_size, emb_dim, timesteps))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            ones = ones.cuda()\n",
    "\n",
    "        return ones * categorical_data.reshape(batch_size, emb_dim, 1)\n",
    "\n",
    "    def _forward_embedding_module_different(self, categorical_data):\n",
    "        \"\"\"\n",
    "        If all columns do not have the same value, apply the embedding method to the whole input tensor\n",
    "        Parameters\n",
    "        ----------\n",
    "        categorical_data : pytorch.Tensor\n",
    "            categorical input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            resulting tensor of the embedding module.\n",
    "        \"\"\"\n",
    "        features = categorical_data.shape[1]\n",
    "        timesteps = categorical_data.shape[2]\n",
    "\n",
    "        categorical_data = self.embedding_module(\n",
    "            categorical_data.permute(0, 2, 1).reshape(-1, features)\n",
    "        )\n",
    "        categorical_data = categorical_data.reshape(\n",
    "            -1, timesteps, categorical_data.shape[1]\n",
    "        ).permute(0, 2, 1)\n",
    "\n",
    "        return categorical_data\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super().train(mode)\n",
    "        set_train_mode(self, mode)\n",
    "\n",
    "    def network_split(self):\n",
    "        \"Default split of the between body and head\"\n",
    "\n",
    "        if self.embedding_module is not None:\n",
    "            splitter = lambda m: L(\n",
    "                m.layers.temporal_blocks[0],\n",
    "                m.embedding_module,\n",
    "                m.layers.temporal_blocks[1:-1],\n",
    "                m.layers.temporal_blocks[-1:],\n",
    "            ).map(params)\n",
    "\n",
    "            lr = L(1e-6, 1e-6, 1e-6, 1e-4)\n",
    "        else:\n",
    "            splitter = lambda m: L(\n",
    "                m.layers.temporal_blocks[0],\n",
    "                m.layers.temporal_blocks[1:-1],\n",
    "                m.layers.temporal_blocks[-1:],\n",
    "            ).map(params)\n",
    "\n",
    "            lr = L(1e-6, 1e-6, 1e-4)\n",
    "\n",
    "        return splitter, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalCNN(cnn_structure=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalCNN(\n",
       "  (bn_cont): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): TemporalConvNet(\n",
       "    (temporal_blocks): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv1d(1, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (act_func1): Identity()\n",
       "        (dropout1): Dropout2d(p=0.0, inplace=False)\n",
       "        (conv2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (act_func2): Identity()\n",
       "        (dropout2): Dropout2d(p=0.0, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(1, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (1): Chomp1d()\n",
       "          (2): Identity()\n",
       "          (3): Dropout2d(p=0.0, inplace=False)\n",
       "          (4): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (5): Chomp1d()\n",
       "          (6): Identity()\n",
       "          (7): Dropout2d(p=0.0, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(1, 2, kernel_size=(1,), stride=(1,))\n",
       "        (act_func3): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
