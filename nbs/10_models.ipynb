{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp autoencoder.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder.model\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from fastrenewables.tabular.model import *\n",
    "from fastrenewables.timeseries.model import *\n",
    "from fastai.tabular.all import *\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_structure = [10,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        z = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "        \n",
    "    \n",
    "    def decode(self, categorical_data, continuous_data, as_np=False):\n",
    "        x = self.decoder(categorical_data, continuous_data)\n",
    "        \n",
    "        if as_np: return to_np(x)\n",
    "        else: return x\n",
    "        \n",
    "    def forward(self, categorical_data, continuous_data):\n",
    "        x = self.encode(categorical_data, continuous_data)\n",
    "        x = self.decode(categorical_data, x)\n",
    "        \n",
    "        return continuous_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = Autoencoder(MultiLayerPerceptron(ann_structure), MultiLayerPerceptron(ann_structure[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (final_activation): Identity()\n",
       "  (embeds): ModuleList()\n",
       "  (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): LinBnDrop(\n",
       "      (0): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10), requires_grad=True)\n",
    "yhat = ae(None, x)\n",
    "yhat.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3278,  1.4655, -0.5048, -1.6327,  0.3967,  0.1263, -0.7591, -0.2791,\n",
       "         -2.2149,  0.4619],\n",
       "        [ 0.5561, -0.3939, -1.8063, -0.8131,  0.4049,  0.0339, -1.3636,  0.3701,\n",
       "          1.2007, -0.1760],\n",
       "        [-0.6178,  2.6330,  1.0283, -0.7023,  0.1291,  0.1873,  1.1784,  1.0725,\n",
       "          1.4288,  2.4448]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): TemporalCNN(\n",
       "    (bn_cont): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layers): TemporalConvNet(\n",
       "      (temporal_blocks): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv1d(10, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp1): Chomp1d()\n",
       "          (act_func1): Identity()\n",
       "          (dropout1): Dropout2d(p=0.0, inplace=False)\n",
       "          (conv2): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp2): Chomp1d()\n",
       "          (act_func2): Identity()\n",
       "          (dropout2): Dropout2d(p=0.0, inplace=False)\n",
       "          (net): Sequential(\n",
       "            (0): Conv1d(10, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (1): Chomp1d()\n",
       "            (2): Identity()\n",
       "            (3): Dropout2d(p=0.0, inplace=False)\n",
       "            (4): Conv1d(2, 2, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (5): Chomp1d()\n",
       "            (6): Identity()\n",
       "            (7): Dropout2d(p=0.0, inplace=False)\n",
       "          )\n",
       "          (downsample): Conv1d(10, 2, kernel_size=(1,), stride=(1,))\n",
       "          (act_func3): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TemporalCNN(\n",
       "    (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layers): TemporalConvNet(\n",
       "      (temporal_blocks): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv1d(2, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp1): Chomp1d()\n",
       "          (act_func1): Identity()\n",
       "          (dropout1): Dropout2d(p=0.0, inplace=False)\n",
       "          (conv2): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "          (chomp2): Chomp1d()\n",
       "          (act_func2): Identity()\n",
       "          (dropout2): Dropout2d(p=0.0, inplace=False)\n",
       "          (net): Sequential(\n",
       "            (0): Conv1d(2, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (1): Chomp1d()\n",
       "            (2): Identity()\n",
       "            (3): Dropout2d(p=0.0, inplace=False)\n",
       "            (4): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "            (5): Chomp1d()\n",
       "            (6): Identity()\n",
       "            (7): Dropout2d(p=0.0, inplace=False)\n",
       "          )\n",
       "          (downsample): Conv1d(2, 10, kernel_size=(1,), stride=(1,))\n",
       "          (act_func3): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_tcn = Autoencoder(TemporalCNN(ann_structure), TemporalCNN(ann_structure[::-1]))\n",
    "ae_tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([3, 10, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10,2), requires_grad=True)\n",
    "yhat = ae_tcn(None, x)\n",
    "yhat.requires_grad, yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2362,  2.1993],\n",
       "        [ 0.9431, -0.9357],\n",
       "        [-0.3133, -1.1119],\n",
       "        [ 0.0310,  0.6986],\n",
       "        [ 0.1010, -0.1660],\n",
       "        [-0.3526, -2.0143],\n",
       "        [-0.1462, -0.6417],\n",
       "        [ 0.0596,  0.4791],\n",
       "        [-1.0819,  1.0265],\n",
       "        [ 1.7406, -0.9789]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "#     def __init__(self, size):\n",
    "#         self.size = size\n",
    "        \n",
    "    def forward(self, input, dims):\n",
    "        return input.view(*dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(Autoencoder):\n",
    "    def __init__(self, encoder, decoder, h_dim, z_dim):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.flatten = Flatten()\n",
    "        self.unflatten = UnFlatten()\n",
    "        \n",
    "        self.hidden2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hidden2logvar = nn.Linear(h_dim, z_dim)\n",
    "        \n",
    "    def encode(self, categorical_data, continuous_data, as_np=False):\n",
    "        x_hidden = self.encoder(categorical_data, continuous_data)\n",
    "        \n",
    "        x_hidden = self.flatten(x_hidden)\n",
    "        \n",
    "        mu, logvar = self.hidden2mu(x_hidden), self.hidden2logvar(x_hidden)\n",
    "        print(\"reparam\")\n",
    "        z = self.reparam(mu, logvar)\n",
    "        \n",
    "        if as_np: return to_np(z)\n",
    "        else: return z\n",
    "        \n",
    "    def get_posteriors(self, categorical_data, continuous_data):\n",
    "\n",
    "        return self.encode(continuous_data, categorical_data)\n",
    "\n",
    "    def get_z(self, categorical_data, continuous_data):\n",
    "        \"\"\"Encode a batch of data points, x, into their z representations.\"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(categorical_data, continuous_data)\n",
    "        return self.reparam(mu, logvar)\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        \"\"\"Reparameterisation trick to sample z values.\n",
    "        This is stochastic during training, and returns the mode during evaluation.\"\"\"\n",
    "\n",
    "        if self.training:\n",
    "            print(\"sample\")\n",
    "            # convert logarithmic variance to standard deviation representation\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            # create normal distribution as large as the data\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            # scale by learned mean and standard deviation\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "#     def decode(self, categorical_data, continuous_data, unflatten_dims=None, as_np=False):\n",
    "        \n",
    "#         if not unflatten_dims: \n",
    "                \n",
    "#         continuous_data = self.unflatten(continuous_data, unflatten_dims)\n",
    "        \n",
    "#         x = self.decoder(categorical_data, continuous_data)\n",
    "        \n",
    "#         if as_np: return to_np(x)\n",
    "        \n",
    "#         else: return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,10), requires_grad=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = MultiLayerPerceptron(ann_structure)\n",
    "dec = MultiLayerPerceptron(ann_structure[::-1])\n",
    "\n",
    "vae = VariationalAutoencoder(enc, dec, ann_structure[-1], ann_structure[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4aa07d9b0f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;31m# why is this not sampling???\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0 # why is this not sampling???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reparam\n",
      "sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4814, -0.3851,  1.4582,  0.5811,  1.9413, -0.9969,  0.0164,  0.4830,\n",
       "         -0.5062, -0.3639],\n",
       "        [-0.0331, -0.1893,  0.7329, -1.1195,  2.5510,  0.7493,  0.6637,  0.0988,\n",
       "         -0.2158,  0.4984],\n",
       "        [ 0.8742, -0.3657, -1.0747,  0.2848,  0.2856,  0.2524, -0.3153,  0.1281,\n",
       "         -1.3582,  0.3956]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.training = True\n",
    "vae.forward(None, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
