{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# losses\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import nn\n",
    "import torch\n",
    "from fastai.losses import MSELossFlat\n",
    "from fastrenewables.utils import unflatten_to_ts, flatten_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VILoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the Kullback-Leibler divergence loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dies.embedding\n",
    "        the given embedding to base the loss on\n",
    "    lambd : float\n",
    "        scalar for the loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        base_loss=torch.nn.MSELoss(),\n",
    "        kl_weight=0.1,\n",
    "        scale_log_likelihood=True\n",
    "    ):\n",
    "        super(VILoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.model = model\n",
    "        self.kl_weight = kl_weight\n",
    "        self.scale_log_likelihood=scale_log_likelihood\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        y_hat : pytorch.Tensor\n",
    "            a tensor with the same shape as 'y'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting accumulated loss\n",
    "        \"\"\"\n",
    "        base_loss = self.base_loss(y, y_hat)\n",
    "\n",
    "        n_samples = max(len(y), 1)\n",
    "\n",
    "        if self.scale_log_likelihood:\n",
    "            base_loss = base_loss * n_samples\n",
    "\n",
    "        kl = self.model.nn_kl_divergence()\n",
    "\n",
    "        loss = base_loss + self.kl_weight * kl\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"VILoss(\\n  (base_loss):\" + str(self.base_loss) + f\"\\n  (kl_weight): {self.kl_weight} \\n)\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Quantile_Score(torch.nn.Module):\n",
    "    \n",
    "    def reshape_1(self, x):\n",
    "        return x.view(x.size()[0],1)\n",
    "    \n",
    "    def __init__(self, taus=[0.25, 0.5, 0.75]):\n",
    "        super(Quantile_Score, self).__init__()\n",
    "        \n",
    "        self.taus =  torch.autograd.Variable(torch.tensor(taus, dtype=torch.float32), \n",
    "                                             requires_grad=False)\n",
    "        \n",
    "        self.taus = self.reshape_1(self.taus).t()\n",
    "        \n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Example:\n",
    "            y = np.array([0.2, 0.1, 0.3, 0.4])\n",
    "            tau=np.array([0.25,0.5,0.75])\n",
    "            for each sample we have one row \n",
    "            y_hat = np.array([[0, 0.2, 0.3], \n",
    "                          [0.05, 0.1, 0.35], \n",
    "                          [0.2, 0.3, 0.6],\n",
    "                          [0.3, 0.4, 0.45],])\n",
    "            res = array([0.125 , 0.2   , 0.25  , 0.0625])\n",
    "        \"\"\"\n",
    "        y = flatten_ts(y)\n",
    "        y_hat = flatten_ts(y_hat)\n",
    "        y = self.reshape_1(y)\n",
    "        v = y - y_hat\n",
    "        \n",
    "        r = (torch.abs(v*(v>0).float()) * self.taus + \\\n",
    "             torch.abs(v*(v<0).float()) * (1-self.taus))\n",
    "\n",
    "        # this would calculate the loss for each sample\n",
    "        # r =  torch.sum(r,dim=1)\n",
    "        r =  torch.sum(r)\n",
    "        \n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.5000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = Quantile_Score(taus=[0.25, 0.5, 0.75]).to(\"cpu\")\n",
    "probabilistic_forecasts = torch.tensor([[1,2,3],[4,5,6],[7,8,9],])\n",
    "measurements = torch.tensor([2,5,8])\n",
    "loss(probabilistic_forecasts, measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CnnMSELoss(torch.nn.MSELoss):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Calculate the MSELoss and take the mean over all features\n",
    "        \"\"\"\n",
    "        super(CnnMSELoss, self).__init__(None, None, \"mean\")\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        target : pytorch.Tensor\n",
    "            a tensor with the same shape as 'input'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting loss\n",
    "        \"\"\"\n",
    "        return torch.mean(torch.mean(torch.mean(torch.pow((y - yhat), 2), 2), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VAEReconstructionLoss(nn.Module):\n",
    "    def __init__(self, model, reconstruction_cost_function=MSELossFlat()):\n",
    "        \"\"\"\n",
    "        Calculate the sum of the Kullback–Leibler divergence loss and the loss of any given function\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : dies.autoencoder\n",
    "            model of the autoencoder for which the loss is to be calculated\n",
    "        \"\"\"\n",
    "        super(VAEReconstructionLoss, self).__init__()\n",
    "        self.reconstruction_cost_function = reconstruction_cost_function\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x_hat, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        x_hat : pytorch.Tensor\n",
    "            a tensor with the same shape as 'x'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting accumulated loss\n",
    "        \"\"\"\n",
    "        mu, logvar = self.model._mu, self.model._logvar\n",
    "        \n",
    "        # how well do input x and output x_hat agree?\n",
    "        generation_loss = self.reconstruction_cost_function(x_hat, x)\n",
    "        # KLD is Kullback–Leibler divergence -- how much does one learned\n",
    "        # distribution deviate from another, in this specific case the\n",
    "        # learned distribution from the unit Gaussian\n",
    "\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # - D_{KL} = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        # note the negative D_{KL} in appendix B of the paper\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        # Normalise by same number of elements as in reconstruction\n",
    "        KLD /= x.shape[0] * x.shape[1]\n",
    "\n",
    "        # BCE tries to make our reconstruction as accurate as possible\n",
    "        # KLD tries to push the distributions as close as possible to unit Gaussian\n",
    "        \n",
    "            \n",
    "        return generation_loss + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BTuningLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the B-Tuning Loss based on Ranking and `Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs`\n",
    "    Parameters\n",
    "    ----------\n",
    "        lambd : float\n",
    "        scalar for the loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_loss=MSELossFlat(),\n",
    "        lambd=0.5,\n",
    "    ):\n",
    "        super(BTuningLoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        y_hat : pytorch.Tensor\n",
    "            a tensor where in each sample the first model is the actual prediction, where the others are from a LinearTransferModel.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting accumulated loss\n",
    "        \"\"\"\n",
    "        y_hat_model = y_hat[:,0]\n",
    "        y_hat_reference_models = y_hat[:,1:]\n",
    "        n_reference_models = y_hat_reference_models.shape[1]\n",
    "        \n",
    "        base_loss = self.base_loss(y, y_hat_model)\n",
    "        \n",
    "        b_tuning_loss = 0\n",
    "        for idx in range(n_reference_models):\n",
    "#             b_tuning_loss += self.base_loss(y_hat_reference_models[:,idx], y_hat_model)\n",
    "            b_tuning_loss += (y_hat_reference_models[:,idx].ravel()- y_hat_model.ravel())**2\n",
    "        b_tuning_loss /= n_reference_models\n",
    "        b_tuning_loss = b_tuning_loss.mean()\n",
    "\n",
    "        loss = base_loss + self.lambd*b_tuning_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"BTuningLoss(\\n  (base_loss):\" + str(self.base_loss) + f\"\\n  (lambd): {self.lambd} \\n)\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def btuning_rmse(y_hat, y):\n",
    "    return ((y.ravel()-y_hat[:,0].ravel())**2).mean()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# class GaussianNegativeLogLikelihoodLoss(nn.Module):\n",
    "# #     def __init__(self):\n",
    "# #         \"\"\"\n",
    "# #         .. math::\n",
    "# #         Loss = \\frac{1}{N} \\sum_{i} \\frac{1}{2} \\exp(- s_i) || y_i - \\hat{y}_i ||^2 + \\frac{1}{2} s_i\n",
    "# #         \"\"\"\n",
    "# #         super(GaussianNegativeLogLikelihoodLoss, self).__init__()\n",
    "\n",
    "\n",
    "#     def forward(self, output, target):\n",
    "#         \"\"\"\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         y : pytorch.Tensor\n",
    "#             any given tensor. Shape: [n, ]\n",
    "#         output : pytorch.Tensor\n",
    "#             a tensor of mu [:,0] and sigma [:,1]. Shape: [n,2]\n",
    "        \n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         pytorch.Tensor\n",
    "#             the resulting loss\n",
    "#         \"\"\"\n",
    "#         if isinstance(output, tuple):\n",
    "#             mean, sigma = output[0], output[1]\n",
    "#         else:\n",
    "#             mean, sigma = output[:,0], output[:,1]\n",
    "            \n",
    "#         squared_error = torch.pow((target - mean), 2)\n",
    "\n",
    "#         loss = torch.mean(0.5 * (torch.exp(-sigma) * squared_error + sigma))\n",
    "\n",
    "#         return loss\n",
    "\n",
    "class GaussianNegativeLogLikelihoodLoss(torch.nn.Module):\n",
    "    # Heteroscedastic Aleatoric Uncertainty Loss\n",
    "\n",
    "    r\"\"\"\n",
    "    .. math::\n",
    "        Loss = \\frac{1}{N} \\sum_{i} \\frac{1}{2} \\exp(- s_i) || y_i - \\hat{y}_i ||^2 + \\frac{1}{2} s_i\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \n",
    "        # flatten so that it also works for timeseries\n",
    "        squared_error = torch.pow((target[:, 0].flatten() - output[:, 0].flatten()), 2)\n",
    "        loss = torch.mean(0.5 * (torch.exp(-output[:, 1].flatten()) * squared_error + output[:, 1].flatten()))\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RSSLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Calculate the Residual sum of squares loss\n",
    "        \"\"\"\n",
    "        super(RSSLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        y_hat : pytorch.Tensor\n",
    "            a tensor with the same shape as 'y'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting loss\n",
    "        \"\"\"\n",
    "        return ((y - y_hat) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class L2SPLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the L2SP Loss based on Explicit inductive bias for transfer learning with convolutional networks\n",
    "    Parameters\n",
    "    ----------\n",
    "        lambd : float\n",
    "        scalar for the loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_params,\n",
    "        target_params,\n",
    "        base_loss=MSELossFlat(),\n",
    "        lambd=0.1,\n",
    "    ):\n",
    "        super(L2SPLoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.lambd = lambd\n",
    "        self.source_params = source_params\n",
    "        self.target_params = target_params\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        y_hat : pytorch.Tensor\n",
    "            a tensor where in each sample the first model is the actual prediction, where the others are from a LinearTransferModel.\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting accumulated loss\n",
    "        \"\"\"\n",
    "\n",
    "        base_loss = self.base_loss(y, y_hat)\n",
    "\n",
    "        l2_loss = 0\n",
    "        for sp, tp in zip(\n",
    "            self.source_params.parameters(), self.target_params.parameters()\n",
    "        ):\n",
    "            # print(\"source\", sp)\n",
    "            # print(\"target\", tp)\n",
    "            l2_loss += self.base_loss(sp, tp)\n",
    "        # l2_loss = 0\n",
    "        loss = base_loss + self.lambd * l2_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = (\n",
    "            \"L2SPLoss(\\n  (base_loss):\"\n",
    "            + str(self.base_loss)\n",
    "            + f\"\\n  (lambd): {self.lambd} \\n)\"\n",
    "        )\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00a_utils.ipynb.\n",
      "Converted 00b_losses.ipynb.\n",
      "Converted 00c_utils_blitz.ipynb.\n",
      "Converted 00d_baselines.ipynb.\n",
      "Converted 00e_metrics.ipynb.\n",
      "Converted 00f_utils_pytorch.ipynb.\n",
      "Converted 01_tabular.core.ipynb.\n",
      "Converted 02_tabular.data.ipynb.\n",
      "Converted 03_tabular.model.ipynb.\n",
      "Converted 04_tabular.learner.ipynb.\n",
      "Converted 05_timeseries.core.ipynb.\n",
      "Converted 06_timeseries.data.ipynb.\n",
      "Converted 07_timeseries.model.ipynb.\n",
      "Converted 08_timeseries.learner.ipynb.\n",
      "Converted 09_gan.core.ipynb.\n",
      "Converted 10_gan.model.ipynb.\n",
      "Converted 11_gan.learner.ipynb.\n",
      "Converted 12_autoencoder_models.ipynb.\n",
      "Converted 13_probabilistic_models.ipynb.\n",
      "Converted 14_transfer_models.ipynb.\n",
      "Converted 15_ensemble_models.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
