{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# losses\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VILoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the Kullback-Leibler divergence loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dies.embedding\n",
    "        the given embedding to base the loss on\n",
    "    lambd : float\n",
    "        scalar for the loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        base_loss=torch.nn.MSELoss(),\n",
    "        kl_weight=0.1,\n",
    "        scale_log_likelihood=True\n",
    "    ):\n",
    "        super(VILoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.model = model\n",
    "        self.kl_weight = kl_weight\n",
    "        self.scale_log_likelihood=scale_log_likelihood\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        y_hat : pytorch.Tensor\n",
    "            a tensor with the same shape as 'y'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting accumulated loss\n",
    "        \"\"\"\n",
    "        base_loss = self.base_loss(y, y_hat)\n",
    "\n",
    "        n_samples = max(len(y), 1)\n",
    "\n",
    "        if self.scale_log_likelihood:\n",
    "            base_loss = base_loss * n_samples\n",
    "\n",
    "        kl = self.model.nn_kl_divergence()\n",
    "\n",
    "        loss = base_loss + self.kl_weight * kl\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"VILoss(\\n  (base_loss):\" + str(self.base_loss) + f\"\\n  (kl_weight): {self.kl_weight} \\n)\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Quantile_Score(torch.nn.Module):\n",
    "    \n",
    "    def reshape_1(self, x):\n",
    "        return x.view(x.size()[0],1)\n",
    "    \n",
    "    def __init__(self, taus=[0.25, 0.5, 0.75]):\n",
    "        super(Quantile_Score, self).__init__()\n",
    "        \n",
    "        self.taus =  torch.autograd.Variable(torch.tensor(taus, dtype=torch.float32), \n",
    "                                             requires_grad=False)\n",
    "        \n",
    "        self.taus = self.reshape_1(self.taus).t()\n",
    "        \n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Example:\n",
    "            y = np.array([0.2, 0.1, 0.3, 0.4])\n",
    "            tau=np.array([0.25,0.5,0.75])\n",
    "            for each sample we have one row \n",
    "            y_hat = np.array([[0, 0.2, 0.3], \n",
    "                          [0.05, 0.1, 0.35], \n",
    "                          [0.2, 0.3, 0.6],\n",
    "                          [0.3, 0.4, 0.45],])\n",
    "            res = array([0.125 , 0.2   , 0.25  , 0.0625])\n",
    "        \"\"\"\n",
    "        \n",
    "        y = self.reshape_1(y)\n",
    "        v = y - y_hat\n",
    "        \n",
    "        r = (torch.abs(v*(v>0).float()) * self.taus + \\\n",
    "             torch.abs(v*(v<0).float()) * (1-self.taus))\n",
    "\n",
    "        # this would calculate the loss for each sample\n",
    "        # r =  torch.sum(r,dim=1)\n",
    "        r =  torch.sum(r)\n",
    "        \n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = Quantile_Score(taus=[0.25, 0.5, 0.75]).to(\"cpu\")\n",
    "probabilistic_forecasts = torch.tensor([[1,2,3],[4,5,6],[7,8,9],])\n",
    "measurements = torch.tensor([2,5,8])\n",
    "loss(probabilistic_forecasts, measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CnnMSELoss(torch.nn.MSELoss):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Calculate the MSELoss and take the mean over all features\n",
    "        \"\"\"\n",
    "        super(CnnMSELoss, self).__init__(None, None, \"mean\")\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        target : pytorch.Tensor\n",
    "            a tensor with the same shape as 'input'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting loss\n",
    "        \"\"\"\n",
    "        return torch.mean(torch.mean(torch.mean(torch.pow((y - yhat), 2), 2), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VAEReconstructionLoss(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        \"\"\"\n",
    "        Calculate the sum of the Kullback–Leibler divergence loss and the loss of any given function\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : dies.autoencoder\n",
    "            model of the autoencoder for which the loss is to be calculated\n",
    "        \"\"\"\n",
    "        super(VAEReconstructionLoss, self).__init__()\n",
    "        self.reconstruction_function = torch.nn.MSELoss()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x_hat, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        x_hat : pytorch.Tensor\n",
    "            a tensor with the same shape as 'x'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting accumulated loss\n",
    "        \"\"\"\n",
    "        recon_x = x_hat\n",
    "\n",
    "        # how well do input x and output recon_x agree?\n",
    "\n",
    "        generation_loss = self.reconstruction_function(recon_x, x)\n",
    "        # KLD is Kullback–Leibler divergence -- how much does one learned\n",
    "        # distribution deviate from another, in this specific case the\n",
    "        # learned distribution from the unit Gaussian\n",
    "\n",
    "        if self.model.embedding_module is None:\n",
    "            mu, logvar = self.model.get_posteriors(x)\n",
    "            # see Appendix B from VAE paper:\n",
    "            # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "            # https://arxiv.org/abs/1312.6114\n",
    "            # - D_{KL} = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "            # note the negative D_{KL} in appendix B of the paper\n",
    "            KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            # Normalise by same number of elements as in reconstruction\n",
    "            KLD /= x.shape[0] * x.shape[1]\n",
    "\n",
    "            # BCE tries to make our reconstruction as accurate as possible\n",
    "            # KLD tries to push the distributions as close as possible to unit Gaussian\n",
    "        else:\n",
    "            KLD = 0\n",
    "        return generation_loss + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# class GaussianNegativeLogLikelihoodLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         Calculate the Residual sum of squares loss\n",
    "#         \"\"\"\n",
    "#         super(GaussianNegativeLogLikelihoodLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, y, mu_sigma):\n",
    "#         \"\"\"\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         y : pytorch.Tensor\n",
    "#             any given tensor. Shape: [n, ]\n",
    "#         mu : pytorch.Tensor\n",
    "#             a tensor with the same shape as 'y'\n",
    "#         sigma : pytorch.Tensor\n",
    "#             a tensor with the same shape as 'y'\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         pytorch.Tensor\n",
    "#             the resulting loss\n",
    "#         \"\"\"\n",
    "#         mu, sigma = mu_sigma[:,0], mu_sigma[:,1]\n",
    "#         nll =  torch.log(sigma)+(1/(2*sigma**2 ))*(y-mu)**2\n",
    "#         nll =  nll.mean()\n",
    "#         return nll\n",
    "\n",
    "#export\n",
    "class GaussianNegativeLogLikelihoodLoss(nn.Module):\n",
    "    def __init__(self, num_mini_batches_mse=5):\n",
    "        \"\"\"\n",
    "        Calculate the Residual sum of squares loss\n",
    "        \"\"\"\n",
    "        super(GaussianNegativeLogLikelihoodLoss, self).__init__()\n",
    "        self.loss = nn.GaussianNLLLoss(reduction=\"mean\")\n",
    "        self.num_mini_batches_mse = num_mini_batches_mse\n",
    "        self.counter=0\n",
    "\n",
    "    def forward(self, mu_sigma, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        mu : pytorch.Tensor\n",
    "            a tensor with the same shape as 'y'\n",
    "        sigma : pytorch.Tensor\n",
    "            a tensor with the same shape as 'y'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting loss\n",
    "        \"\"\"\n",
    "        y_pred, var_pred = mu_sigma[0].reshape(-1,1), mu_sigma[1].reshape(-1,1)\n",
    "        \n",
    "        _mse = (y - y_pred)**2\n",
    "        \n",
    "        N = y.shape[0]\n",
    "        \n",
    "        if self.counter % self.num_mini_batches_mse*2 < self.num_mini_batches_mse:\n",
    "            loss = 1 / (N * 2 * var_pred.exp()) + var_pred/2\n",
    "        else:\n",
    "            loss = _mse\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        self.counter += 1\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RSSLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Calculate the Residual sum of squares loss\n",
    "        \"\"\"\n",
    "        super(RSSLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pytorch.Tensor\n",
    "            any given tensor. Shape: [n, ]\n",
    "        y_hat : pytorch.Tensor\n",
    "            a tensor with the same shape as 'y'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pytorch.Tensor\n",
    "            the resulting loss\n",
    "        \"\"\"\n",
    "        return ((y - y_hat) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00a_utils.ipynb.\n",
      "Converted 00b_losses.ipynb.\n",
      "Converted 00c_utils_blitz.ipynb.\n",
      "Converted 00d_baselines.ipynb.\n",
      "Converted 00e_metrics.ipynb.\n",
      "Converted 01_tabular.core.ipynb.\n",
      "Converted 02_tabular.data.ipynb.\n",
      "Converted 03_tabular.model.ipynb.\n",
      "Converted 04_tabular.learner.ipynb.\n",
      "Converted 05_timeseries.core.ipynb.\n",
      "Converted 06_timeseries.data.ipynb.\n",
      "Converted 07_timeseries.model.ipynb.\n",
      "Converted 08_timeseries.learner.ipynb.\n",
      "Converted 09_gan.core.ipynb.\n",
      "Converted 10_autoencoder_models.ipynb.\n",
      "Converted 11_probabilistic_models.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
