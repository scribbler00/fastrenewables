{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: gan.learner.html\n",
    "title: gan.learner\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# default_exp gan.learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCELoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from fastai.basics import set_seed\n",
    "from fastrenewables.utils_pytorch import CATWrapper\n",
    "from fastrenewables.synthetic_data import *\n",
    "from fastrenewables.gan.model import *\n",
    "from fastrenewables.tabular.model import EmbeddingModule\n",
    "\n",
    "from fastrenewables.timeseries.core import *\n",
    "from fastrenewables.timeseries.data import *\n",
    "from fastrenewables.timeseries.model import *\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "#import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# export    \n",
    "class GAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, generator, discriminator, gen_optim, dis_optim, n_z=100,  \\\n",
    "                 auxiliary=False, auxiliary_weighting_factor=0.1, label_bias=0, label_noise=0):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.gen_optim = gen_optim\n",
    "        self.dis_optim = dis_optim\n",
    "        self.n_z = n_z\n",
    "        self.real_loss = []\n",
    "        self.fake_loss = []\n",
    "        self.aux_loss = []\n",
    "        self.auxiliary = auxiliary\n",
    "        self.bce_loss = BCELoss()\n",
    "        self.auxiliary_loss_function = CrossEntropyLoss()\n",
    "        self.auxiliary_weighting_factor=auxiliary_weighting_factor\n",
    "        self.label_bias = label_bias\n",
    "        self.label_noise = label_noise\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.to_device(self.device)\n",
    "        \n",
    "    def noise(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            z = torch.randn(x.shape[0], self.n_z).to(self.device)\n",
    "        elif len(x.shape) == 3:\n",
    "            z = torch.randn(x.shape[0], self.n_z, x.shape[2]).to(self.device)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return z\n",
    "    \n",
    "    def to_device(self, device):\n",
    "        self.device = device\n",
    "        self.generator = self.generator.to(device)\n",
    "        self.discriminator = self.discriminator.to(device)\n",
    "        self.bce_loss = self.bce_loss.to(device)\n",
    "        self.auxiliary_loss_function = self.auxiliary_loss_function.to(device)\n",
    "        \n",
    "    def _split_pred(self, y):\n",
    "        if self.auxiliary:\n",
    "            y, class_probs = y\n",
    "        else:\n",
    "            y, class_probs = y, None\n",
    "        return y, class_probs\n",
    "    \n",
    "    def auxiliary_loss(self, class_probs, y):\n",
    "        return self.auxiliary_loss_function(class_probs, y.long().squeeze())*self.auxiliary_weighting_factor\n",
    "    \n",
    "    def train_generator(self, x_cat, x_cont, y):\n",
    "        z = self.noise(x_cont)\n",
    "        self.generator.zero_grad()\n",
    "        x_cont_fake = self.generator(x_cat, z)\n",
    "        y_fake = self.discriminator(x_cat, x_cont_fake)  \n",
    "        y_fake, class_probs = self._split_pred(y_fake)\n",
    "        label = (1-self.label_bias)*torch.ones_like(y_fake) + self.label_noise*torch.randn(y_fake.shape).to(self.device)\n",
    "        label = label.clamp(0, 1)\n",
    "        loss = self.bce_loss(y_fake, label)\n",
    "        if self.auxiliary:\n",
    "            aux_loss = self.auxiliary_loss(class_probs, y)\n",
    "            loss = (loss + aux_loss)\n",
    "        loss.backward()\n",
    "        self.gen_optim.step()\n",
    "        return\n",
    "    \n",
    "    def train_discriminator(self, x_cat, x_cont, y):\n",
    "        z = self.noise(x_cont)\n",
    "        self.discriminator.zero_grad()\n",
    "        y_real = self.discriminator(x_cat, x_cont)\n",
    "        y_real, class_probs = self._split_pred(y_real)\n",
    "        label = (1-self.label_bias)*torch.ones_like(y_real) + self.label_noise*torch.randn(y_real.shape).to(self.device)\n",
    "        label = label.clamp(0, 1)\n",
    "        real_loss = self.bce_loss(y_real, label)\n",
    "        self.real_loss.append(real_loss.item())\n",
    "        if self.auxiliary:\n",
    "            aux_loss = self.auxiliary_loss(class_probs, y)\n",
    "            self.aux_loss.append(aux_loss.item())\n",
    "            real_loss = (real_loss + aux_loss)\n",
    "        \n",
    "        real_loss.backward()\n",
    "        self.dis_optim.step()\n",
    "        \n",
    "        \n",
    "        z = self.noise(x_cont)\n",
    "        self.discriminator.zero_grad()\n",
    "        x_cont_fake = self.generator(x_cat, z).detach()\n",
    "        y_fake = self.discriminator(x_cat, x_cont_fake)\n",
    "        y_fake, class_probs = self._split_pred(y_fake)\n",
    "        \n",
    "        label = (0+self.label_bias)*torch.ones_like(y_fake) + self.label_noise*torch.randn(y_fake.shape).to(self.device)\n",
    "        label = label.clamp(0, 1)\n",
    "        fake_loss =  self.bce_loss(y_fake, label)\n",
    "        self.fake_loss.append(fake_loss.item())\n",
    "        if self.auxiliary:\n",
    "            aux_loss = self.auxiliary_loss(class_probs, y)\n",
    "            fake_loss = (fake_loss + aux_loss)\n",
    "            \n",
    "        fake_loss.backward()\n",
    "        self.dis_optim.step()\n",
    "        return\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        z = self.noise(x_cont)\n",
    "        x_gen = self.generator(x_cat, z)\n",
    "        assert(x_gen.shape == x_cont.shape)\n",
    "        y = self.discriminator(x_cat, x_gen)\n",
    "        out = self._split_pred(y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class WGAN(GAN):\n",
    "    def __init__(self, generator, discriminator, gen_optim, dis_optim, n_z=100, clip=0.01, auxiliary=False):\n",
    "        super(WGAN, self).__init__(generator, discriminator, gen_optim, dis_optim, n_z, clip, auxiliary)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.gen_optim = gen_optim\n",
    "        self.dis_optim = dis_optim\n",
    "        self.n_z = n_z\n",
    "        self.clip = clip\n",
    "        self.auxiliary = auxiliary\n",
    "        self.real_loss = []\n",
    "        self.fake_loss = []\n",
    "        \n",
    "    def train_generator(self, x_cat, x_cont, y):\n",
    "        z = self.noise(x_cont)\n",
    "        self.generator.zero_grad()\n",
    "        x_cont_fake = self.generator(x_cat, z)\n",
    "        y_fake = self.discriminator(x_cat, x_cont_fake)\n",
    "        loss = - y_fake.mean()\n",
    "        loss.backward()\n",
    "        self.gen_optim.step()\n",
    "        return\n",
    "    \n",
    "    def train_discriminator(self, x_cat, x_cont, y):\n",
    "        z = self.noise(x_cont)\n",
    "        self.discriminator.zero_grad()\n",
    "        y_real = self.discriminator(x_cat, x_cont)\n",
    "        real_loss = - y_real.mean()\n",
    "        real_loss.backward()\n",
    "        self.dis_optim.step()\n",
    "        self.real_loss.append(real_loss.item())\n",
    "        \n",
    "        z = self.noise(x_cont)\n",
    "        self.discriminator.zero_grad()\n",
    "        x_cont_fake = self.generator(x_cat, z).detach()\n",
    "        y_fake = self.discriminator(x_cat, x_cont_fake)\n",
    "        fake_loss = y_fake.mean()\n",
    "        fake_loss.backward()\n",
    "        self.dis_optim.step()\n",
    "        self.fake_loss.append(fake_loss.item())\n",
    "        \n",
    "        for p in self.discriminator.parameters():\n",
    "            p = torch.clamp(p, -self.clip, self.clip)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def get_gan_model(\n",
    "    structure,\n",
    "    n_classes=2,\n",
    "    emb_module=None,\n",
    "    bn=True,\n",
    "    gan_type=\"bce\",\n",
    "    aux_factor=1,\n",
    "    label_noise=0,\n",
    "    label_bias=0,\n",
    "    model_type=\"MLP\",\n",
    "    len_ts=-1,\n",
    "):\n",
    "\n",
    "    if model_type.lower() == \"mlp\":\n",
    "        len_ts = 1\n",
    "    elif model_type.lower() == \"tcn\" and len_ts == -1 and gan_type.lower() == \"aux\":\n",
    "        raise AttributeError(\"Please provide timeseries length.\")\n",
    "\n",
    "    structure = structure.copy()\n",
    "    gen_structure = structure.copy()\n",
    "    structure.reverse()\n",
    "    dis_structure = structure\n",
    "    dis_structure[-1] = 1\n",
    "    n_z = gen_structure[0]\n",
    "\n",
    "    if gan_type == \"bce\" or gan_type == \"aux\":\n",
    "        final_act_dis = nn.Sigmoid\n",
    "        opt_fct = torch.optim.Adam\n",
    "        gan_class = GAN\n",
    "    elif gan_type == \"wgan\":\n",
    "        final_act_dis = None\n",
    "        opt_fct = torch.optim.RMSprop\n",
    "        gan_class = WGAN\n",
    "\n",
    "    if model_type.lower() == \"mlp\":\n",
    "        generator = GANMLP(\n",
    "            ann_structure=gen_structure,\n",
    "            act_fct=nn.ReLU,\n",
    "            final_act_fct=nn.Sigmoid,\n",
    "            embedding_module=emb_module,\n",
    "            bn_cont=bn,\n",
    "        )\n",
    "    elif model_type.lower() == \"tcn\":\n",
    "        generator = TemporalCNN(\n",
    "            cnn_structure=gen_structure,\n",
    "            act_func=nn.ReLU,\n",
    "            cnn_type=\"tcn\",\n",
    "            final_activation=nn.Sigmoid,\n",
    "            embedding_module=emb_module,\n",
    "            batch_norm_cont=bn,\n",
    "            # add_embedding_at_layer=[idx for idx in range(len(gen_structure) - 2)],\n",
    "            add_embedding_at_layer=[0],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    if gan_type == \"aux\":\n",
    "        auxiliary = True\n",
    "        dis_structure = dis_structure[:-1]\n",
    "        final_input_size = dis_structure[-1]\n",
    "\n",
    "        if model_type.lower() == \"mlp\":\n",
    "            discriminator = GANMLP(\n",
    "                ann_structure=dis_structure,\n",
    "                act_fct=nn.LeakyReLU,\n",
    "                final_act_fct=final_act_dis,\n",
    "                embedding_module=emb_module,\n",
    "                bn_cont=False,\n",
    "            )\n",
    "        elif model_type.lower() == \"tcn\":\n",
    "            discriminator = TemporalCNN(\n",
    "                cnn_structure=dis_structure,\n",
    "                act_func=nn.LeakyReLU,\n",
    "                cnn_type=\"tcn\",\n",
    "                final_activation=final_act_dis,\n",
    "                embedding_module=emb_module,\n",
    "                batch_norm_cont=False,\n",
    "                # add_embedding_at_layer=[idx for idx in range(len(dis_structure) - 2)],\n",
    "                add_embedding_at_layer=[0],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        discriminator = AuxiliaryDiscriminator(\n",
    "            basic_discriminator=discriminator,\n",
    "            n_classes=n_classes,\n",
    "            final_input_size=final_input_size,\n",
    "            len_ts=len_ts,\n",
    "        )\n",
    "    else:\n",
    "        auxiliary = False\n",
    "        if model_type.lower() == \"mlp\":\n",
    "            discriminator = GANMLP(\n",
    "                ann_structure=dis_structure,\n",
    "                act_fct=nn.LeakyReLU,\n",
    "                final_act_fct=final_act_dis,\n",
    "                embedding_module=emb_module,\n",
    "                bn_cont=False,\n",
    "            )\n",
    "        elif model_type.lower() == \"tcn\":\n",
    "            # output_layer = nn.Sequential(\n",
    "            #     nn.Linear(dis_structure[-2] * len_ts, 1), nn.Sigmoid()\n",
    "            # )\n",
    "            # output_layer = CATWrapper(output_layer)\n",
    "            discriminator = TemporalCNN(\n",
    "                cnn_structure=dis_structure,\n",
    "                act_func=nn.LeakyReLU,\n",
    "                cnn_type=\"tcn\",\n",
    "                # final_activation=nn.LeakyReLU,\n",
    "                embedding_module=emb_module,\n",
    "                batch_norm_cont=False,\n",
    "                input_sequence_length=len_ts,\n",
    "                final_activation=nn.Sigmoid,\n",
    "                # add_embedding_at_layer=[idx for idx in range(len(dis_structure) - 2)],\n",
    "                add_embedding_at_layer=[0],\n",
    "                # sequence_transform=output_layer,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    gen_opt = opt_fct(params=generator.parameters())\n",
    "    dis_opt = opt_fct(params=discriminator.parameters())\n",
    "    model = gan_class(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        gen_optim=gen_opt,\n",
    "        dis_optim=dis_opt,\n",
    "        n_z=n_z,\n",
    "        auxiliary=auxiliary,\n",
    "        auxiliary_weighting_factor=aux_factor,\n",
    "        label_noise=label_noise,\n",
    "        label_bias=label_bias,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class GANLearner():\n",
    "    def __init__(self, gan, n_gen=1, n_dis=1):\n",
    "        super(GANLearner, self).__init__()\n",
    "        # gan should contain a class which itself contains a generator and discriminator/critic class and combines them\n",
    "        self.gan = gan\n",
    "        self.n_gen = n_gen\n",
    "        self.n_dis = n_dis\n",
    "   \n",
    "    def generate_samples(self, x_cat, x_cont):\n",
    "        with torch.no_grad():\n",
    "            z = self.gan.noise(x_cont)\n",
    "            fake_samples = self.gan.generator(x_cat, z)\n",
    "        return fake_samples\n",
    "    \n",
    "    def fit(self, dl, epochs=10, lr=1e-3, plot_epochs=10, save_model=False, save_dir='models/', save_file='tmp', figsize=(16, 9)):\n",
    "        \n",
    "        self.gan.to_device(torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "        self.gan.gen_optim.param_groups[0]['lr'] = lr\n",
    "        self.gan.dis_optim.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        self.gan.train()\n",
    "        \n",
    "        for e in tqdm(range(epochs)):\n",
    "            for x_cat, x_cont, y in dl:\n",
    "                x_cat = x_cat.to(self.gan.device).long()\n",
    "                x_cont = x_cont.to(self.gan.device)\n",
    "                y = y.to(self.gan.device)\n",
    "                                \n",
    "                for _ in range(self.n_dis):\n",
    "                    self.gan.train_discriminator(x_cat, x_cont, y)\n",
    "\n",
    "                for _ in range(self.n_gen):\n",
    "                    self.gan.train_generator(x_cat, x_cont, y)\n",
    "                \n",
    "            #if (e+1)%plot_epochs==0:\n",
    "                #plt.figure(figsize=figsize)\n",
    "                #plt.plot(self.gan.real_loss, label='Real Loss')\n",
    "                #plt.plot(self.gan.fake_loss, label='Fake Loss')\n",
    "                #if len(self.gan.aux_loss) > 0:\n",
    "                #    plt.plot(self.gan.aux_loss, label='Aux Loss')\n",
    "                #plt.legend()\n",
    "                #plt.show()\n",
    "                \n",
    "                #fig, ax1 = plt.subplots(figsize=figsize)\n",
    "                #ax1.set_xlabel('iterations')\n",
    "                #ax1.set_ylabel('bce loss')\n",
    "                #ax1.plot(self.gan.real_loss, label='real', color='red')\n",
    "                #ax1.plot(self.gan.fake_loss, label='fake', color='blue') \n",
    "                #ax1.tick_params(axis='y')\n",
    "                #ax1.legend(loc='upper right')\n",
    "                #\n",
    "                #ax2 = ax1.twinx()\n",
    "                #ax2.set_ylabel('aux loss')\n",
    "                #ax2.plot(self.gan.aux_loss, label='aux', color='green')\n",
    "                #ax2.tick_params(axis='y')\n",
    "                #ax2.legend(loc='lower right')\n",
    "                #\n",
    "                #fig.tight_layout()\n",
    "                #plt.show()\n",
    "        \n",
    "        self.gan.eval()\n",
    "        \n",
    "        if save_model:\n",
    "            self.gan.to('cpu')\n",
    "            Path(save_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            torch.save(self.gan.to('cpu').state_dict(), save_dir+save_file+'.pt')\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "n_samples = 2**12\n",
    "n_classes = 4\n",
    "n_features = 1\n",
    "batch_size = 2**9\n",
    "n_z = 10\n",
    "n_in = n_features\n",
    "n_hidden = 256\n",
    "epochs = 1\n",
    "lr = 5e-5\n",
    "n_gen = 1\n",
    "n_dis = 1\n",
    "\n",
    "data = GaussianDataset(n_samples, n_classes)\n",
    "train_data, test_data = torch.utils.data.random_split(data, [int(len(data)*3/4), int(len(data)*1/4)])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=len(test_data), shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def evaluate_gan(gan_type='bce', aux_factor=1, epochs=2):\n",
    "    print(gan_type, aux_factor)\n",
    "    set_seed(1337)\n",
    "    emb_module = EmbeddingModule(categorical_dimensions=[n_classes+1]).to(\"cpu\")\n",
    "    model = get_gan_model(structure=[n_z, n_hidden, n_hidden, n_in], n_classes=n_classes, emb_module=emb_module, gan_type=gan_type, aux_factor=aux_factor, label_noise=0.1, label_bias=0.25)\n",
    "    model.to_device(\"cpu\")\n",
    "    learner = GANLearner(gan=model, n_gen=n_gen, n_dis=n_dis)\n",
    "    \n",
    "    learner.fit(train_dl, epochs=epochs, lr=lr, plot_epochs=epochs, save_model=True)\n",
    "    for x_cat, x_cont, y in test_dl:\n",
    "        x_cat = x_cat.long().to(\"cpu\")\n",
    "        print('distribution of real data:')\n",
    "        d_real = fit_kde(x_cont, bandwidth=1/25, show_plot=True)\n",
    "        x_fake = learner.generate_samples(x_cat.to(\"cpu\"), x_cont.to(\"cpu\"))\n",
    "        print('distribution of generated data:')\n",
    "        d_fake = fit_kde(x_fake, bandwidth=1/25, show_plot=True)\n",
    "        break\n",
    "    kld = calculate_kld(d_real, d_fake)\n",
    "        \n",
    "    return kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "# evaluate_gan('bce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "# evaluate_gan('aux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#epochs = 100\n",
    "#\n",
    "#bce_kld = evaluate_gan('bce', 1, epochs)\n",
    "#\n",
    "#afs = [4, 2, 1, 1/2, 1/4]\n",
    "#klds = [evaluate_gan('aux', af, epochs) for af in afs]\n",
    "#\n",
    "#afs.append(0)\n",
    "#klds.append(bce_kld)\n",
    "#\n",
    "#data = {'aux_factor': afs,\n",
    "#       'kld': klds}\n",
    "#\n",
    "#df = pd.DataFrame(data)\n",
    "#df.to_csv('klds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('klds.csv')\n",
    "#df\n",
    "#\n",
    "#afs = df['aux_factor'].values\n",
    "#klds = df['kld'].values\n",
    "#\n",
    "#plt.figure(figsize=(16, 9))\n",
    "#plt.plot(afs[:-1], klds[:-1], 'o--', label='auxiliary')\n",
    "#plt.axhline(klds[-1], color='red', linestyle='--', label='conditional')\n",
    "#plt.legend()\n",
    "#plt.xscale('log',base=2) \n",
    "#plt.xticks(afs[:-1])\n",
    "#plt.xlabel('auxiliary weighting factor')\n",
    "#plt.ylabel('KLD')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
