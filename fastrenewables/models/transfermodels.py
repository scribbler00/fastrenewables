# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14_transfer_models.ipynb (unless otherwise specified).

__all__ = ['LinearTransferModel']

# Cell
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import numpy as np
import torch
from torch import nn
from ..tabular.model import *
from ..timeseries.model import *
from fastai.tabular.all import *
from torch.autograd import Variable
from sklearn.datasets import make_regression
from fastai.learner import *
from ..utils_pytorch import *
import copy


# Cell
# export
def _create_matrices(n_features, alpha):
    w_mean = torch.zeros(n_features)
    w_precision = torch.eye(n_features) / alpha

    return w_mean, w_precision
# minimal check if we can create the matricies
mean, precision = _create_matrices(10, 10)
test_eq(0, mean.sum())
test_eq(0.1, precision[0,0])

# Cell
class LinearTransferModel(nn.Module):
    def __init__(self, source_model, num_layers_to_remove=1,
                 name_layers="layers", use_original_weights=True,
                alpha=1, beta=1):
        super().__init__()
        self.are_weights_initialized = False
        self.alpha, self.beta = alpha, beta
        self.num_layers_to_remove = num_layers_to_remove
        self.ts_length = 1

        layers = getattrs(source_model, name_layers, default=None)[0]
        if layers is None:
            raise ValueError(f"Could not find layers by given name {name_layers}.")

        self.source_model = copy.deepcopy(source_model)


        if not isinstance(layers, torch.nn.modules.container.Sequential):
            raise ValueError(f"Only sequential layers are supported.")

        if num_layers_to_remove != 1 and use_original_weights:
            raise ValueError("Can only reuse weights when using the last layers due to the dimension.")
        elif num_layers_to_remove == 1 and use_original_weights:

            for element in layers[-1]:
                if isinstance(element, nn.Linear):
                    # create mean matrix including bias
                    self.w_mean = copy.copy(element.weight.data)
                    bias = copy.copy(element.bias.data)
                    self.w_mean = self.w_mean.reshape(self.w_mean.shape[1])
                    self.w_mean = torch.cat([bias, self.w_mean])

                    # create precision and variance matrix
                    self.n_features = self.w_mean.shape[0]
                    _, self.w_precision = _create_matrices(self.n_features, self.alpha)
                    self.w_covariance = torch.linalg.inv(self.w_precision)

                    self.are_weights_initialized = True

            if not self.are_weights_initialized:
                raise ValueError(f"Could not find linear layer in last layer {self.layers[-1]}")

        layers = layers[0:-self.num_layers_to_remove]
        setattr(self.source_model, name_layers, layers)
        freeze(self.source_model)

    def transform(self, cats, conts):
        n_samples = conts.shape[0]

        x_transformed =  self.source_model(cats, conts)

        # flatten in case of timeseries model
        x_transformed = x_transformed.reshape(n_samples, -1)

        # add feature for bias
        x_transformed = torch.cat([ torch.ones(n_samples).reshape(n_samples,1), x_transformed], axis=1)

        return x_transformed

    def forward(self, cats, conts):
        n_samples = conts.shape[0]
        self.ts_length = 1
        if len(conts.shape) == 3:
            self.ts_length = conts.shape[2]

        x_transformed = self.transform(cats, conts)

        if not self.are_weights_initialized:
            self.n_features = x_transformed.shape[1]*self.ts_length

            self.w_mean, self.w_precision = _create_matrices(self.n_features, self.alpha)
            self.w_covariance = torch.linalg.inv(self.w_precision)
            self.are_weights_initialized=True

        if self.training:
            return x_transformed
        else:
            preds = self._predict(x_transformed)[0]

            # TODO: where does it make sense to reshape in the timeseries representation
#             if self.ts_length != 1:
#                 preds = preds.reshape(n_samples, 1, self.ts_length)

            return preds

    def _predict(self, X):
        # calcualte the predictive mean (Bishop eq. 3.58)
        y_pred_mean = X @ self.w_mean

        # calculate the predictive variance (Bishop eq. 3.59)
        y_pred_var = 1 / self.beta + (X @ self.w_covariance * X).sum(axis=1)

        # Drop a dimension from the mean and variance in case x and y were singletons
        y_pred_mean = torch.squeeze(y_pred_mean)
        y_pred_var = torch.squeeze(y_pred_var)

        return y_pred_mean, y_pred_var ** 0.5

    def update(self, X, y):
        """Update mean and precision. X needs to be the output of the original source model."""

        w_precision = self.w_precision + self.beta * X.T @ X

        w_covariance = torch.linalg.inv(w_precision)
        w_mean = w_covariance @ (self.w_precision @ self.w_mean + self.beta * y @ X)

        self.w_precision = w_precision
        self.w_covariance = torch.linalg.inv(w_precision)

        self.w_mean = w_mean

        return self


    def predict(self, cats, conts):
        x_transformed = self.transform(cats, conts)
        y_pred_mean, _ = self._predict(x_transformed)

        return y_pred_mean

    def predict_proba(self, cats, conts):
        x_transformed = self.transform(cats, conts)
        y_pred_mean, y_pred_std = self._predict(x_transformed)

        return y_pred_mean, y_pred_std

    def loss_func(x_transformed, ys):

        self.update(x_transformed, ys)

        return 0
