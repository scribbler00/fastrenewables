# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14_transfer_models.ipynb (unless otherwise specified).

__all__ = ['LinearTransferModel', 'reduce_layers_tcn_model']

# Cell
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import numpy as np
import torch
from torch import nn
from ..tabular.model import *
from ..timeseries.model import *
from fastai.tabular.all import *
from torch.autograd import Variable
from sklearn.datasets import make_regression
from fastai.learner import *
from ..utils_pytorch import *
import copy


# Cell
# export
def _create_matrices(n_features, alpha):
    w_mean = torch.zeros(n_features)
    w_precision = torch.eye(n_features) / alpha

    return w_mean, w_precision
# minimal check if we can create the matricies
mean, precision = _create_matrices(10, 10)
test_eq(0, mean.sum())
test_eq(0.1, precision[0,0])

# Cell
class LinearTransferModel(nn.Module):
    def __init__(self, source_model, num_layers_to_remove=1,
                 name_layers_or_function_to_remove="layers", use_original_weights=True,
                alpha=1, beta=1, include_original_features=False):
        super().__init__()
        self.are_weights_initialized = False
        self.alpha, self.beta = alpha, beta
        self.num_layers_to_remove = num_layers_to_remove
        self.ts_length = 1
        self.source_model = copy.deepcopy(source_model)


        if callable(name_layers_or_function_to_remove):
            name_layers_or_function_to_remove(self.source_model, num_layers_to_remove)
        elif type(name_layers_or_function_to_remove) == str:
            layers = getattrs(self.source_model, name_layers_or_function_to_remove, default=None)[0]
            if layers is None:
                raise ValueError(f"Could not find layers by given name {name_layers_or_function_to_remove}.")
            elif isinstance(layers, torch.nn.modules.container.Sequential):
                setattr(self.source_model, name_layers_or_function_to_remove, layers[0:-self.num_layers_to_remove])
            else:
                raise ValueError(f"Only sequential layers are supported.")
        else:
            ValueError("Unknown type for name_layers_or_function_to_remove")

        if num_layers_to_remove != 1 and use_original_weights:
            raise ValueError("Can only reuse weights when using the last layers due to the dimension.")
        elif num_layers_to_remove == 1 and use_original_weights:

            for element in layers[-1]:
                if isinstance(element, nn.Linear):
                    # create mean matrix including bias
                    self.w_mean = copy.copy(element.weight.data)
                    bias = copy.copy(element.bias.data)
                    self.w_mean = self.w_mean.reshape(self.w_mean.shape[1])
                    self.w_mean = torch.cat([bias, self.w_mean])

                    # create precision and variance matrix
                    self.n_features = self.w_mean.shape[0]
                    _, self.w_precision = _create_matrices(self.n_features, self.alpha)
                    self.w_covariance = torch.linalg.inv(self.w_precision)

                    self.are_weights_initialized = True

            if not self.are_weights_initialized:
                raise ValueError(f"Could not find linear layer in last layer {self.layers[-1]}")


        freeze(self.source_model)
        self.include_original_features = include_original_features

        # fake param so that it can be used with pytorch trainers
        self.fake_param=nn.Parameter(torch.zeros((1,1), dtype=torch.float))
        self.fake_param.requires_grad =True

    def transform(self, cats, conts):
        n_samples = conts.shape[0]

        x_transformed =  self.source_model(cats, conts)

        # flatten and update n_samples in case of timeseries model
        x_transformed = x_transformed.reshape(n_samples*self.ts_length, -1)
        n_samples = x_transformed.shape[0]

        if self.include_original_features:
            x_transformed = torch.cat([ conts.reshape(n_samples,-1), x_transformed], axis=1)

        # add feature for bias
        x_transformed = torch.cat([ torch.ones(n_samples).reshape(n_samples,1), x_transformed], axis=1)

        return x_transformed

    def forward(self, cats, conts):
        n_samples = conts.shape[0]
        self.ts_length = 1
        if len(conts.shape) == 3:
            self.ts_length = conts.shape[2]

        x_transformed = self.transform(cats, conts)

        if not self.are_weights_initialized:
            self.n_features = x_transformed.shape[1]#*self.ts_length

            self.w_mean, self.w_precision = _create_matrices(self.n_features, self.alpha)
            self.w_covariance = torch.linalg.inv(self.w_precision)
            self.are_weights_initialized=True

        if self.training:
            return x_transformed
        else:
            preds = self._predict(x_transformed)[0]

            return preds

    def _predict(self, X):
        # calcualte the predictive mean (Bishop eq. 3.58)
        y_pred_mean = X @ self.w_mean

        # calculate the predictive variance (Bishop eq. 3.59)
        y_pred_var = 1 / self.beta + (X @ self.w_covariance * X).sum(axis=1)

        # Drop a dimension from the mean and variance in case x and y were singletons
        y_pred_mean = torch.squeeze(y_pred_mean)
        y_pred_var = torch.squeeze(y_pred_var)

        return y_pred_mean, y_pred_var ** 0.5

    def update(self, X, y):
        """Update mean and precision. X needs to be the output of the original source model."""

        w_precision = self.w_precision + self.beta * X.T @ X

        w_covariance = torch.linalg.inv(w_precision)
        w_mean = w_covariance @ (self.w_precision @ self.w_mean + self.beta * y @ X)

        self.w_precision = w_precision
        self.w_covariance = torch.linalg.inv(w_precision)

        self.w_mean = w_mean

        return self


    def predict(self, cats, conts):
        x_transformed = self.transform(cats, conts)
        y_pred_mean, _ = self._predict(x_transformed)

        return y_pred_mean

    def predict_proba(self, cats, conts):
        x_transformed = self.transform(cats, conts)
        y_pred_mean, y_pred_std = self._predict(x_transformed)

        return y_pred_mean, y_pred_std

    def loss_func(self, x_transformed, ys):

        _tmp = self.update(x_transformed, ys.ravel())

        fake_loss = torch.tensor([0], dtype=torch.float)
        fake_loss.requires_grad=True
        return self.fake_param + fake_loss


    def _log_prior(self, w):
        return -0.5 * self.alpha * torch.sum(w ** 2)

    def _log_likelihood(self, X, y, w):
        return -0.5 * self.beta * torch.square(y - X @ w).sum()


    def _log_posterior(self, X, y, w):
        return self._log_likelihood(X, y, w) + self._log_prior(w)

    def log_evidence(self, X, y):
        X, y = self._check_and_prep(X, y)

        N, M = X.shape

        # E(\mathbf{m}_n) = \beta/2 \cdot ||y- X \mathbf{m}_n|| + \alpha/2 \mathbf{m}_n^T \mathbf{m}_n,
        # where \mathbf{m}_n is the mean weight. This is the same as the negative of the posterior
        Emn = -self._log_posterior(X, y, self.w_mean)

        # Bishop eq. 3.86
        return 0.5 * (M * np.log(self.alpha) + N * np.log(self.beta)
            - np.linalg.slogdet(self.w_precision)[1] - N * np.log(2 * np.pi)
        ) - Emn


# Cell
def reduce_layers_tcn_model(tcn_model, num_layers=1):
    tcn_model.layers.temporal_blocks = tcn_model.layers.temporal_blocks[:-num_layers]