# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14_transfer_models.ipynb (unless otherwise specified).

__all__ = ['LinearTransferModel', 'reduce_layers_tcn_model']

# Cell
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import numpy as np
import torch
from torch import nn
from ..tabular.model import *
from ..timeseries.model import *
from fastai.tabular.all import *
from torch.autograd import Variable
from sklearn.datasets import make_regression
from fastai.learner import *
from ..utils_pytorch import *
import copy
from ..timeseries.model import *
from ..baselines import BayesLinReg

# Cell
# export
def _create_matrices(n_features, alpha):
    w_mean = torch.zeros(n_features)
    w_precision = torch.eye(n_features) / alpha

    return w_mean, w_precision
# minimal check if we can create the matricies
mean, precision = _create_matrices(10, 10)
test_eq(0, mean.sum())
test_eq(0.1, precision[0,0])

# Cell
class LinearTransferModel(nn.Module):
    def __init__(self, source_model, num_layers_to_remove=1,
                 name_layers_or_function_to_remove="layers",
                 use_original_weights=True,
                 prediction_model=BayesLinReg(alpha=1, beta=1, empirical_bayes=False),
                include_original_features=False):
        super().__init__()
        self.are_weights_initialized = False
        self.num_layers_to_remove = num_layers_to_remove
        self.ts_length = 1
        self.source_model = copy.deepcopy(source_model)
        self._prediction_model = prediction_model
        self.prediction_models = []

        if callable(name_layers_or_function_to_remove):
            name_layers_or_function_to_remove(self.source_model, num_layers_to_remove)
        elif type(name_layers_or_function_to_remove) == str:
            layers = getattrs(self.source_model, name_layers_or_function_to_remove, default=None)[0]
            if layers is None:
                raise ValueError(f"Could not find layers by given name {name_layers_or_function_to_remove}.")
            elif isinstance(layers, torch.nn.modules.container.Sequential):
                setattr(self.source_model, name_layers_or_function_to_remove, layers[0:-self.num_layers_to_remove])
            else:
                raise ValueError(f"Only sequential layers are supported.")
        else:
            ValueError("Unknown type for name_layers_or_function_to_remove")

        if num_layers_to_remove != 1 and use_original_weights:
            raise ValueError("Can only reuse weights when using the last layers due to the dimension.")
        elif num_layers_to_remove == 1 and use_original_weights:

            for element in layers[-1]:
                if isinstance(element, nn.Linear):
                    # create mean matrix including bias
                    w_mean = copy.copy(element.weight.data)
                    bias = copy.copy(element.bias.data)
                    w_mean = w_mean.reshape(w_mean.shape[1])
                    w_mean = torch.cat([bias, w_mean])

                    # create precision and variance matrix
                    self.n_features = w_mean.shape[0]
                    _, w_precision = _create_matrices(self.n_features, self.prediction_model.alpha)
                    w_covariance = torch.linalg.inv(w_precision)

                    w_mean= to_np(w_mean)
                    w_covariance= to_np(w_covariance)
                    w_precision= to_np(w_precision)

                    self.prediction_model.w_mean = w_mean
                    self.prediction_model.w_covariance = w_covariance
                    self.prediction_model.w_precision = w_precision

                    self.are_weights_initialized = True

            if not self.are_weights_initialized:
                raise ValueError(f"Could not find linear layer in last layer {self.layers[-1]}")


        freeze(self.source_model)
        self.include_original_features = include_original_features

        # fake param so that it can be used with pytorch trainers
        self.fake_param=nn.Parameter(torch.zeros((1,1), dtype=torch.float))
        self.fake_param.requires_grad =True
    def _create_single_model(n_features):
        model = BayesLinReg(self._prediction_model.alpha, self._prediction_model.beta)
        model.n_features=n_features
        return model


    @property
    def alpha(self):
        return self._prediction_model.alpha

    @property
    def beta(self):
        return self._prediction_model.beta

    @alpha.setter
    def alpha(self, alpha):
        self.prediction_model.alpha = alpha

    @beta.setter
    def beta(self, beta):
        self.prediction_model.beta = beta

    def transform(self, cats, conts):
        n_samples = conts.shape[0]

        x_transformed =  self.source_model(cats, conts)

        # flatten and update n_samples in case of timeseries model
        x_transformed = x_transformed.reshape(n_samples*self.ts_length, -1)
        n_samples = x_transformed.shape[0]

        if self.include_original_features:
            x_transformed = torch.cat([ conts.reshape(n_samples,-1), x_transformed], axis=1)

        # add feature for bias
#         x_transformed = torch.cat([  x_transformed], axis=1)

        return x_transformed

    def forward(self, cats, conts):
        n_samples = conts.shape[0]
        self.ts_length = 1
        if len(conts.shape) == 3:
            self.ts_length = conts.shape[2]

        x_transformed = self.transform(cats, conts)

        if not self.are_weights_initialized:
            self.n_features = x_transformed.shape[1]+1#*self.ts_length
            self.prediction_model._create_matrices(to_np(x_transformed))
#             w_mean, w_precision = _create_matrices(self.n_features, self.alpha)
#             w_covariance = torch.linalg.inv(w_precision)

#             self.prediction_model.w_mean = to_np(w_mean)
#             self.prediction_model.w_covariance = to_np(w_covariance)
#             self.prediction_model.w_precision = to_np(w_precision)

            self.are_weights_initialized=True

        if self.training:
            return x_transformed
        else:
            preds = self.predict(cats, conts)

            return preds



    def update(self, X, y):
        X = to_np(X)
        y = to_np(y)

        self.prediction_model.update(X,y)

        return self

    def predict(self, cats, conts):
        x_transformed = self.transform(cats, conts)
        x_transformed = to_np(x_transformed)

        y_pred_mean = self.prediction_model.predict(x_transformed)

        return torch.tensor(y_pred_mean)

    def predict_proba(self, cats, conts):
        x_transformed = self.transform(cats, conts)
        x_transformed = to_np(x_transformed)

        y_pred_mean, y_pred_std = self.prediction_model.predict_proba(x_transformed)

        return torch.tensor(y_pred_mean), torch.tensor(y_pred_std)

    def loss_func(self, x_transformed, ys):

        _tmp = self.update(x_transformed, ys.ravel())

        fake_loss = torch.tensor([0], dtype=torch.float)
        fake_loss.requires_grad=True
        return self.fake_param + fake_loss



# Cell
def reduce_layers_tcn_model(tcn_model, num_layers=1):
    tcn_model.layers.temporal_blocks = tcn_model.layers.temporal_blocks[:-num_layers]