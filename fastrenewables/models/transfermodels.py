# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14_transfer_models.ipynb (unless otherwise specified).

__all__ = ['LinearTransferModel', 'reduce_layers_tcn_model']

# Cell
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import numpy as np
import torch
from torch import nn
from ..tabular.model import *
from ..timeseries.model import *
from fastai.tabular.all import *
from torch.autograd import Variable
from sklearn.datasets import make_regression
from fastai.learner import *
from ..utils_pytorch import *
import copy
from ..timeseries.model import *
from ..baselines import BayesLinReg
from ..tabular.learner import convert_to_tensor

# Cell
# export
def _create_matrices(n_features, alpha):
    w_mean = torch.zeros(n_features)
    w_precision = torch.eye(n_features) / alpha

    return w_mean, w_precision
# minimal check if we can create the matricies
mean, precision = _create_matrices(10, 10)
test_eq(0, mean.sum())
test_eq(0.1, precision[0,0])

# Cell
class LinearTransferModel(nn.Module):
    def __init__(self, source_model, num_layers_to_remove=1,
                 name_layers_or_function_to_remove="layers",
                 use_original_weights=True,
                 prediction_model=BayesLinReg(alpha=1, beta=1, empirical_bayes=False)):
        super().__init__()
        self.are_weights_initialized = False
        self.num_layers_to_remove = num_layers_to_remove
        self.ts_length = 1
        self.source_model = copy.deepcopy(source_model)
        self._prediction_model = prediction_model
        self.prediction_models = []
        if use_original_weights:
            self._prediction_model.empirical_bayes=False

        if callable(name_layers_or_function_to_remove):
            name_layers_or_function_to_remove(self.source_model, num_layers_to_remove)
        elif type(name_layers_or_function_to_remove) == str:
            layers = getattrs(self.source_model, name_layers_or_function_to_remove, default=None)[0]
            if layers is None:
                raise ValueError(f"Could not find layers by given name {name_layers_or_function_to_remove}.")
            elif isinstance(layers, torch.nn.modules.container.Sequential):
                setattr(self.source_model, name_layers_or_function_to_remove, layers[0:-self.num_layers_to_remove])
            else:
                raise ValueError(f"Only sequential layers are supported.")
        else:
            ValueError("Unknown type for name_layers_or_function_to_remove")

        if num_layers_to_remove != 1 and use_original_weights:
            raise ValueError("Can only reuse weights when using the last layers due to the dimension.")
        elif num_layers_to_remove == 1 and use_original_weights:

            for element in layers[-1]:
                if isinstance(element, nn.Linear):
                    # create mean matrix including bias
                    w_mean = copy.copy(element.weight.data)
                    bias = copy.copy(element.bias.data)
                    w_mean = w_mean.reshape(w_mean.shape[1])
                    w_mean = to_np(torch.cat([bias, w_mean]))

                    # create precision and variance matrix
                    self.n_features = w_mean.shape[0]

                    model = self._create_single_model(self.n_features)
                    model.w_mean = w_mean
                    self.prediction_models.append(model)

                    self.are_weights_initialized = True

            if not self.are_weights_initialized:
                raise ValueError(f"Could not find linear layer in last layer {self.layers[-1]}")


        freeze(self.source_model)

        # fake param so that it can be used with pytorch trainers
        self.fake_param=nn.Parameter(torch.zeros((1,1), dtype=torch.float))
        self.fake_param.requires_grad =True

    def _create_single_model(self,n_features):
        model = copy.copy(self._prediction_model)
        model._create_matrices(np.ones(n_features).reshape(1, n_features))
        model.w_covariance = np.linalg.inv(model.w_precision)
        return model


    @property
    def alpha(self):
        return self._prediction_model.alpha

    @property
    def beta(self):
        return self._prediction_model.beta

    @alpha.setter
    def alpha(self, alpha):
        self._prediction_model.alpha = alpha

    @beta.setter
    def beta(self, beta):
        self._prediction_model.beta = beta

    def correct_shape(self, x):
        n_samples = x.shape[0]
        return x.reshape(n_samples, -1)

    def transform(self, cats, conts, as_np=False):
        x_transformed =  self.source_model(cats, conts)

        x_transformed = self.correct_shape(x_transformed)

        if as_np: return to_np(x_transformed)
        else: return x_transformed

    def forward(self, cats, conts):
        n_samples = conts.shape[0]
        self.ts_length = 1
        if len(conts.shape) == 3:
            self.ts_length = conts.shape[2]

        x_transformed = self.transform(cats, conts)

        if not self.are_weights_initialized:
            self.n_features = x_transformed.shape[1]+1

            for idx in range(self.ts_length):
                model = self._create_single_model(self.n_features)
                self.prediction_models.append(model)

            self.are_weights_initialized=True

        if self.training:
            return x_transformed
        else:
            preds = self.pred_transformed_X(x_transformed)

            return preds

    def update(self, X, y):
        X = to_np(X)
        y = to_np(y)
        y = self.correct_shape(y)

        for idx, prediction_model in enumerate(self.prediction_models):
            prediction_model.fit(X, y[:,idx].ravel())

        return self

    def predict(self, cats, conts):
        x_transformed = self.transform(cats, conts, as_np=True)
        return self.pred_transformed_X(x_transformed)

    def predict_proba(self, cats, conts):
        x_transformed = self.transform(cats, conts, as_np=True)
        return self.pred_transformed_X(x_transformed, include_std=True)

    def pred_transformed_X(self, x_transformed, include_std=False):
        y_pred_means = np.zeros((len(x_transformed), len(self.prediction_models)))
        y_pred_stds = np.zeros((len(x_transformed), len(self.prediction_models)))

        for idx, prediction_model in enumerate(self.prediction_models):
            y_pred_mean, y_pred_std = prediction_model.predict_proba(x_transformed)
            y_pred_means[:,idx] = y_pred_mean
            y_pred_stds[:,idx] = y_pred_std
        if include_std:
            return torch.tensor(y_pred_means, dtype=torch.float32), torch.tensor(y_pred_stds, dtype=torch.float32),
        else:
            return torch.tensor(y_pred_means, dtype=torch.float32)

    def loss_func(self, x_transformed, ys):
        ys = self.correct_shape(ys)
        if self.training:
            self.update(x_transformed, ys)

            fake_loss = torch.tensor([0], dtype=torch.float)
            fake_loss.requires_grad=True
            return self.fake_param + fake_loss
        else:
            # in case of validation return MSE
            return ((x_transformed-ys)**2).mean()

    def log_posterior(self, cats, conts, ys):
        ys = to_np(self.correct_shape(ys))

        x_transformed = self.transform(cats, conts, as_np=True)

        posteriors = np.zeros((len(self.prediction_models),1))
        for idx, pred_model in enumerate(self.prediction_models):
            log_posterior = pred_model.log_posterior(x_transformed, ys[:,idx].ravel())
            posteriors[idx] = log_posterior
        return posteriors

    def log_evidence(self, cats, conts, ys, logme=False):
        evidences = []
        ys = to_np(self.correct_shape(ys))

        x_transformed = self.transform(cats, conts, as_np=True)
        for idx, pred_model in enumerate(self.prediction_models):
            ev = pred_model.log_evidence(x_transformed, ys[:,idx].ravel())
            evidences.append(ev)

        evidences = np.array(evidences, dtype=np.float)

        if logme:
            evidences = evidences / len(conts)

        return evidences.mean()

# Cell
def reduce_layers_tcn_model(tcn_model, num_layers=0):
    tcn_model.layers.temporal_blocks = tcn_model.layers.temporal_blocks[:-num_layers]