# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/12_autoencoder_models.ipynb (unless otherwise specified).

__all__ = ['Autoencoder', 'AutoencoderForecast', 'UnFlatten', 'VariationalAutoencoder']

# Cell
import numpy as np
import torch
from torch import nn
from ..tabular.model import *
from ..timeseries.model import *
from fastai.tabular.all import *
from torch.autograd import Variable
from sklearn.datasets import make_regression
from fastai.learner import *
from ..utils_pytorch import *
from ..losses import VAEReconstructionLoss
from blitz.utils import variational_estimator
from ..utils_blitz import set_train_mode

# Cell
@variational_estimator
class Autoencoder(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()

        self.encoder = encoder

        self.decoder = decoder

    def encode(self, categorical_data, continuous_data, as_np=False):
        z = self.encoder(categorical_data, continuous_data)

        if as_np: return to_np(z)
        else: return z


    def decode(self, categorical_data, continuous_data, as_np=False):
        x = self.decoder(categorical_data, continuous_data)

        if as_np: return to_np(x)
        else: return x

    def forward(self, categorical_data, continuous_data):
        x = self.encode(categorical_data, continuous_data)
        x = self.decode(categorical_data, x)

        return x

    def train(self, mode: bool = True):
        super().train(mode)
        set_train_mode(self, mode)

# Cell
@variational_estimator
class AutoencoderForecast(nn.Module):
    def __init__(self, autoencoder, forecast_model):
        super().__init__()
        self.autoencoder = autoencoder
        self.forecast_model = forecast_model

    def forward(self, categorical_data, continuous_data):

        latent_space = self.autoencoder.encode(categorical_data, continuous_data)
        yhat = self.forecast_model(categorical_data, latent_space)

        return yhat

    def train(self, mode: bool = True):
        super().train(mode)
        set_train_mode(self, mode)

# Cell
class UnFlatten(nn.Module):

    def forward(self, input, dims):
        return input.view(*dims)

# Cell
@variational_estimator
class VariationalAutoencoder(Autoencoder):
    def __init__(self, encoder, decoder, h_dim, z_dim):
        super().__init__(encoder, decoder)
        self.h_dim = h_dim
        self.z_dim = z_dim
        self.flatten = Flatten()
        self.unflatten = UnFlatten()

        self.hidden2mu = nn.Linear(h_dim, z_dim)
        self.hidden2logvar = nn.Linear(h_dim, z_dim)
        self.latent_dimensions = None
        self._mu, self._logvar = None, None

    def encode(self, categorical_data, continuous_data, as_np=False):

        x_hidden = self.encoder(categorical_data, continuous_data)

        self.latent_dimensions = x_hidden.shape

        x_hidden = self.flatten(x_hidden)

        mu, logvar = self.hidden2mu(x_hidden), self.hidden2logvar(x_hidden)

        # required for vae loss
        self._mu, self._logvar = mu, logvar

        z = self.reparam(mu, logvar)

        if as_np: return to_np(z)
        else: return z

    def decode(self, categorical_data, continuous_data, as_np=False, latent_dimensions=None):

        if not latent_dimensions and not self.latent_dimensions:
            raise ValueError("latent_dimensions are not set to unflatten data.")
        if not latent_dimensions:
            latent_dimensions = self.latent_dimensions

        x = self.unflatten(continuous_data, latent_dimensions)

        x = self.decoder(categorical_data, x)

        if as_np: return to_np(x)
        else: return x

    def get_posteriors(self, categorical_data, continuous_data):

        return self.encode(continuous_data, categorical_data)

    def get_z(self, categorical_data, continuous_data):
        """Encode a batch of data points, x, into their z representations."""

        mu, logvar = self.encode(categorical_data, continuous_data)

        return self.reparam(mu, logvar)

    def reparam(self, mu, logvar):
        """Reparameterisation trick to sample z values.
        This is stochastic during training, and returns the mode during evaluation."""

        if self.training:
            # convert logarithmic variance to standard deviation representation
            std = torch.exp(logvar / 2)

            # create normal distribution as large as the data
            eps = torch.randn_like(std)
            # scale by learned mean and standard deviation
            return mu + eps*std
        else:
            return mu

    def train(self, mode: bool = True):
        super().train(mode)
        set_train_mode(self, mode)