# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/11_gan.learner.ipynb (unless otherwise specified).

__all__ = ['DummyDataset', 'DummyDatasetTS', 'Gan', 'W_Gan', 'GanLearner']

# Cell

import torch
import torch.nn as nn
import matplotlib.pyplot as plt

from tqdm import tqdm
from torch.nn import BCELoss, CrossEntropyLoss
from ..tabular.data import *
from ..tabular.core import *
from .model import *
import glob

# Cell

class DummyDataset(torch.utils.data.Dataset):

    def __init__(self, n_samples=1000, n_cat_feats=10, n_cont_feats=10, n_targets=1):

        self.n_samples = n_samples
        self.cat = torch.randn(n_cat_feats, n_samples)
        self.cont = torch.randn(n_cont_feats, n_samples)
        self.y = torch.randn(n_targets, n_samples)

    def __len__(self):
        return self.n_samples

    def __getitem__(self, idx):
        x_cat = self.cat[:, idx]
        x_cont = self.cont[:, idx]
        y = self.y[:, idx]
        return x_cat, x_cont, y

class DummyDatasetTS(torch.utils.data.Dataset):

    def __init__(self, n_samples=1000, n_cat_feats=10, n_cont_feats=10, n_targets=1, len_ts=96):

        self.n_samples = n_samples
        self.cat = torch.randn(n_samples, n_cat_features, len_ts)
        self.cont = torch.randn(n_samples, n_cont_features, len_ts)
        self.y = torch.randn(n_samples, n_targets)


    def __len__(self):
        return self.n_samples

    def __getitem__(self, idx):
        x_cat = self.cat[idx]
        x_cont = self.cont[idx]
        y = self.y[idx]
        return x_cat, x_cont, y


class Gan(nn.Module):
    def __init__(self, generator, discriminator, gen_optim, dis_optim, auxiliary=False, auxiliary_weighting_factor=1):
        super(Gan, self).__init__()
        self.generator = generator
        self.discriminator = discriminator
        self.gen_optim = gen_optim
        self.dis_optim = dis_optim
        self.real_loss = []
        self.fake_loss = []
        self.auxiliary = auxiliary
        self.bce_loss = BCELoss()
        self.auxiliary_loss_function = CrossEntropyLoss()
        self.auxiliary_weighting_factor=auxiliary_weighting_factor

    def to_device(self, device):
        self.generator = self.generator.to(device)
        self.discriminator = self.discriminator.to(device)
        self.bce_loss = self.bce_loss.to(device)
        self.auxiliary_loss_function = self.auxiliary_loss_function.to(device)

    def _split_pred(self, y):
        if self.auxiliary:
            y, class_probs = y
        else:
            y, class_probs = y, None
        return y, class_probs

    def auxiliary_loss(self, class_probs, y):
        return self.auxiliary_loss_function(class_probs, y.ravel().to(torch.int64))*self.auxiliary_weighting_factor

    def train_generator(self, z, x_cat, x_cont, y):
        # train the generator model
        self.generator.zero_grad()
        x_cont_fake = self.generator(x_cat, z)
        y_fake = self.discriminator(None, x_cont_fake)
        y_fake, class_probs = self._split_pred(y_fake)
        loss = self.bce_loss(y_fake, torch.ones_like(y_fake))
        if self.auxiliary:
            loss = (loss + self.auxiliary_loss(class_probs, y))/2
        loss.backward()
        self.gen_optim.step()
        return

    def train_discriminator(self, z, x_cat, x_cont, y):
        # train the discriminator model
        self.discriminator.zero_grad()
        y_real = self.discriminator(None, x_cont)
        y_real, class_probs = self._split_pred(y_real)
        real_loss = self.bce_loss(y_real, torch.ones_like(y_real))
        if self.auxiliary:
            real_loss = (real_loss + self.auxiliary_loss(class_probs, y))/2

        real_loss.backward()
        self.dis_optim.step()
        self.real_loss.append(real_loss.item())

        self.discriminator.zero_grad()
        x_cont_fake = self.generator(x_cat, z).detach()
        y_fake = self.discriminator(None, x_cont_fake)
        y_fake, class_probs = self._split_pred(y_fake)

        fake_loss =  self.bce_loss(y_fake, torch.zeros_like(y_fake))
        if self.auxiliary:
            fake_loss = (fake_loss + self.auxiliary_loss(class_probs, y))/2

        fake_loss.backward()
        self.dis_optim.step()
        self.fake_loss.append(fake_loss.item())
        return


class W_Gan(nn.Module):
    def __init__(self, generator, discriminator, gen_optim, dis_optim, clip=0.001):
        super(W_Gan, self).__init__()
        self.generator = generator
        self.discriminator = discriminator
        self.gen_optim = gen_optim
        self.dis_optim = dis_optim
        self.clip = clip
        self.real_loss = []
        self.fake_loss = []

    def train_generator(self, z, x_cat, x_cont, y):
        # train the generator model
        self.generator.zero_grad()
        x_cont_fake = self.generator(x_cat, z)
        y_fake = self.discriminator(None, x_cont_fake)
        loss = - y_fake.mean()
        loss.backward()
        self.gen_optim.step()
        return

    def train_discriminator(self, z, x_cat, x_cont, y):
        # train the discriminator model
        self.discriminator.zero_grad()
        y_real = self.discriminator(None, x_cont)
        real_loss = - y_real.mean()
        real_loss.backward()
        self.dis_optim.step()
        self.real_loss.append(real_loss.item())

        self.discriminator.zero_grad()
        x_cont_fake = self.generator(x_cat, z).detach()
        y_fake = self.discriminator(None, x_cont_fake)
        fake_loss = y_fake.mean()
        fake_loss.backward()
        self.dis_optim.step()
        self.fake_loss.append(fake_loss.item())

        for p in self.discriminator.parameters():
            p = torch.clamp(p, -self.clip, self.clip)
        return


class GanLearner():
    def __init__(self, gan, n_z=100):
        super(GanLearner, self).__init__()
        # gan should contain a class which itself contains a generator and discriminator/critic class and combines them
        self.gan = gan
        self.n_z = n_z
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

    def noise(self, x):
        if x.dim() == 2:
            z = torch.randn(x.shape[0], self.n_z).to(self.device)
        elif x.dim() == 3:
            z = torch.randn(x.shape[0], self.n_z, x.shape[2]).to(self.device)
        return z

    def generate_samples(self, x):
        z = self.noise(x)
        fake_samples = self.gan.generator(z, z).detach()
        return fake_samples

    def fit(self, dl, epochs=10, n_gen=1, n_dis=1, plot_epochs=10):
        # train gan and store parameters and losses in given class
        self.gan.to_device(self.device)

        for e in tqdm(range(epochs)):

            for x_cat, x_cont, y in dl:
                x_cat = x_cat.to(self.device)
                x_cont = x_cont.to(self.device)
                y = y.to(self.device)
                if y.dim() == 3:
                    y = y.squeeze(1)[:, 0] # <- quite ugly but does the job

                for _ in range(n_dis):
                    z = self.noise(x_cont)
                    self.gan.train_discriminator(z, x_cat, x_cont, y)

                for _ in range(n_gen):
                    z = self.noise(x_cont)
                    self.gan.train_generator(z, x_cat, x_cont, y)
                break

            if (e+1)%plot_epochs==0:
                plt.figure(figsize=(16, 9))
                plt.plot(self.gan.real_loss, label='Real Loss')
                plt.plot(self.gan.fake_loss, label='Fake Loss')
                plt.legend()
                plt.show()

        self.gan.to_device('cpu')

        return