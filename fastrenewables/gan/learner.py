# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/11_gan.learner.ipynb (unless otherwise specified).

__all__ = ['train_gan', 'train_gan']

# Cell
# export
from tqdm.notebook import tqdm, tnrange
import matplotlib.pyplot as plt
# import seaborn as sns
import torch
from fastai.torch_core import set_seed
import pandas as pd

# Cell
# TODO maik: should be a class with a constructur and a fit function
# TODO maik: remove col_name_ws
# TODO maik: can the device be removed?
def train_gan(epochs, dls, Dis, Gen,
              dis_optimizer, gen_optimizer,
              col_name_ws, device,
              n_noise_samples, cols,
              plot_every_n=5, plot_function=None,
              n_iter_dis=5,
              n_iter_gen=1,
             amount_clamp=0.01):
    set_seed(123123)
    n_samples = 1000
    rand_values = torch.rand(n_samples, n_noise_samples)

    dis_real_losses, dis_fake_losses, gen_losses = [],[],[]
    # Gen = Gen.to(device)
    # TODO maik: check if data is shuffled. e.g.:
#      if torch.allclose(dl.one_batch()[1], dl.one_batch()[1]):
#         raise ValueError(
#             "Dataset does not to be seemed shuffle, which is required for training."
#         )

    for epoch in tnrange(epochs, desc="epochs"):

        for i, (noise_sample, real_input_batch_cat,real_input_batch) in enumerate(dls[0]):
            noise_sample = noise_sample.to(device)
            real_input_batch = real_input_batch.to(device)
            minibatchsize = len(real_input_batch)

    #         ## train the discriminator (i-times per epoch):
            # TODO maik: should be configurable
            for i in range(n_iter_dis):
                Dis.zero_grad()
                ## train on real input
                # real_input_batch_cat is rather for the loss of the discriminator
                dis_real = Dis(None, real_input_batch)

                ## train on generated input
                dis_fake = Dis(None, Gen(real_input_batch_cat, noise_sample).detach())
                dis_loss = -(torch.mean(dis_real) - torch.mean(dis_fake))
                dis_loss.backward()
                dis_optimizer.step()


                ## clamp the discriminator parameter
                for p in Dis.parameters():
                    # TODO: what amount of clamp to use?
                    # TODO maik: should be configurable
                    p.data.clamp_(-amount_clamp, amount_clamp)

            dis_real_losses.append(dis_real.detach().mean())
            dis_fake_losses.append(dis_fake.detach().mean())
            # TODO maik: should be configurable
            for i in range(n_iter_gen):
                Gen.zero_grad()
                gen_out = Gen(real_input_batch_cat, noise_sample)
                dis_gen = Dis(None, gen_out)
                gen_loss = -torch.mean(dis_gen)
                gen_loss.backward()
                gen_optimizer.step()

            gen_losses.append(gen_loss.detach().mean())

        disl = dis_loss.detach().mean()
        genl = gen_loss.detach().mean()
        # TODO maik: improve output of trainer
        print(f"{epoch} dis_loss {disl} gen_loss {genl} ")

        # TODO maik: should be configurable
        if epoch % plot_every_n == 0:
            if plot_function is not None:
                with torch.no_grad():
                    fake_samples = Gen(real_input_batch_cat, rand_values.to(device))
                plot_function(fake_samples)

    # TODO maik: rather store as attributes in the class?!
    return dis_real_losses, dis_fake_losses

# Cell
# export
from tqdm.notebook import tqdm, tnrange
import matplotlib.pyplot as plt
# import seaborn as sns
import torch
from fastai.torch_core import set_seed
import pandas as pd

# Cell
# TODO maik: should be a class with a constructur and a fit function
# TODO maik: remove col_name_ws
# TODO maik: can the device be removed?
def train_gan(epochs, dls, Dis, Gen,
              dis_optimizer, gen_optimizer,
              col_name_ws, device,
              n_noise_samples, cols,
              plot_every_n=5, plot_function=None,
              n_iter_dis=5,
              n_iter_gen=1,
             amount_clamp=0.01):
    set_seed(123123)
    n_samples = 1000
    rand_values = torch.rand(n_samples, n_noise_samples)

    dis_real_losses, dis_fake_losses, gen_losses = [],[],[]
    # Gen = Gen.to(device)
    # TODO maik: check if data is shuffled. e.g.:
#      if torch.allclose(dl.one_batch()[1], dl.one_batch()[1]):
#         raise ValueError(
#             "Dataset does not to be seemed shuffle, which is required for training."
#         )

    for epoch in tnrange(epochs, desc="epochs"):

        for i, (noise_sample, real_input_batch_cat,real_input_batch) in enumerate(dls[0]):
            noise_sample = noise_sample.to(device)
            real_input_batch = real_input_batch.to(device)
            minibatchsize = len(real_input_batch)

    #         ## train the discriminator (i-times per epoch):
            # TODO maik: should be configurable
            for i in range(n_iter_dis):
                Dis.zero_grad()
                ## train on real input
                # real_input_batch_cat is rather for the loss of the discriminator
                dis_real = Dis(None, real_input_batch)

                ## train on generated input
                dis_fake = Dis(None, Gen(real_input_batch_cat, noise_sample).detach())
                dis_loss = -(torch.mean(dis_real) - torch.mean(dis_fake))
                dis_loss.backward()
                dis_optimizer.step()


                ## clamp the discriminator parameter
                for p in Dis.parameters():
                    # TODO: what amount of clamp to use?
                    # TODO maik: should be configurable
                    p.data.clamp_(-amount_clamp, amount_clamp)

            dis_real_losses.append(dis_real.detach().mean())
            dis_fake_losses.append(dis_fake.detach().mean())
            # TODO maik: should be configurable
            for i in range(n_iter_gen):
                Gen.zero_grad()
                gen_out = Gen(real_input_batch_cat, noise_sample)
                dis_gen = Dis(None, gen_out)
                gen_loss = -torch.mean(dis_gen)
                gen_loss.backward()
                gen_optimizer.step()

            gen_losses.append(gen_loss.detach().mean())

        disl = dis_loss.detach().mean()
        genl = gen_loss.detach().mean()
        # TODO maik: improve output of trainer
        print(f"{epoch} dis_loss {disl} gen_loss {genl} ")

        # TODO maik: should be configurable
        if epoch % plot_every_n == 0:
            if plot_function is not None:
                with torch.no_grad():
                    fake_samples = Gen(real_input_batch_cat, rand_values.to(device))
                plot_function(fake_samples)

    # TODO maik: rather store as attributes in the class?!
    return dis_real_losses, dis_fake_losses