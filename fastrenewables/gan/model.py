# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/10_gan.model.ipynb (unless otherwise specified).

__all__ = ['LinBnAct', 'GANMLP', 'AuxiliaryDiscriminator']

# Cell

import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

from functools import partial

from tqdm import tqdm

from ..utils import flatten_ts
from ..synthetic_data import GaussianDataset, plot_class_hists, DummyDataset
from ..timeseries.model import TemporalCNN
from ..tabular.model import EmbeddingModule

# Cell

def LinBnAct(si, so, use_bn, act_cls):
    layers = [nn.Linear(si, so)]
    if use_bn:
        layers += [nn.BatchNorm1d(so)]
    if act_cls is not None:
        layers += [act_cls]

    return nn.Sequential(*layers)

# Cell

class GANMLP(torch.nn.Module):
    def __init__(self, ann_structure, bn_cont=False, act_fct=torch.nn.ReLU,
                 final_act_fct=nn.Sigmoid, embedding_module=None):
        super(GANMLP, self).__init__()

        n_cont = ann_structure[0]
        if embedding_module is not None:
            emb_sz = []
            ann_structure[0] = ann_structure[0] + embedding_module.no_of_embeddings

        self.embedding_module = embedding_module

        layers = []
        for idx in range(1, len(ann_structure)):
            cur_use_bn = bn_cont
            cur_act_fct = act_fct()
            if idx == 1 and not bn_cont:
                cur_use_bn = False
            if idx == len(ann_structure)-1:
                cur_act_fct = None
                cur_use_bn = False

            layer = LinBnAct(ann_structure[idx-1], ann_structure[idx], cur_use_bn, cur_act_fct)
            layers.append(layer)
        if final_act_fct is not None:
            layers.append(final_act_fct())

        self.model = nn.Sequential(*layers)

    def forward(self, x_cat, x_cont):
        if self.embedding_module is not None:
            x_cat = self.embedding_module(x_cat)
            x_cont = torch.cat([x_cat, x_cont], 1)

        return self.model(x_cont)

# Cell

class AuxiliaryDiscriminator(torch.nn.Module):
    def __init__(self, basic_discriminator, n_classes, final_input_size, len_ts=1, bn=False):
        super(AuxiliaryDiscriminator, self).__init__()
        self.basic_discriminator = basic_discriminator
        self.n_classes = n_classes
        self.final_input_size = final_input_size
        self.len_ts = len_ts

        self.adv_layer = nn.Sequential(nn.Linear(self.final_input_size*len_ts, 1), nn.Sigmoid())
        self.aux_layer = nn.Sequential(nn.Linear(self.final_input_size*len_ts, n_classes), nn.Softmax(dim=1))

    def forward(self, cats, conts):
        out = self.basic_discriminator(cats, conts)
        out = torch.flatten(out, start_dim=1)
        validity = self.adv_layer(out)
        label = self.aux_layer(out)
        return (validity, label)