# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00d_baselines.ipynb (unless otherwise specified).

__all__ = ['BayesLinReg', 'RidgeRegression', 'relu', 'identity', 'sigmoid', 'tmp', 'test_acts', 'ELM',
           'EchoStateRegression', 'RocketReservoirRegression']

# Cell
#hide
import numpy as np
from sklearn.base import BaseEstimator
from sklearn.multioutput import MultiOutputRegressor
from sklearn.utils.validation import check_is_fitted
from sktime.transformations.panel.rocket import Rocket
from sktime.utils.data_processing import from_3d_numpy_to_nested
import matplotlib.pyplot as plt
from fastcore.basics import listify

# Cell
def _append_one_col(X):
    """append array with ones in order to replace a fitted bias in the output"""
    return np.hstack([np.ones((X.shape[0], 1)), X])

# Cell
class BayesLinReg(BaseEstimator):
    """Batch wise linear regression"""

    def __init__(self,  alpha:float=1., beta:float=1., empirical_bayes:bool=True, n_iter:int=100):
        self.alpha = alpha
        self.beta = beta
        self.n_features = None
        self.n_iter = n_iter
        self.empirical_bayes = empirical_bayes


    def _create_matrices(self, X):
        if self.n_features is None:
            self.n_features = X.shape[1]

            self.w_mean = np.zeros(self.n_features)
            self.w_precision = np.identity(self.n_features) / self.alpha

    def fit_empirical_bayes(self, X, y):
        X,y = self._check_and_prep(X, y)
        """
           Fitting empirical bayes based on training data to determine alpha and beta.
           This can be used when N>>M (more training samples than parameters).
        """

        M = X.T @ X
        eigenvalues = np.linalg.eigvalsh(M)
        identity_matrix = np.eye(np.size(X,1))#np.identity(self.n_features)
        N = len(X)


        for idx in range(self.n_iter):
            params = [self.alpha, self.beta]

            # update the inverse covariance matrix (Bishop eq. 3.51)
            w_precision = self.alpha * identity_matrix + self.beta * X.T @ X
            # update the mean vector (Bishop eq. 3.50)
            w_mean = self.beta * np.linalg.solve(w_precision, X.T @ y)


            # TODO (Bishop eq. 3.91)
            gamma = np.sum(eigenvalues / (self.alpha + eigenvalues))
            # TODO (Bishop eq. 3.98)
            self.alpha = float(gamma / np.sum(w_mean ** 2).clip(min=1e-10))

            # TODO (Bishop eq. 3.99)
            self.beta = float((N-gamma) / np.sum(np.square(y - X @ w_mean)))

            if np.allclose(params, [self.alpha, self.beta]):
                break

        if idx+1==self.n_iter:
            print(f"Optimization of alpha {self.alpha:0.2f} and beta {self.beta:0.2f} not terminated. ")

        self.w_mean = w_mean
        self.w_precision = w_precision
        # calculate the covariance in advance for faster inference
        self.w_covariance = np.linalg.inv(w_precision)


    def fit(self, X, y):
        if self.empirical_bayes:
            self.fit_empirical_bayes(X,y)
        else:
            self.update(X,y)

        return self

    def _check_and_prep(self, X, y = None):
        X = np.atleast_2d(X)
        X = _append_one_col(X)

        self._create_matrices(X)
        if y is None:
            return X
        else:
            y = np.atleast_1d(y)
            return X,y

    def update(self, X, y):

        X, y = self._check_and_prep(X, y)

        # update the inverse covariance matrix (Bishop eq. 3.51)
        w_precision = self.w_precision + self.beta * X.T @ X

        # update the mean vector (Bishop eq. 3.50)
        w_covariance = np.linalg.inv(w_precision)
        w_mean = w_covariance @ (self.w_precision @ self.w_mean + self.beta * y @ X)

        self.w_precision = w_precision
        self.w_covariance = np.linalg.inv(w_precision)
        self.w_mean = w_mean

        return self

    def _predict(self,X):
        X = self._check_and_prep(X)

        # calcualte the predictive mean (Bishop eq. 3.58)
        y_pred_mean = X @ self.w_mean

        # calculate the predictive variance (Bishop eq. 3.59)
        y_pred_var = 1 / self.beta + (X @ self.w_covariance * X).sum(axis=1)

        # Drop a dimension from the mean and variance in case x and y were singletons
        # There might be a more elegant way to proceed but this works!
        y_pred_mean = np.squeeze(y_pred_mean)
        y_pred_var = np.squeeze(y_pred_var)

        return y_pred_mean, y_pred_var ** 0.5


    def predict(self, X):

        y_pred_mean, _ =self._predict(X)

        return y_pred_mean

    def predict_proba(self, X):

        y_pred_mean, y_pred_std = self._predict(X)

        return y_pred_mean, y_pred_std


    def _log_prior(self, w):
        return -0.5 * self.alpha * np.sum(w ** 2)

    def _log_likelihood(self, X, y, w):
        return -0.5 * self.beta * np.square(y - X @ w).sum()


    def _log_posterior(self, X, y, w):
        return self._log_likelihood(X, y, w) + self._log_prior(w)

    def log_evidence(self, X:np.ndarray, y:np.ndarray):
        X, y = self._check_and_prep(X, y)

        N = len(y)
        D = np.size(X, 1)

        return 0.5 * (D * np.log(self.alpha) + N * np.log(self.beta)
            - np.linalg.slogdet(self.w_precision)[1] - D * np.log(2 * np.pi)
        ) + self._log_posterior(X, y, self.w_mean)

# Cell
class RidgeRegression(BaseEstimator):
    def __init__(self, l2_reg=1e-3):
        self.l2_reg = l2_reg

    def _ridge_regression(self, X, y):
        """ridge / tikhonov regularized linear multiple, multivariate regression"""
        return np.linalg.inv(X.T @ X + self.l2_reg * np.eye(X.shape[1])) @ X.T @ y

    def fit(self, X, y):
        X = _append_one_col(X)
        self.W = self._ridge_regression(X, y)

        return self

    def predict(self, X):
        X = _append_one_col(X)
        y_hat = X @ self.W

        return y_hat

# Cell
def relu(x):
    return np.maximum(x, 0)

def identity(x):
    return x

def sigmoid(x):
    return 1/(1+np.exp(-x))

# Cell
#hide
tmp = ["2"]
test_acts = "relu,sigmoid"
tmp += test_acts.split(",")
tmp

# Cell
class ELM(BaseEstimator):
    def __init__(
        self, n_hidden=20, activations=relu, prediction_model=BayesLinReg(),
                include_original_features=True,**kwargs):

        """[summary]

        Args:
            n_hidden (int, optional): [description]. Defaults to 100.
            activations ([type], optional): [description]. Defaults to relu.
            prediction_model_type (str, optional): [description]. Defaults to "ridge".
        """

        self.prediction_model = prediction_model
        self.n_hidden = n_hidden
        self.include_original_features = include_original_features
        self._hidden_weights, self._biases  = None, None

        if self.prediction_model is None:
            self._prediction_model = BayesLinReg()
        else:
            self._prediction_model = self.prediction_model

        self._convert_activations(activations)

        super(ELM).__init__()

    def _convert_activations(self, activations):
        self.activations = []
        activations = listify(activations)
        conversion_dict = {"relu": relu, "identity": identity, "sigmoid": sigmoid,
                           "tanh": np.tanh}
        supported_activations = list(conversion_dict.keys())

        for activation in activations:
            if type(activation) is str:
                activation = activation.split(",")
                for act in activation:
                    if act in supported_activations:
                        self.activations.append(conversion_dict[act])
            elif callable(activation):
                self.activations.append(activation)

    def transform_X(self, X, W, b, activations):
        G = np.dot(X, W) + b
        Hs = []
        for act in activations:
            Hs.append(act(G))
        return np.concatenate(Hs, axis=1)

    def _check_and_prep(self, X, y):
        X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)

        self._n_features = X.shape[1]
        if self._hidden_weights is None:
            self._hidden_weights = 0.1*np.random.normal(size=[self._n_features, self.n_hidden])
            self._biases = 0.1*np.random.normal(size=[self.n_hidden])


        X_transformed = self.transform_X(
            X, self._hidden_weights, self._biases, self.activations
        )

        if self.include_original_features:
            X_transformed = np.concatenate([X_transformed, X], axis=1)

        return X_transformed, y

    def fit(self, X, y):

        X_transformed, y = self._check_and_prep(X, y)

        self._prediction_model.fit(X_transformed, y)

        return self

    def update(self, X, y):

        X_transformed, y = self._check_and_prep(X, y)

        self._prediction_model.update(X_transformed, y)

        return self

    def _prep_pred_X(self, X):
         # Check is fit had been called
        check_is_fitted(self)

        X = self._validate_data(X)

        X_transformed = self.transform_X(
            X, self._hidden_weights, self._biases, listify(self.activations)
        )

        if self.include_original_features:
            X_transformed = np.concatenate([X_transformed, X], axis=1)

        return X_transformed


    def predict(self, X):
        X_transformed = self._prep_pred_X(X)
        y_hat = self._prediction_model.predict(X_transformed)

        return y_hat

    def predict_proba(self, X):
        X_transformed = self._prep_pred_X(X)
        return self._prediction_model.predict_proba(X_transformed)

    def set_params(self, **params):
        local_param_keys = self.get_params(deep=False).keys()
        local_params = {k: v for (k, v) in params.items() if k in local_param_keys}

        for k, v in local_params.items():
            setattr(self, k, v)

        # assume that remaining keys must be part of the prediction model
        non_local_params = {
            k: v for (k, v) in params.items() if k not in local_param_keys
        }
        self.prediction_model.set_params(**non_local_params)

    def get_params(self, deep=False):
        return super().get_params()

    def log_evidence(self, X:np.ndarray, y:np.ndarray):
        X_transformed = self._prep_pred_X(X)
        return self._prediction_model.log_evidence(X_transformed, y)

# Cell
class EchoStateRegression(BaseEstimator):
    def __init__(
        self,
        num_hidden=100,
        activations=[relu, np.tanh, identity],
        activation_momentum=0.9,
        prediction_model=RidgeRegression(),
    ):
        # number of input features (necessary for initialization)
        self.num_in_feature = None
        # number of hidden recurrent units
        self.num_hidden = num_hidden
        # squashing / activation function
        self.activations = activations
        # activation momentum: each recurrent activation is a moving average of the past
        self.activation_momentum = activation_momentum

        self.prediction_model = prediction_model

    def _init_params(self):
        """randomly initialize parameters"""
        # initalize input weights
        self.W_in = (
            np.random.randn(self.num_in_feature, self.num_hidden)
            * np.sqrt(2 / self.num_hidden)
            - 0.5
        )
        # initialize recurrent weights
        self.W_rec = np.random.randn(self.num_hidden, self.num_hidden)
        # compute spectral radius from recurrent weights
        spectral_radius = np.max(np.abs(np.linalg.eigvals(self.W_rec)))
        # normalize recurrent weights via the spectral radius
        self.W_rec /= spectral_radius * np.sqrt(2)
        # biases
        self.b = np.random.randn(1, self.num_hidden)

    def _run_reservoir(self, X):
        """Fcompute reservoir activations of the echo state network"""
        # get number of sequences
        num_sequences = X.shape[0]
        # get number of time steps (assumed to be equal, e.g. by padding)
        num_time_steps = X.shape[1]
        # compute the matrix product between each input sequence element and
        # the input weight matrix (einsum replaces a loop over time steps)
        H_in = np.einsum("ijk,kl", X, self.W_in)

        H_recs = []
        # initialize recurrent activation with zeros
        for act_func in self.activations:
            H_rec = np.zeros((num_sequences, self.num_hidden))
            # loop all time steps
            for time_step in range(num_time_steps):
                # compute recurrent input from previous time step recurrent activation
                rec = H_rec @ self.W_rec
                # comput hidden recurrent activation for next time step
                H_rec = (1 - self.activation_momentum) * act_func(
                    H_in[:, time_step, :] + rec + self.b
                ) + self.activation_momentum * H_rec
            H_recs.append(H_rec)

        H_rec = np.concatenate(H_recs, axis=1)
        # return final activation for the last time step
        return H_rec

    def fit(self, X, y):
        """Fitting a echo state network with a single time step forecast horizon."""

        if self.num_in_feature == None:
            self.num_in_feature = X.shape[2]
            self._init_params()
        # compute reservoir activations
        H = self._run_reservoir(X)

        self.prediction_model = MultiOutputRegressor(self.prediction_model)
        self.prediction_model = self.prediction_model.fit(H, y)

        return self

    def predict(self, X):
        """Predicting the trained echo state network"""
        # assert that number of input features was correct
        assert self.num_in_feature == X.shape[2]
        # compute reservoir activations
        H = self._run_reservoir(X)
        # append array with ones in order to replace a fitted bias in the output
        return self.prediction_model.predict(H)

# Cell
class RocketReservoirRegression(BaseEstimator):
    def __init__(
        self, num_kernels=1000, prediction_model=RidgeRegression(), normalise=False
    ):
        self.num_kernels = num_kernels
        self.prediction_model = prediction_model
        self.rocket = Rocket(num_kernels=num_kernels, normalise=normalise)

    def fit(self, X, y):
        """expects array of dimensions num_samples X timesteps X num features"""
        X_transformed = from_3d_numpy_to_nested(X.swapaxes(1, 2))
        X_transformed = self.rocket.fit_transform(X_transformed)

        self.prediction_model = MultiOutputRegressor(self.prediction_model)
        self.prediction_model = self.prediction_model.fit(X_transformed, y)

        return self

    def predict(self, X):
        X_transformed = from_3d_numpy_to_nested(X.swapaxes(1, 2))
        X_transformed = self.rocket.transform(X_transformed)

        yhat = self.prediction_model.predict(X_transformed)

        return yhat