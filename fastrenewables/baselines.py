# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00d_baselines.ipynb (unless otherwise specified).

__all__ = ['BayesLinReg', 'RidgeRegression', 'relu', 'identity', 'sigmoid', 'tmp', 'test_acts', 'ELM',
           'sample_bayes_linear_model']

# Cell
#hide
import numpy as np
from sklearn.base import BaseEstimator
from sklearn.multioutput import MultiOutputRegressor
from sklearn.utils.validation import check_is_fitted
from sktime.transformations.panel.rocket import Rocket
from sktime.utils.data_processing import from_3d_numpy_to_nested
import matplotlib.pyplot as plt
from fastcore.basics import listify
from fastcore.test import *
np.random.seed(23)

# Cell
def _append_one_col(X):
    """append array with ones in order to replace a fitted bias in the output"""
    return np.hstack([np.ones((X.shape[0], 1)), X])

# Cell
class BayesLinReg(BaseEstimator):
    """Batch wise linear regression"""

    def __init__(self,  alpha:float=1., beta:float=1., empirical_bayes:bool=True, n_iter:int=100):
        self.alpha = alpha
        self.beta = beta
        self.n_features = None
        self.n_iter = n_iter
        self.empirical_bayes = empirical_bayes


    def _create_matrices(self, X):
        if self.n_features is None:
            self.n_features = X.shape[1]

            self.w_mean = np.zeros(self.n_features)
            self.w_precision = np.identity(self.n_features) / self.alpha

    def fit_empirical_bayes(self, X, y):
        X,y = self._check_and_prep(X, y)
        """
           Fitting empirical bayes based on training data to determine alpha and beta.
           This can be used when N>>M (more training samples than parameters).
        """

        M = X.T @ X
        eigenvalues = np.linalg.eigvalsh(M)
        identity_matrix = np.eye(np.size(X,1))#np.identity(self.n_features)
        N = len(X)


        for idx in range(self.n_iter):
            params = [self.alpha, self.beta]

            # update the inverse covariance matrix (Bishop eq. 3.51)
            w_precision = self.alpha * identity_matrix + self.beta * X.T @ X
            # update the mean vector (Bishop eq. 3.50)
            w_mean = self.beta * np.linalg.solve(w_precision, X.T @ y)


            # TODO (Bishop eq. 3.91)
            gamma = np.sum(eigenvalues / (self.alpha + eigenvalues))
            # TODO (Bishop eq. 3.98)
            self.alpha = float(gamma / np.sum(w_mean ** 2).clip(min=1e-10))

            # TODO (Bishop eq. 3.99)
            self.beta = float((N-gamma) / np.sum(np.square(y - X @ w_mean)))

            if np.allclose(params, [self.alpha, self.beta]):
                break

        if idx+1==self.n_iter:
            print(f"Optimization of alpha {self.alpha:0.2f} and beta {self.beta:0.2f} not terminated. ")

        self.w_mean = w_mean
        self.w_precision = w_precision
        # calculate the covariance in advance for faster inference
        self.w_covariance = np.linalg.inv(w_precision)


    def fit(self, X, y):
        if self.empirical_bayes:
            self.fit_empirical_bayes(X,y)
        else:
            self.update(X,y)

        return self

    def _check_and_prep(self, X, y = None):
        X = np.atleast_2d(X)
        X = _append_one_col(X)

        self._create_matrices(X)
        if y is None:
            return X
        else:
            y = np.atleast_1d(y)
            return X,y

    def update(self, X, y):

        X, y = self._check_and_prep(X, y)

        # update the inverse covariance matrix (Bishop eq. 3.51)
        w_precision = self.w_precision + self.beta * X.T @ X

        # update the mean vector (Bishop eq. 3.50)
        w_covariance = np.linalg.inv(w_precision)
        w_mean = w_covariance @ (self.w_precision @ self.w_mean + self.beta * y @ X)

        self.w_precision = w_precision
        self.w_covariance = np.linalg.inv(w_precision)
        self.w_mean = w_mean

        return self

    def _predict(self,X):
        X = self._check_and_prep(X)

        # calcualte the predictive mean (Bishop eq. 3.58)
        y_pred_mean = X @ self.w_mean

        # calculate the predictive variance (Bishop eq. 3.59)
        y_pred_var = 1 / self.beta + (X @ self.w_covariance * X).sum(axis=1)

        # Drop a dimension from the mean and variance in case x and y were singletons
        # There might be a more elegant way to proceed but this works!
        y_pred_mean = np.squeeze(y_pred_mean)
        y_pred_var = np.squeeze(y_pred_var)

        return y_pred_mean, y_pred_var ** 0.5


    def predict(self, X):

        y_pred_mean, _ =self._predict(X)

        return y_pred_mean

    def predict_proba(self, X):

        y_pred_mean, y_pred_std = self._predict(X)

        return y_pred_mean, y_pred_std


    def _log_prior(self, w):
        return -0.5 * self.alpha * np.sum(w ** 2)

    def _log_likelihood(self, X, y, w):
        return -0.5 * self.beta * np.square(y - X @ w).sum()


    def _log_posterior(self, X, y, w):
        return self._log_likelihood(X, y, w) + self._log_prior(w)

    def log_evidence(self, X:np.ndarray, y:np.ndarray):
        X, y = self._check_and_prep(X, y)

        N, M = X.shape

        # E(\mathbf{m}_n) = \beta/2 \cdot ||y- X \mathbf{m}_n|| + \alpha/2 \mathbf{m}_n^T \mathbf{m}_n,
        # where \mathbf{m}_n is the mean weight. This is the same as the negative of the posterior
        Emn = -self._log_posterior(X, y, self.w_mean)

        # Bishop eq. 3.86
        return 0.5 * (M * np.log(self.alpha) + N * np.log(self.beta)
            - np.linalg.slogdet(self.w_precision)[1] - N * np.log(2 * np.pi)
        ) - Emn

# Cell
class RidgeRegression(BaseEstimator):
    def __init__(self, l2_reg=1e-3):
        self.l2_reg = l2_reg

    def _ridge_regression(self, X, y):
        """ridge / tikhonov regularized linear multiple, multivariate regression"""
        return np.linalg.inv(X.T @ X + self.l2_reg * np.eye(X.shape[1])) @ X.T @ y

    def fit(self, X, y):
        X = _append_one_col(X)
        self.W = self._ridge_regression(X, y)

        return self

    def predict(self, X):
        X = _append_one_col(X)
        y_hat = X @ self.W

        return y_hat

# Cell
def relu(x):
    return np.maximum(x, 0)

def identity(x):
    return x

def sigmoid(x):
    return 1/(1+np.exp(-x))

# Cell
#hide
tmp = ["2"]
test_acts = "relu,sigmoid"
tmp += test_acts.split(",")
test_eq(tmp, ['2', 'relu', 'sigmoid'])

# Cell
class ELM(BaseEstimator):
    def __init__(
        self, n_hidden=20, activations=relu, prediction_model=BayesLinReg(),
                include_original_features=True,**kwargs):

        """[summary]

        Args:
            n_hidden (int, optional): [description]. Defaults to 100.
            activations ([type], optional): [description]. Defaults to relu.
            prediction_model_type (str, optional): [description]. Defaults to "ridge".
        """

        self.prediction_model = prediction_model
        self.n_hidden = n_hidden
        self.include_original_features = include_original_features
        self._hidden_weights, self._biases  = None, None

        if self.prediction_model is None:
            self._prediction_model = BayesLinReg()
        else:
            self._prediction_model = self.prediction_model

        self._convert_activations(activations)

        super(ELM).__init__()

    def _convert_activations(self, activations):
        self.activations = []
        activations = listify(activations)
        conversion_dict = {"relu": relu, "identity": identity, "sigmoid": sigmoid,
                           "tanh": np.tanh}
        supported_activations = list(conversion_dict.keys())

        for activation in activations:
            if type(activation) is str:
                activation = activation.split(",")
                for act in activation:
                    if act in supported_activations:
                        self.activations.append(conversion_dict[act])
            elif callable(activation):
                self.activations.append(activation)

    def transform_X(self, X, W, b, activations):
        G = np.dot(X, W) + b
        Hs = []
        for act in activations:
            Hs.append(act(G))
        return np.concatenate(Hs, axis=1)

    def _check_and_prep(self, X, y):
        X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)

        self._n_features = X.shape[1]
        if self._hidden_weights is None:
            self._hidden_weights = 0.1*np.random.normal(size=[self._n_features, self.n_hidden])
            self._biases = 0.1*np.random.normal(size=[self.n_hidden])


        X_transformed = self.transform_X(
            X, self._hidden_weights, self._biases, self.activations
        )

        if self.include_original_features:
            X_transformed = np.concatenate([X_transformed, X], axis=1)

        return X_transformed, y

    def fit(self, X, y):

        X_transformed, y = self._check_and_prep(X, y)

        self._prediction_model.fit(X_transformed, y)

        return self

    def update(self, X, y):

        X_transformed, y = self._check_and_prep(X, y)

        self._prediction_model.update(X_transformed, y)

        return self

    def _prep_pred_X(self, X):
         # Check is fit had been called
        check_is_fitted(self)

        X = self._validate_data(X)

        X_transformed = self.transform_X(
            X, self._hidden_weights, self._biases, listify(self.activations)
        )

        if self.include_original_features:
            X_transformed = np.concatenate([X_transformed, X], axis=1)

        return X_transformed


    def predict(self, X):
        X_transformed = self._prep_pred_X(X)
        y_hat = self._prediction_model.predict(X_transformed)

        return y_hat

    def predict_proba(self, X):
        X_transformed = self._prep_pred_X(X)
        return self._prediction_model.predict_proba(X_transformed)

    def set_params(self, **params):
        local_param_keys = self.get_params(deep=False).keys()
        local_params = {k: v for (k, v) in params.items() if k in local_param_keys}

        for k, v in local_params.items():
            setattr(self, k, v)

        # assume that remaining keys must be part of the prediction model
        non_local_params = {
            k: v for (k, v) in params.items() if k not in local_param_keys
        }
        self.prediction_model.set_params(**non_local_params)

    def get_params(self, deep=False):
        return super().get_params()

    def log_evidence(self, X:np.ndarray, y:np.ndarray):
        X_transformed = self._prep_pred_X(X)
        return self._prediction_model.log_evidence(X_transformed, y)

# Cell
def sample_bayes_linear_model(model, X, n_samples=100):
    if isinstance(model, ELM):
        X = model._prep_pred_X(X)
        prediction_model = model.prediction_model
    elif isinstance(model, BayesLinReg):
        prediction_model = model
    else:
        raise ValueError("Not supported model.")

    # add bias
    X = _append_one_col(X)

    sampled_mean = np.random.multivariate_normal(prediction_model.w_mean.reshape(-1),\
                                                 prediction_model.w_covariance, size=n_samples)
    y_sampled = X @ sampled_mean.T

    return y_sampled